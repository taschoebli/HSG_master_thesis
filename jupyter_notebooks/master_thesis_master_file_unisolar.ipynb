{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MnSuayJCd541"
      },
      "outputs": [],
      "source": [
        "MODEL_TYPE = \"LSTM\" # Choose between LSTM, DeepAR and MOIRAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N023cFtseXHa",
        "outputId": "c216d8ca-ecbc-44da-c798-28ea1f0b913b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM model type stuff\n"
          ]
        }
      ],
      "source": [
        "# Initialize Directories and Configurations\n",
        "if MODEL_TYPE == \"LSTM\":\n",
        "  print(\"LSTM model type stuff\")\n",
        "\n",
        "elif MODEL_TYPE == \"DeepAR\":\n",
        "  print(\"DeepAR model type stuff\")\n",
        "  # install with support for torch models\n",
        "  !pip install \"gluonts[torch]\"\n",
        "  # install with support for mxnet models\n",
        "  !pip install \"gluonts[mxnet]\"\n",
        "\n",
        "elif MODEL_TYPE == \"MOIRAI\":\n",
        "  print(\"MOIRAI model type stuff\")\n",
        "  !git clone https://github.com/taschoebli/uni2ts.git\n",
        "  %cd uni2ts\n",
        "  !pip install -e '.[notebook]'\n",
        "  #!pip install uni2ts\n",
        "  !touch .env\n",
        "\n",
        "else:\n",
        "  print(\"Invalid model type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6YkS5WEYYYx",
        "outputId": "435a2774-eeb2-4b80-a958-164f267d365c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "General imports loaded\n",
            "LSTM specific imports loaded\n"
          ]
        }
      ],
      "source": [
        "#General imports\n",
        "print(\"General imports loaded\")\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "if MODEL_TYPE == \"LSTM\":\n",
        "  print(\"LSTM specific imports loaded\")\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "elif MODEL_TYPE == \"DeepAR\":\n",
        "  print(\"DeepAR specific imports loaded\")\n",
        "  from gluonts.torch import DeepAREstimator\n",
        "  from gluonts.dataset.pandas import PandasDataset\n",
        "  from gluonts.dataset.split import split\n",
        "\n",
        "elif MODEL_TYPE == \"MOIRAI\":\n",
        "  print(\"MOIRAI specific imports loaded\")\n",
        "  from huggingface_hub import hf_hub_download\n",
        "  from uni2ts.model.moirai import MoiraiForecast, MoiraiModule\n",
        "  from uni2ts.model.moirai_moe import MoiraiMoEForecast, MoiraiMoEModule\n",
        "  from gluonts.dataset.pandas import PandasDataset\n",
        "  from gluonts.dataset.split import split\n",
        "\n",
        "else:\n",
        "  print(\"Invalid model type\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "af2SkhDQZ0Bv"
      },
      "outputs": [],
      "source": [
        "# Functions, Classes and other Helpers\n",
        "\n",
        "# General ----------------------------------------------------------------------\n",
        "\n",
        "# Load data from csv\n",
        "def load_data(filepath):\n",
        "    data = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "    return data\n",
        "\n",
        "# Filename extractor\n",
        "def file_name_no_extension(filepath):\n",
        "  # Extract the filename without the folder path\n",
        "  filename_with_extension = os.path.basename(filepath)\n",
        "\n",
        "  # Remove the `.csv` extension\n",
        "  filename_without_extension = os.path.splitext(filename_with_extension)[0]\n",
        "\n",
        "  return filename_without_extension\n",
        "\n",
        "\n",
        "# Day extractor\n",
        "def get_values_by_day(df_internal, offset=0):\n",
        "  # Ensure index is datetime\n",
        "  if not isinstance(df_internal.index, pd.DatetimeIndex):\n",
        "      df_internal.index = pd.to_datetime(df_internal.index)\n",
        "\n",
        "  # Apply offset by removing the last 'offset' rows\n",
        "  if offset > 0:\n",
        "      df_internal = df_internal.iloc[:-offset]\n",
        "\n",
        "  # Filter timestamps to only include those at exactly 08:00\n",
        "  df_filtered = df_internal[df_internal.index.hour == 8]\n",
        "\n",
        "  # Group by date and store in a dictionary\n",
        "  #result = {}\n",
        "  #for date, group in df_filtered.groupby(df_filtered.index.date):\n",
        "      #result[str(date)] = group.index.tolist()\n",
        "  # Extract timestamps as strings instead of lists\n",
        "  result = {group.index[0].strftime('%Y-%m-%d %H:%M:%S') for date, group in df_filtered.groupby(df_filtered.index.date)}\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "# Calculate SMAPE metric\n",
        "def calc_smape(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculate the Symmetric Mean Absolute Percentage Error (SMAPE).\n",
        "  y_true: array-like, actual values\n",
        "  y_pred: array-like, predicted values\n",
        "  \"\"\"\n",
        "  epsilon = 1e-10  # Small constant to avoid division by zero\n",
        "  denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
        "  smape_values = np.abs(y_true - y_pred) / (denominator + epsilon)\n",
        "  smape_values = np.nan_to_num(smape_values, nan=0.0, posinf=0.0, neginf=0.0)  # Handle division by zero\n",
        "\n",
        "  return np.mean(smape_values) * 100  # Convert to percentage\n",
        "\n",
        "def optimize_memory():\n",
        "  # Memory optimization\n",
        "  # Set CUDA memory management configuration to avoid fragmentation\n",
        "  # https://pytorch.org/docs/stable/notes/cuda.html#using-custom-memory-allocators-for-cuda\n",
        "  os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "  # Check if CUDA is available\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"Using device: {device}\")\n",
        "  torch.cuda.empty_cache()\n",
        "  os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\n",
        "\n",
        "\n",
        "# LSTM specific ----------------------------------------------------------------\n",
        "\n",
        "# Create sequences for time-series prediction\n",
        "def create_sequences(features, target, context_length, prediction_length):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(features) - context_length - prediction_length + 1):\n",
        "        X_seq.append(features[i:i+context_length])  # Past `context_length` values\n",
        "        y_seq.append(target[i+context_length:i+context_length+prediction_length])  # Next `prediction_length` values\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Define the LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        out = self.fc(hidden[-1])  # Use the last hidden state\n",
        "        return out\n",
        "\n",
        "def train_LSTM_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}')\n",
        "    return epoch_loss/len(train_loader)\n",
        "\n",
        "def evaluate_LSTM_model(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            predictions.extend(outputs.tolist())\n",
        "            actuals.extend(y_batch.tolist())\n",
        "    # Feature added here!!!\n",
        "    # Truncate negative predictions to 0\n",
        "    predictions = np.maximum(predictions, 0)\n",
        "    return np.array(predictions), np.array(actuals)\n",
        "\n",
        "def get_index_and_timestamp_for_validation(hour, original_df, train_size):\n",
        "    \"\"\"\n",
        "    Finds the first available testdata tensor index that matches the given hour.\n",
        "\n",
        "    Parameters:\n",
        "    - hour (int): The desired hour (0-23).\n",
        "    - original_df (pd.DataFrame): The original dataset with a datetime index.\n",
        "    - train_size (int): The number of samples in the training dataset + Context Length.\n",
        "\n",
        "    Returns:\n",
        "    - test_index (int): The index relative to testdata tensor.\n",
        "    - timestamp (str): The corresponding datetime string.\n",
        "    \"\"\"\n",
        "    # Ensure datetime format\n",
        "    original_df.index = pd.to_datetime(original_df.index)\n",
        "\n",
        "    # Extract test dataset portion\n",
        "    test_df = original_df.iloc[train_size:]  # This contains only the test set\n",
        "\n",
        "    # Find the first row where the hour matches\n",
        "    for i, timestamp in enumerate(test_df.index):\n",
        "        if timestamp.hour == hour:\n",
        "            test_index = i  # Relative index in testdata tensor\n",
        "            return test_index, timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    raise ValueError(f\"No test data found at hour {hour}:00.\")\n",
        "\n",
        "\n",
        "def truncate_tensor_with_interval(tensor, start_index, offset=1):\n",
        "    \"\"\"\n",
        "    Truncates the given tensor along the first dimension, starting at start_index,\n",
        "    and selects every `offset`-th row.\n",
        "\n",
        "    Parameters:\n",
        "    - tensor (torch.Tensor): The input tensor (shape: [X, Y]).\n",
        "    - start_index (int): The base index from which truncation starts.\n",
        "    - offset (int): Step interval to select elements (e.g., every 4th, 12th, etc.).\n",
        "\n",
        "    Returns:\n",
        "    - truncated_tensor (torch.Tensor): The truncated tensor with selected intervals.\n",
        "    \"\"\"\n",
        "    if start_index >= tensor.shape[0]:\n",
        "        raise ValueError(\"Start index exceeds tensor dimensions.\")\n",
        "\n",
        "    # Make sure that below 24 hours, offset is set to one day!\n",
        "    if offset < 24:\n",
        "      offset = 24\n",
        "\n",
        "    return tensor[start_index::offset]  # Truncate and select every `offset` step\n",
        "\n",
        "def data_validation_LSTM(df, actuals, X_train, CONTEXT_LENGTH, PREDICTION_LENGTH, hour=8):\n",
        "    \"\"\"\n",
        "    Validates the actual LSTM predictions against the true values in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The original dataset with a datetime index.\n",
        "    - actuals (torch.Tensor): The model's predicted values (shape: [X, Y]).\n",
        "    - X_train (numpy array or tensor): The training dataset.\n",
        "    - CONTEXT_LENGTH (int): The number of past timesteps used for forecasting.\n",
        "    - PREDICTION_LENGTH (int): The number of future timesteps predicted.\n",
        "    - hour (int): The desired hour (default: 8 AM).\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Get the test index and timestamp\n",
        "    INDEX, timestamp = get_index_and_timestamp_for_validation(hour, df, train_size=len(X_train) + CONTEXT_LENGTH)\n",
        "\n",
        "    # Step 2: Extract predictions using the computed index\n",
        "    actuals_daily = truncate_tensor_with_interval(actuals, INDEX, offset=PREDICTION_LENGTH)\n",
        "\n",
        "    # Step 3: Retrieve true values from `df['dc_power']`\n",
        "    start_index = len(X_train) + CONTEXT_LENGTH + INDEX\n",
        "    true_values = df['dc_power'].iloc[start_index:start_index + PREDICTION_LENGTH].values  # Convert to NumPy array\n",
        "\n",
        "    # Step 4: Format values for comparison\n",
        "    formatted_actuals = np.round(actuals_daily[0], 1)  # Convert tensor to NumPy & round\n",
        "    formatted_true_values = np.round(true_values, 1)  # Round true values for comparison\n",
        "\n",
        "    # Step 5: Print Debugging Information\n",
        "    print(\"🔍 Validation at timestamp:\", timestamp)\n",
        "    print(\"Predicted Values (Rounded):\")\n",
        "    print(f\"{formatted_actuals}\")\n",
        "    print(\"True Values (Rounded):\")\n",
        "    print(f\"{formatted_true_values}\")\n",
        "\n",
        "    # Step 6: Perform Validation Check\n",
        "    validation_passed = np.array_equal(formatted_actuals, formatted_true_values)\n",
        "\n",
        "    print(\"\\n✅ Validation Passed\" if validation_passed else \"\\n❌ Validation Failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b_AgDYt-8_z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7oYYV-DYpgB",
        "outputId": "18545d98-9100-4f66-e3f5-20a5a64ad0ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM model definitions and constants loaded\n"
          ]
        }
      ],
      "source": [
        "# Definitions and Constants\n",
        "#FILEPATH = '/content/sample_data/230401_250108_PT1H_Solcast_reduced_features.csv'\n",
        "FILEPATH = '/content/sample_data/200101_220423_PT1H_Unisolar.csv'\n",
        "PREDICTION_LENGTH_LIST = [4, 12, 24, 48, 96, 336] # Use the past 4, 12, 24, 48, 96, 336 time steps for prediction (4 hours to 14 days)\n",
        "CONTEXT_LENGTH_FOLD_LIST = [1, 2, 4, 8] # Use 1, 2, 4 and 8 fold PREDICTION_LENGTH\n",
        "START_HOUR = 8 # Use to start plots at this hour\n",
        "#TRAIN_DATA_LAST_INDEX = 8784 # Corresponds to '2024-04-01 00:00:00', used for LSTM and DeepAR in 230401_250108_PT1H_Solcast_reduced_features.csv\n",
        "TRAIN_DATA_LAST_INDEX = 8785 # Corresponds to '2021-01-01 00:00:00', used for LSTM and DeepAR in 200101_220423_PT1H_Unisolar.csv\n",
        "\n",
        "if MODEL_TYPE == \"LSTM\":\n",
        "  print(\"LSTM model definitions and constants loaded\")\n",
        "  HIDDEN_SIZE = 64  # Number of hidden units -> # of neurons in the LSTM's hidden layer\n",
        "  BATCH_SIZE = 64  # Batch size -> # of samples processed in parallel\n",
        "  NUM_LAYERS = 2  # Number of LSTM layers -> # of stacked LSTM layers\n",
        "  LR = 1e-3\n",
        "  NUM_EPOCHS = 25\n",
        "\n",
        "elif MODEL_TYPE == \"DeepAR\":\n",
        "  print(\"DeepAR model type stuff\")\n",
        "  HIDDEN_SIZE = 64 # Number of RNN cells for each layer (default: 40)\n",
        "  NUM_LAYERS = 2 # Number of RNN layers (default: 2)\n",
        "  LR = 1e-3\n",
        "  NUM_EPOCHS = 25\n",
        "  optimize_memory()\n",
        "\n",
        "elif MODEL_TYPE == \"MOIRAI\":\n",
        "  PATCH_SIZE = 32 # patch size, Number of samples for each layer or sequence, 32 or 64 recommended for hourly\n",
        "  BATCH_SIZE = 16 # batch size, samples processed in parallel\n",
        "  LR = \"nA\"\n",
        "  NUM_EPOCHS = \"nA\"\n",
        "  LOSS = \"nA\"\n",
        "  DROPOUT_P = \"nA\"\n",
        "  MODEL = \"moirai-1.1-R\"  # model name: choose from {'moirai-1.1-R', 'moirai-moe-1.0-R'}\n",
        "  SIZE = \"base\"  # model size: choose from {'small', 'base', 'large'}\n",
        "  MOIRAI_MODEL_STR = f\"Salesforce/{MODEL}-{SIZE}\"\n",
        "  MOIRAI_MODEL_PATH_SAFE = MOIRAI_MODEL_STR.replace(\"Salesforce/\", \"\").replace(\"/\", \"-\").replace(\".\", \"-\")\n",
        "  optimize_memory()\n",
        "\n",
        "else:\n",
        "  print(\"Invalid model type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwqJv1T9cjfL",
        "outputId": "7763c295-9e4f-4bf2-b8cf-1f271a1d94c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM model type stuff\n"
          ]
        }
      ],
      "source": [
        "if MODEL_TYPE == \"LSTM\":\n",
        "  print(\"LSTM model type stuff\")\n",
        "\n",
        "elif MODEL_TYPE == \"DeepAR\":\n",
        "\n",
        "  print(\"DeepAR model type stuff\")\n",
        "\n",
        "elif MODEL_TYPE == \"MOIRAI\":\n",
        "\n",
        "  print(\"MOIRAI model type stuff\")\n",
        "\n",
        "else:\n",
        "  print(\"Invalid model type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eOuzkZPrXDYf"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load data from csv\n",
        "df = load_data(FILEPATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwyPxPrUcrFi",
        "outputId": "0288f46f-331d-4ecc-99e4-054ccbabdd83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM feature scaling\n"
          ]
        }
      ],
      "source": [
        "## Step 2: Feature Scaling\n",
        "if MODEL_TYPE == \"LSTM\":\n",
        "  print(\"LSTM feature scaling\")\n",
        "  # Separate features and target\n",
        "  target_column = 'dc_power'\n",
        "  features = [\n",
        "      'air_temp', 'albedo', 'azimuth', 'clearsky_dhi', 'clearsky_dni', 'clearsky_ghi', 'clearsky_gti',\n",
        "      'cloud_opacity', 'dewpoint_temp', 'dhi', 'dni', 'ghi', 'gti', 'precipitable_water',\n",
        "      'precipitation_rate', 'relative_humidity', 'surface_pressure', 'snow_depth',\n",
        "      'snow_water_equivalent', 'snow_soiling_rooftop', 'snow_soiling_ground',\n",
        "      'wind_direction_100m', 'wind_direction_10m', 'wind_speed_100m', 'wind_speed_10m', 'zenith'\n",
        "  ]\n",
        "  \"\"\"\n",
        "  # For Solcast\n",
        "  features = [\n",
        "      'air_temp', 'azimuth', 'cloud_opacity', 'dhi', 'dni', 'ghi', 'zenith'\n",
        "  ]\n",
        "  \"\"\"\n",
        "  # For Unisolar\n",
        "  features = [\n",
        "      'CloudOpacity', 'Ghi', 'ApparentTemperature', 'AirTemperature', 'DewPointTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection'\n",
        "  ]\n",
        "  X = df[features]\n",
        "  y = df[target_column]\n",
        "\n",
        "  # Standardize the features\n",
        "  scaler = StandardScaler()\n",
        "  #scaler = MinMaxScaler()\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "  # Convert target to a numpy array\n",
        "  y = y.to_numpy()\n",
        "\n",
        "elif MODEL_TYPE == \"DeepAR\" or \"MOIRAI\":\n",
        "  print(\"DeepAR OR MOIRAI feature scaling\")\n",
        "  # https://ts.gluon.ai/dev/api/gluonts/gluonts.dataset.pandas.html\n",
        "  # Convert into GluonTS dataset with features\n",
        "  \"\"\"\n",
        "  # For Solcast\n",
        "  dataset = PandasDataset(df, target=\"dc_power\",\n",
        "                        past_feat_dynamic_real=['dc_power', 'air_temp', 'azimuth', 'cloud_opacity', 'dhi', 'dni', 'ghi', 'zenith'],\n",
        "                        feat_dynamic_real=['air_temp', 'azimuth', 'cloud_opacity', 'dhi', 'dni', 'ghi', 'zenith'])\n",
        "  \"\"\"\n",
        "  # For Unisolar\n",
        "  dataset = PandasDataset(df, target=\"dc_power\",\n",
        "                        past_feat_dynamic_real=['dc_power', 'CloudOpacity', 'Ghi', 'ApparentTemperature', 'AirTemperature', 'DewPointTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection'],\n",
        "                        feat_dynamic_real=['CloudOpacity', 'Ghi', 'ApparentTemperature', 'AirTemperature', 'DewPointTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection'])\n",
        "\n",
        "elif MODEL_TYPE == \"MOIRAI\":\n",
        "  print(\"MOIRAI feature scaling\")\n",
        "\n",
        "else:\n",
        "  print(\"Invalid model type - feature scaling failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD2g7m_5hOaq",
        "outputId": "be989cb3-3e55-43ed-c789-6be454a7f1ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.8601\n",
            "Epoch 2/25, Loss: 32.3324\n",
            "Epoch 3/25, Loss: 25.3260\n",
            "Epoch 4/25, Loss: 22.2177\n",
            "Epoch 5/25, Loss: 21.3512\n",
            "Epoch 6/25, Loss: 20.7118\n",
            "Epoch 7/25, Loss: 20.1065\n",
            "Epoch 8/25, Loss: 19.6330\n",
            "Epoch 9/25, Loss: 19.2451\n",
            "Epoch 10/25, Loss: 18.8816\n",
            "Epoch 11/25, Loss: 18.5234\n",
            "Epoch 12/25, Loss: 18.1818\n",
            "Epoch 13/25, Loss: 17.8517\n",
            "Epoch 14/25, Loss: 17.5634\n",
            "Epoch 15/25, Loss: 17.2887\n",
            "Epoch 16/25, Loss: 17.0366\n",
            "Epoch 17/25, Loss: 16.8037\n",
            "Epoch 18/25, Loss: 16.5850\n",
            "Epoch 19/25, Loss: 16.3886\n",
            "Epoch 20/25, Loss: 16.2126\n",
            "Epoch 21/25, Loss: 16.0546\n",
            "Epoch 22/25, Loss: 15.9037\n",
            "Epoch 23/25, Loss: 15.7575\n",
            "Epoch 24/25, Loss: 15.6206\n",
            "Epoch 25/25, Loss: 15.4899\n",
            "🔍 Validation at timestamp: 2021-01-01 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[2.7 7.3 9.1 4.9]\n",
            "True Values (Rounded):\n",
            "[2.7 7.3 9.1 4.9]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=4, CL=4, Model=LSTM\n",
            "Length of: RMSE=478, MAE=478, SMAPE=478, R2=478\n",
            "Mean of: RMSE=4.6392, MAE=4.1937, SMAPE=65.67%, R^2=-8.1822\n",
            "Median of: RMSE=4.3828, MAE=3.8978, SMAPE=57.26%, R^2=-0.0298\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 43.0126\n",
            "Epoch 2/25, Loss: 27.2396\n",
            "Epoch 3/25, Loss: 21.7222\n",
            "Epoch 4/25, Loss: 18.0286\n",
            "Epoch 5/25, Loss: 15.4199\n",
            "Epoch 6/25, Loss: 13.9595\n",
            "Epoch 7/25, Loss: 13.2114\n",
            "Epoch 8/25, Loss: 12.7121\n",
            "Epoch 9/25, Loss: 12.3846\n",
            "Epoch 10/25, Loss: 12.1850\n",
            "Epoch 11/25, Loss: 12.0079\n",
            "Epoch 12/25, Loss: 11.8468\n",
            "Epoch 13/25, Loss: 11.7112\n",
            "Epoch 14/25, Loss: 11.5873\n",
            "Epoch 15/25, Loss: 11.4613\n",
            "Epoch 16/25, Loss: 11.3577\n",
            "Epoch 17/25, Loss: 11.2667\n",
            "Epoch 18/25, Loss: 11.1649\n",
            "Epoch 19/25, Loss: 11.0912\n",
            "Epoch 20/25, Loss: 11.0040\n",
            "Epoch 21/25, Loss: 10.9188\n",
            "Epoch 22/25, Loss: 10.8473\n",
            "Epoch 23/25, Loss: 10.7931\n",
            "Epoch 24/25, Loss: 10.7206\n",
            "Epoch 25/25, Loss: 10.6501\n",
            "🔍 Validation at timestamp: 2021-01-02 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 4.1 11.8 11.9 14.9]\n",
            "True Values (Rounded):\n",
            "[ 4.1 11.8 11.9 14.9]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=4, CL=8, Model=LSTM\n",
            "Length of: RMSE=477, MAE=477, SMAPE=477, R2=477\n",
            "Mean of: RMSE=4.1552, MAE=3.6948, SMAPE=58.07%, R^2=-9.7949\n",
            "Median of: RMSE=3.7202, MAE=3.1917, SMAPE=48.26%, R^2=0.3642\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 41.6499\n",
            "Epoch 2/25, Loss: 21.3296\n",
            "Epoch 3/25, Loss: 14.9318\n",
            "Epoch 4/25, Loss: 13.1590\n",
            "Epoch 5/25, Loss: 12.4951\n",
            "Epoch 6/25, Loss: 12.0120\n",
            "Epoch 7/25, Loss: 11.5836\n",
            "Epoch 8/25, Loss: 11.3178\n",
            "Epoch 9/25, Loss: 11.1664\n",
            "Epoch 10/25, Loss: 11.0654\n",
            "Epoch 11/25, Loss: 10.9864\n",
            "Epoch 12/25, Loss: 10.9190\n",
            "Epoch 13/25, Loss: 10.7590\n",
            "Epoch 14/25, Loss: 10.6736\n",
            "Epoch 15/25, Loss: 10.5409\n",
            "Epoch 16/25, Loss: 10.4013\n",
            "Epoch 17/25, Loss: 10.2519\n",
            "Epoch 18/25, Loss: 10.1133\n",
            "Epoch 19/25, Loss: 9.9735\n",
            "Epoch 20/25, Loss: 9.8349\n",
            "Epoch 21/25, Loss: 9.7170\n",
            "Epoch 22/25, Loss: 9.7599\n",
            "Epoch 23/25, Loss: 9.7111\n",
            "Epoch 24/25, Loss: 9.9599\n",
            "Epoch 25/25, Loss: 9.5925\n",
            "🔍 Validation at timestamp: 2021-01-02 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 4.1 11.8 11.9 14.9]\n",
            "True Values (Rounded):\n",
            "[ 4.1 11.8 11.9 14.9]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=4, CL=16, Model=LSTM\n",
            "Length of: RMSE=477, MAE=477, SMAPE=477, R2=477\n",
            "Mean of: RMSE=3.9597, MAE=3.5248, SMAPE=56.31%, R^2=-9.3329\n",
            "Median of: RMSE=3.4507, MAE=3.0399, SMAPE=46.18%, R^2=0.4081\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.0869\n",
            "Epoch 2/25, Loss: 39.1941\n",
            "Epoch 3/25, Loss: 36.7615\n",
            "Epoch 4/25, Loss: 19.7252\n",
            "Epoch 5/25, Loss: 15.0372\n",
            "Epoch 6/25, Loss: 12.9585\n",
            "Epoch 7/25, Loss: 12.3702\n",
            "Epoch 8/25, Loss: 11.8953\n",
            "Epoch 9/25, Loss: 11.5374\n",
            "Epoch 10/25, Loss: 11.2695\n",
            "Epoch 11/25, Loss: 11.0733\n",
            "Epoch 12/25, Loss: 10.9873\n",
            "Epoch 13/25, Loss: 10.9251\n",
            "Epoch 14/25, Loss: 10.8475\n",
            "Epoch 15/25, Loss: 10.7094\n",
            "Epoch 16/25, Loss: 10.5618\n",
            "Epoch 17/25, Loss: 10.4390\n",
            "Epoch 18/25, Loss: 10.5326\n",
            "Epoch 19/25, Loss: 10.2389\n",
            "Epoch 20/25, Loss: 10.0419\n",
            "Epoch 21/25, Loss: 9.8622\n",
            "Epoch 22/25, Loss: 9.6206\n",
            "Epoch 23/25, Loss: 9.4616\n",
            "Epoch 24/25, Loss: 9.2872\n",
            "Epoch 25/25, Loss: 9.1918\n",
            "🔍 Validation at timestamp: 2021-01-03 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[1.8 6.8 6.1 6.9]\n",
            "True Values (Rounded):\n",
            "[1.8 6.8 6.1 6.9]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=4, CL=32, Model=LSTM\n",
            "Length of: RMSE=476, MAE=476, SMAPE=476, R2=476\n",
            "Mean of: RMSE=4.7810, MAE=4.3195, SMAPE=62.97%, R^2=-13.9600\n",
            "Median of: RMSE=4.0948, MAE=3.5582, SMAPE=51.57%, R^2=0.1763\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 43.2606\n",
            "Epoch 2/25, Loss: 29.8757\n",
            "Epoch 3/25, Loss: 21.5040\n",
            "Epoch 4/25, Loss: 16.8122\n",
            "Epoch 5/25, Loss: 14.7566\n",
            "Epoch 6/25, Loss: 13.6488\n",
            "Epoch 7/25, Loss: 12.9502\n",
            "Epoch 8/25, Loss: 12.4592\n",
            "Epoch 9/25, Loss: 12.0792\n",
            "Epoch 10/25, Loss: 11.8538\n",
            "Epoch 11/25, Loss: 11.6641\n",
            "Epoch 12/25, Loss: 11.5058\n",
            "Epoch 13/25, Loss: 11.3609\n",
            "Epoch 14/25, Loss: 11.2515\n",
            "Epoch 15/25, Loss: 11.1501\n",
            "Epoch 16/25, Loss: 11.0231\n",
            "Epoch 17/25, Loss: 10.8967\n",
            "Epoch 18/25, Loss: 10.7726\n",
            "Epoch 19/25, Loss: 10.6340\n",
            "Epoch 20/25, Loss: 10.5101\n",
            "Epoch 21/25, Loss: 10.3948\n",
            "Epoch 22/25, Loss: 10.2869\n",
            "Epoch 23/25, Loss: 10.1867\n",
            "Epoch 24/25, Loss: 10.0980\n",
            "Epoch 25/25, Loss: 9.9999\n",
            "🔍 Validation at timestamp: 2021-01-02 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 4.1 11.8 11.9 14.9 20.5 21.4 16.5 20.4 16.8 11.9  6.7  2. ]\n",
            "True Values (Rounded):\n",
            "[ 4.1 11.8 11.9 14.9 20.5 21.4 16.5 20.4 16.8 11.9  6.7  2. ]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=12, CL=12, Model=LSTM\n",
            "Length of: RMSE=477, MAE=477, SMAPE=477, R2=477\n",
            "Mean of: RMSE=4.7135, MAE=3.8614, SMAPE=70.37%, R^2=-0.6789\n",
            "Median of: RMSE=4.3412, MAE=3.6253, SMAPE=66.66%, R^2=0.3489\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 43.7362\n",
            "Epoch 2/25, Loss: 37.6350\n",
            "Epoch 3/25, Loss: 28.2088\n",
            "Epoch 4/25, Loss: 19.5110\n",
            "Epoch 5/25, Loss: 15.7339\n",
            "Epoch 6/25, Loss: 14.1479\n",
            "Epoch 7/25, Loss: 13.1797\n",
            "Epoch 8/25, Loss: 12.5332\n",
            "Epoch 9/25, Loss: 12.0631\n",
            "Epoch 10/25, Loss: 11.7388\n",
            "Epoch 11/25, Loss: 11.4668\n",
            "Epoch 12/25, Loss: 11.2505\n",
            "Epoch 13/25, Loss: 11.1109\n",
            "Epoch 14/25, Loss: 11.0503\n",
            "Epoch 15/25, Loss: 10.9753\n",
            "Epoch 16/25, Loss: 10.8335\n",
            "Epoch 17/25, Loss: 10.7268\n",
            "Epoch 18/25, Loss: 10.5498\n",
            "Epoch 19/25, Loss: 10.4039\n",
            "Epoch 20/25, Loss: 10.3046\n",
            "Epoch 21/25, Loss: 10.1855\n",
            "Epoch 22/25, Loss: 10.0484\n",
            "Epoch 23/25, Loss: 9.9128\n",
            "Epoch 24/25, Loss: 9.7813\n",
            "Epoch 25/25, Loss: 9.6599\n",
            "🔍 Validation at timestamp: 2021-01-02 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 4.1 11.8 11.9 14.9 20.5 21.4 16.5 20.4 16.8 11.9  6.7  2. ]\n",
            "True Values (Rounded):\n",
            "[ 4.1 11.8 11.9 14.9 20.5 21.4 16.5 20.4 16.8 11.9  6.7  2. ]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=12, CL=24, Model=LSTM\n",
            "Length of: RMSE=477, MAE=477, SMAPE=477, R2=477\n",
            "Mean of: RMSE=4.9350, MAE=4.0665, SMAPE=74.29%, R^2=-1.0908\n",
            "Median of: RMSE=4.4472, MAE=3.6680, SMAPE=70.57%, R^2=0.2632\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.1398\n",
            "Epoch 2/25, Loss: 39.4739\n",
            "Epoch 3/25, Loss: 39.4307\n",
            "Epoch 4/25, Loss: 39.4736\n",
            "Epoch 5/25, Loss: 39.4546\n",
            "Epoch 6/25, Loss: 39.3524\n",
            "Epoch 7/25, Loss: 39.0778\n",
            "Epoch 8/25, Loss: 33.6219\n",
            "Epoch 9/25, Loss: 25.4802\n",
            "Epoch 10/25, Loss: 18.9067\n",
            "Epoch 11/25, Loss: 16.1825\n",
            "Epoch 12/25, Loss: 14.8375\n",
            "Epoch 13/25, Loss: 13.9333\n",
            "Epoch 14/25, Loss: 13.2413\n",
            "Epoch 15/25, Loss: 12.7385\n",
            "Epoch 16/25, Loss: 12.3591\n",
            "Epoch 17/25, Loss: 12.0468\n",
            "Epoch 18/25, Loss: 11.7786\n",
            "Epoch 19/25, Loss: 11.5689\n",
            "Epoch 20/25, Loss: 11.5343\n",
            "Epoch 21/25, Loss: 11.3660\n",
            "Epoch 22/25, Loss: 11.2606\n",
            "Epoch 23/25, Loss: 11.0808\n",
            "Epoch 24/25, Loss: 11.0106\n",
            "Epoch 25/25, Loss: 10.9848\n",
            "🔍 Validation at timestamp: 2021-01-03 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 1.8  6.8  6.1  6.9  3.  12.4 20.4  1.3 16.9  6.7  7.7  4.2]\n",
            "True Values (Rounded):\n",
            "[ 1.8  6.8  6.1  6.9  3.  12.4 20.4  1.3 16.9  6.7  7.7  4.2]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=12, CL=48, Model=LSTM\n",
            "Length of: RMSE=476, MAE=476, SMAPE=476, R2=476\n",
            "Mean of: RMSE=5.3243, MAE=4.4584, SMAPE=76.67%, R^2=-2.4734\n",
            "Median of: RMSE=4.9343, MAE=4.1152, SMAPE=74.44%, R^2=0.1305\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.0008\n",
            "Epoch 2/25, Loss: 37.2261\n",
            "Epoch 3/25, Loss: 26.3353\n",
            "Epoch 4/25, Loss: 18.4375\n",
            "Epoch 5/25, Loss: 15.6128\n",
            "Epoch 6/25, Loss: 14.2170\n",
            "Epoch 7/25, Loss: 13.3064\n",
            "Epoch 8/25, Loss: 12.6245\n",
            "Epoch 9/25, Loss: 12.1034\n",
            "Epoch 10/25, Loss: 11.7451\n",
            "Epoch 11/25, Loss: 11.4971\n",
            "Epoch 12/25, Loss: 11.3497\n",
            "Epoch 13/25, Loss: 11.2523\n",
            "Epoch 14/25, Loss: 11.1513\n",
            "Epoch 15/25, Loss: 11.0855\n",
            "Epoch 16/25, Loss: 11.0274\n",
            "Epoch 17/25, Loss: 10.8684\n",
            "Epoch 18/25, Loss: 10.7270\n",
            "Epoch 19/25, Loss: 10.6130\n",
            "Epoch 20/25, Loss: 10.5476\n",
            "Epoch 21/25, Loss: 10.5640\n",
            "Epoch 22/25, Loss: 10.8387\n",
            "Epoch 23/25, Loss: 10.6035\n",
            "Epoch 24/25, Loss: 10.4951\n",
            "Epoch 25/25, Loss: 10.6669\n",
            "🔍 Validation at timestamp: 2021-01-05 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 5.  10.3 15.9 19.3 21.6 20.4  0.  21.3 18.8 15.4 10.2  5.7]\n",
            "True Values (Rounded):\n",
            "[ 5.  10.3 15.9 19.3 21.6 20.4  0.  21.3 18.8 15.4 10.2  5.7]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=12, CL=96, Model=LSTM\n",
            "Length of: RMSE=474, MAE=474, SMAPE=474, R2=474\n",
            "Mean of: RMSE=5.5671, MAE=4.6701, SMAPE=88.38%, R^2=-1.1973\n",
            "Median of: RMSE=5.4385, MAE=4.4511, SMAPE=86.48%, R^2=-0.0795\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.1376\n",
            "Epoch 2/25, Loss: 38.7271\n",
            "Epoch 3/25, Loss: 35.7418\n",
            "Epoch 4/25, Loss: 27.3350\n",
            "Epoch 5/25, Loss: 21.1430\n",
            "Epoch 6/25, Loss: 19.0230\n",
            "Epoch 7/25, Loss: 17.7177\n",
            "Epoch 8/25, Loss: 16.7878\n",
            "Epoch 9/25, Loss: 15.8945\n",
            "Epoch 10/25, Loss: 14.9664\n",
            "Epoch 11/25, Loss: 14.1798\n",
            "Epoch 12/25, Loss: 13.5435\n",
            "Epoch 13/25, Loss: 12.9787\n",
            "Epoch 14/25, Loss: 12.6348\n",
            "Epoch 15/25, Loss: 12.3166\n",
            "Epoch 16/25, Loss: 12.0785\n",
            "Epoch 17/25, Loss: 11.9037\n",
            "Epoch 18/25, Loss: 11.8252\n",
            "Epoch 19/25, Loss: 11.7043\n",
            "Epoch 20/25, Loss: 11.5637\n",
            "Epoch 21/25, Loss: 11.4276\n",
            "Epoch 22/25, Loss: 11.3781\n",
            "Epoch 23/25, Loss: 11.3123\n",
            "Epoch 24/25, Loss: 11.2002\n",
            "Epoch 25/25, Loss: 11.0981\n",
            "🔍 Validation at timestamp: 2021-01-02 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 4.1 11.8 11.9 14.9 20.5 21.4 16.5 20.4 16.8 11.9  6.7  2.   0.7  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.2]\n",
            "True Values (Rounded):\n",
            "[ 4.1 11.8 11.9 14.9 20.5 21.4 16.5 20.4 16.8 11.9  6.7  2.   0.7  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.2]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=24, CL=24, Model=LSTM\n",
            "Length of: RMSE=476, MAE=476, SMAPE=476, R2=476\n",
            "Mean of: RMSE=3.5379, MAE=2.2988, SMAPE=93.69%, R^2=0.1444\n",
            "Median of: RMSE=3.3097, MAE=2.1428, SMAPE=86.55%, R^2=0.6202\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.3211\n",
            "Epoch 2/25, Loss: 39.3734\n",
            "Epoch 3/25, Loss: 39.3947\n",
            "Epoch 4/25, Loss: 39.3883\n",
            "Epoch 5/25, Loss: 39.2692\n",
            "Epoch 6/25, Loss: 39.0546\n",
            "Epoch 7/25, Loss: 39.0712\n",
            "Epoch 8/25, Loss: 38.8730\n",
            "Epoch 9/25, Loss: 38.0933\n",
            "Epoch 10/25, Loss: 34.2399\n",
            "Epoch 11/25, Loss: 29.0049\n",
            "Epoch 12/25, Loss: 24.0034\n",
            "Epoch 13/25, Loss: 20.6849\n",
            "Epoch 14/25, Loss: 18.8828\n",
            "Epoch 15/25, Loss: 17.8537\n",
            "Epoch 16/25, Loss: 17.1414\n",
            "Epoch 17/25, Loss: 16.5597\n",
            "Epoch 18/25, Loss: 15.8771\n",
            "Epoch 19/25, Loss: 15.1831\n",
            "Epoch 20/25, Loss: 14.4899\n",
            "Epoch 21/25, Loss: 13.9042\n",
            "Epoch 22/25, Loss: 13.4308\n",
            "Epoch 23/25, Loss: 13.0915\n",
            "Epoch 24/25, Loss: 12.7581\n",
            "Epoch 25/25, Loss: 12.5750\n",
            "🔍 Validation at timestamp: 2021-01-03 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 1.8  6.8  6.1  6.9  3.  12.4 20.4  1.3 16.9  6.7  7.7  4.2  0.6  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.8]\n",
            "True Values (Rounded):\n",
            "[ 1.8  6.8  6.1  6.9  3.  12.4 20.4  1.3 16.9  6.7  7.7  4.2  0.6  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.8]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=24, CL=48, Model=LSTM\n",
            "Length of: RMSE=475, MAE=475, SMAPE=475, R2=475\n",
            "Mean of: RMSE=3.4932, MAE=2.3255, SMAPE=92.19%, R^2=-0.1984\n",
            "Median of: RMSE=3.2096, MAE=2.1671, SMAPE=89.37%, R^2=0.6826\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.1191\n",
            "Epoch 2/25, Loss: 39.4875\n",
            "Epoch 3/25, Loss: 39.5152\n",
            "Epoch 4/25, Loss: 39.5254\n",
            "Epoch 5/25, Loss: 39.5311\n",
            "Epoch 6/25, Loss: 39.5347\n",
            "Epoch 7/25, Loss: 39.5375\n",
            "Epoch 8/25, Loss: 39.5403\n",
            "Epoch 9/25, Loss: 39.5423\n",
            "Epoch 10/25, Loss: 39.5514\n",
            "Epoch 11/25, Loss: 39.5519\n",
            "Epoch 12/25, Loss: 39.5516\n",
            "Epoch 13/25, Loss: 39.5471\n",
            "Epoch 14/25, Loss: 39.5727\n",
            "Epoch 15/25, Loss: 39.5167\n",
            "Epoch 16/25, Loss: 38.5085\n",
            "Epoch 17/25, Loss: 33.1563\n",
            "Epoch 18/25, Loss: 25.2406\n",
            "Epoch 19/25, Loss: 21.8029\n",
            "Epoch 20/25, Loss: 19.7834\n",
            "Epoch 21/25, Loss: 18.4219\n",
            "Epoch 22/25, Loss: 17.6010\n",
            "Epoch 23/25, Loss: 16.6811\n",
            "Epoch 24/25, Loss: 15.7757\n",
            "Epoch 25/25, Loss: 15.1164\n",
            "🔍 Validation at timestamp: 2021-01-05 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 5.  10.3 15.9 19.3 21.6 20.4  0.  21.3 18.8 15.4 10.2  5.7  0.8  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.9]\n",
            "True Values (Rounded):\n",
            "[ 5.  10.3 15.9 19.3 21.6 20.4  0.  21.3 18.8 15.4 10.2  5.7  0.8  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.9]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=24, CL=96, Model=LSTM\n",
            "Length of: RMSE=473, MAE=473, SMAPE=473, R2=473\n",
            "Mean of: RMSE=3.8521, MAE=2.7426, SMAPE=100.77%, R^2=-0.7396\n",
            "Median of: RMSE=3.6870, MAE=2.6187, SMAPE=92.19%, R^2=0.6298\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.4954\n",
            "Epoch 2/25, Loss: 38.4386\n",
            "Epoch 3/25, Loss: 34.0693\n",
            "Epoch 4/25, Loss: 28.7390\n",
            "Epoch 5/25, Loss: 23.7704\n",
            "Epoch 6/25, Loss: 20.9291\n",
            "Epoch 7/25, Loss: 19.6276\n",
            "Epoch 8/25, Loss: 18.9208\n",
            "Epoch 9/25, Loss: 18.3262\n",
            "Epoch 10/25, Loss: 17.7392\n",
            "Epoch 11/25, Loss: 17.1496\n",
            "Epoch 12/25, Loss: 16.5597\n",
            "Epoch 13/25, Loss: 15.7661\n",
            "Epoch 14/25, Loss: 15.3235\n",
            "Epoch 15/25, Loss: 14.4908\n",
            "Epoch 16/25, Loss: 13.9016\n",
            "Epoch 17/25, Loss: 13.5845\n",
            "Epoch 18/25, Loss: 13.3015\n",
            "Epoch 19/25, Loss: 13.0398\n",
            "Epoch 20/25, Loss: 12.8077\n",
            "Epoch 21/25, Loss: 12.5305\n",
            "Epoch 22/25, Loss: 12.4288\n",
            "Epoch 23/25, Loss: 12.4127\n",
            "Epoch 24/25, Loss: 12.1822\n",
            "Epoch 25/25, Loss: 11.9569\n",
            "🔍 Validation at timestamp: 2021-01-09 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 5.7 11.2  0.   0.  20.8  0.  20.3 19.7 17.8 14.6  9.8  4.4  0.8  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.4]\n",
            "True Values (Rounded):\n",
            "[ 5.7 11.2  0.   0.  20.8  0.  20.3 19.7 17.8 14.6  9.8  4.4  0.8  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.4]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=24, CL=192, Model=LSTM\n",
            "Length of: RMSE=469, MAE=469, SMAPE=469, R2=469\n",
            "Mean of: RMSE=3.4285, MAE=2.2752, SMAPE=104.38%, R^2=0.0672\n",
            "Median of: RMSE=3.1543, MAE=2.1038, SMAPE=110.00%, R^2=0.6801\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.3032\n",
            "Epoch 2/25, Loss: 39.0909\n",
            "Epoch 3/25, Loss: 38.7998\n",
            "Epoch 4/25, Loss: 39.1165\n",
            "Epoch 5/25, Loss: 38.8042\n",
            "Epoch 6/25, Loss: 37.0165\n",
            "Epoch 7/25, Loss: 32.6444\n",
            "Epoch 8/25, Loss: 27.6358\n",
            "Epoch 9/25, Loss: 23.1505\n",
            "Epoch 10/25, Loss: 20.6645\n",
            "Epoch 11/25, Loss: 19.4778\n",
            "Epoch 12/25, Loss: 18.7653\n",
            "Epoch 13/25, Loss: 17.9904\n",
            "Epoch 14/25, Loss: 17.2755\n",
            "Epoch 15/25, Loss: 16.6308\n",
            "Epoch 16/25, Loss: 16.0643\n",
            "Epoch 17/25, Loss: 15.5292\n",
            "Epoch 18/25, Loss: 14.9990\n",
            "Epoch 19/25, Loss: 14.5383\n",
            "Epoch 20/25, Loss: 14.1094\n",
            "Epoch 21/25, Loss: 13.7938\n",
            "Epoch 22/25, Loss: 13.5425\n",
            "Epoch 23/25, Loss: 13.4003\n",
            "Epoch 24/25, Loss: 13.3297\n",
            "Epoch 25/25, Loss: 13.2643\n",
            "🔍 Validation at timestamp: 2021-01-03 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 1.8  6.8  6.1  6.9  3.  12.4 20.4  1.3 16.9  6.7  7.7  4.2  0.6  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.8  4.6  5.4  8.5 11.1\n",
            "  8.9  9.6 19.7 14.6  0.   8.7  1.4  0.2  0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.8]\n",
            "True Values (Rounded):\n",
            "[ 1.8  6.8  6.1  6.9  3.  12.4 20.4  1.3 16.9  6.7  7.7  4.2  0.6  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.8  4.6  5.4  8.5 11.1\n",
            "  8.9  9.6 19.7 14.6  0.   8.7  1.4  0.2  0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   0.8]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=48, CL=48, Model=LSTM\n",
            "Length of: RMSE=237, MAE=237, SMAPE=237, R2=237\n",
            "Mean of: RMSE=3.7709, MAE=2.5791, SMAPE=122.05%, R^2=0.3582\n",
            "Median of: RMSE=3.5651, MAE=2.4870, SMAPE=121.88%, R^2=0.5795\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.5431\n",
            "Epoch 2/25, Loss: 37.9336\n",
            "Epoch 3/25, Loss: 34.3520\n",
            "Epoch 4/25, Loss: 29.3848\n",
            "Epoch 5/25, Loss: 24.2710\n",
            "Epoch 6/25, Loss: 21.1757\n",
            "Epoch 7/25, Loss: 20.0328\n",
            "Epoch 8/25, Loss: 19.2661\n",
            "Epoch 9/25, Loss: 18.6807\n",
            "Epoch 10/25, Loss: 18.1109\n",
            "Epoch 11/25, Loss: 17.4612\n",
            "Epoch 12/25, Loss: 16.8538\n",
            "Epoch 13/25, Loss: 16.2433\n",
            "Epoch 14/25, Loss: 15.7333\n",
            "Epoch 15/25, Loss: 15.2675\n",
            "Epoch 16/25, Loss: 14.8528\n",
            "Epoch 17/25, Loss: 14.5238\n",
            "Epoch 18/25, Loss: 14.2611\n",
            "Epoch 19/25, Loss: 14.0151\n",
            "Epoch 20/25, Loss: 13.8356\n",
            "Epoch 21/25, Loss: 13.6718\n",
            "Epoch 22/25, Loss: 13.5350\n",
            "Epoch 23/25, Loss: 13.4036\n",
            "Epoch 24/25, Loss: 13.2813\n",
            "Epoch 25/25, Loss: 13.1570\n",
            "🔍 Validation at timestamp: 2021-01-05 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 5.  10.3 15.9 19.3 21.6 20.4  0.  21.3 18.8 15.4 10.2  5.7  0.8  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.9  1.4  0.4  4.   8.9\n",
            "  9.9  0.  20.9 15.   5.8 16.2  2.6  4.7  1.1  0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.5]\n",
            "True Values (Rounded):\n",
            "[ 5.  10.3 15.9 19.3 21.6 20.4  0.  21.3 18.8 15.4 10.2  5.7  0.8  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.9  1.4  0.4  4.   8.9\n",
            "  9.9  0.  20.9 15.   5.8 16.2  2.6  4.7  1.1  0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.5]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=48, CL=96, Model=LSTM\n",
            "Length of: RMSE=236, MAE=236, SMAPE=236, R2=236\n",
            "Mean of: RMSE=3.5918, MAE=2.5016, SMAPE=122.60%, R^2=0.3292\n",
            "Median of: RMSE=3.3003, MAE=2.4001, SMAPE=126.06%, R^2=0.6487\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.6019\n",
            "Epoch 2/25, Loss: 39.8290\n",
            "Epoch 3/25, Loss: 39.8592\n",
            "Epoch 4/25, Loss: 39.8712\n",
            "Epoch 5/25, Loss: 39.8897\n",
            "Epoch 6/25, Loss: 39.8937\n",
            "Epoch 7/25, Loss: 39.8959\n",
            "Epoch 8/25, Loss: 39.8919\n",
            "Epoch 9/25, Loss: 39.8732\n",
            "Epoch 10/25, Loss: 39.4943\n",
            "Epoch 11/25, Loss: 38.0107\n",
            "Epoch 12/25, Loss: 33.0839\n",
            "Epoch 13/25, Loss: 28.2196\n",
            "Epoch 14/25, Loss: 23.7766\n",
            "Epoch 15/25, Loss: 21.0223\n",
            "Epoch 16/25, Loss: 19.8387\n",
            "Epoch 17/25, Loss: 19.1543\n",
            "Epoch 18/25, Loss: 18.5762\n",
            "Epoch 19/25, Loss: 17.9588\n",
            "Epoch 20/25, Loss: 17.2796\n",
            "Epoch 21/25, Loss: 16.5436\n",
            "Epoch 22/25, Loss: 15.8707\n",
            "Epoch 23/25, Loss: 15.3121\n",
            "Epoch 24/25, Loss: 14.8374\n",
            "Epoch 25/25, Loss: 14.4897\n",
            "🔍 Validation at timestamp: 2021-01-09 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 5.7 11.2  0.   0.  20.8  0.  20.3 19.7 17.8 14.6  9.8  4.4  0.8  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.4  5.6 11.  15.5 18.2\n",
            " 19.4 19.7 19.8 19.  17.  14.1  9.6  4.3  0.6  0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.2]\n",
            "True Values (Rounded):\n",
            "[ 5.7 11.2  0.   0.  20.8  0.  20.3 19.7 17.8 14.6  9.8  4.4  0.8  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.4  5.6 11.  15.5 18.2\n",
            " 19.4 19.7 19.8 19.  17.  14.1  9.6  4.3  0.6  0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.2]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=48, CL=192, Model=LSTM\n",
            "Length of: RMSE=234, MAE=234, SMAPE=234, R2=234\n",
            "Mean of: RMSE=3.7554, MAE=2.7044, SMAPE=109.16%, R^2=0.2333\n",
            "Median of: RMSE=3.5571, MAE=2.6313, SMAPE=114.77%, R^2=0.6176\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.2332\n",
            "Epoch 2/25, Loss: 39.8662\n",
            "Epoch 3/25, Loss: 39.9283\n",
            "Epoch 4/25, Loss: 39.9420\n",
            "Epoch 5/25, Loss: 39.9491\n",
            "Epoch 6/25, Loss: 39.9522\n",
            "Epoch 7/25, Loss: 39.8718\n",
            "Epoch 8/25, Loss: 38.1034\n",
            "Epoch 9/25, Loss: 33.2208\n",
            "Epoch 10/25, Loss: 28.0817\n",
            "Epoch 11/25, Loss: 24.2619\n",
            "Epoch 12/25, Loss: 21.8394\n",
            "Epoch 13/25, Loss: 20.6823\n",
            "Epoch 14/25, Loss: 19.8751\n",
            "Epoch 15/25, Loss: 19.1723\n",
            "Epoch 16/25, Loss: 18.6313\n",
            "Epoch 17/25, Loss: 18.0910\n",
            "Epoch 18/25, Loss: 17.5381\n",
            "Epoch 19/25, Loss: 16.9044\n",
            "Epoch 20/25, Loss: 16.2561\n",
            "Epoch 21/25, Loss: 15.6785\n",
            "Epoch 22/25, Loss: 15.2003\n",
            "Epoch 23/25, Loss: 14.7804\n",
            "Epoch 24/25, Loss: 14.4587\n",
            "Epoch 25/25, Loss: 14.1702\n",
            "🔍 Validation at timestamp: 2021-01-17 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 5.3  0.  16.  19.4 21.7  0.  21.7 21.2 18.6 15.2 10.3  4.6  0.9  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.1  5.2  0.  15.5 19.1\n",
            " 21.   0.   0.   0.  18.6 15.1 10.3  4.6  0.8  0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.1]\n",
            "True Values (Rounded):\n",
            "[ 5.3  0.  16.  19.4 21.7  0.  21.7 21.2 18.6 15.2 10.3  4.6  0.9  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.1  5.2  0.  15.5 19.1\n",
            " 21.   0.   0.   0.  18.6 15.1 10.3  4.6  0.8  0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.1]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=48, CL=384, Model=LSTM\n",
            "Length of: RMSE=230, MAE=230, SMAPE=230, R2=230\n",
            "Mean of: RMSE=3.8585, MAE=2.7361, SMAPE=108.68%, R^2=0.2092\n",
            "Median of: RMSE=3.6531, MAE=2.6357, SMAPE=97.20%, R^2=0.5655\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.8086\n",
            "Epoch 2/25, Loss: 39.2867\n",
            "Epoch 3/25, Loss: 37.9812\n",
            "Epoch 4/25, Loss: 35.7160\n",
            "Epoch 5/25, Loss: 33.6241\n",
            "Epoch 6/25, Loss: 29.2189\n",
            "Epoch 7/25, Loss: 24.7055\n",
            "Epoch 8/25, Loss: 22.0918\n",
            "Epoch 9/25, Loss: 20.5703\n",
            "Epoch 10/25, Loss: 19.7540\n",
            "Epoch 11/25, Loss: 19.3271\n",
            "Epoch 12/25, Loss: 18.9952\n",
            "Epoch 13/25, Loss: 18.6651\n",
            "Epoch 14/25, Loss: 18.3241\n",
            "Epoch 15/25, Loss: 17.9565\n",
            "Epoch 16/25, Loss: 17.5495\n",
            "Epoch 17/25, Loss: 17.1546\n",
            "Epoch 18/25, Loss: 16.7893\n",
            "Epoch 19/25, Loss: 16.4586\n",
            "Epoch 20/25, Loss: 16.1809\n",
            "Epoch 21/25, Loss: 15.9417\n",
            "Epoch 22/25, Loss: 15.7159\n",
            "Epoch 23/25, Loss: 15.5071\n",
            "Epoch 24/25, Loss: 15.3345\n",
            "Epoch 25/25, Loss: 15.1935\n",
            "🔍 Validation at timestamp: 2021-01-05 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 5.  10.3 15.9 19.3 21.6 20.4  0.  21.3 18.8 15.4 10.2  5.7  0.8  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.9  1.4  0.4  4.   8.9\n",
            "  9.9  0.  20.9 15.   5.8 16.2  2.6  4.7  1.1  0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.5  6.1 11.8 16.6 19.9 21.7 20.2  0.  21.2\n",
            " 19.2 15.6  9.9  4.6  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   6.  11.8 16.3 19.5 21.1 21.7 21.2 20.3 18.4 14.9 10.2  4.6\n",
            "  0.9  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.4]\n",
            "True Values (Rounded):\n",
            "[ 5.  10.3 15.9 19.3 21.6 20.4  0.  21.3 18.8 15.4 10.2  5.7  0.8  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.9  1.4  0.4  4.   8.9\n",
            "  9.9  0.  20.9 15.   5.8 16.2  2.6  4.7  1.1  0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.5  6.1 11.8 16.6 19.9 21.7 20.2  0.  21.2\n",
            " 19.2 15.6  9.9  4.6  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.   6.  11.8 16.3 19.5 21.1 21.7 21.2 20.3 18.4 14.9 10.2  4.6\n",
            "  0.9  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.4]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=96, CL=96, Model=LSTM\n",
            "Length of: RMSE=118, MAE=118, SMAPE=118, R2=118\n",
            "Mean of: RMSE=3.9448, MAE=2.8386, SMAPE=108.92%, R^2=0.3474\n",
            "Median of: RMSE=3.8673, MAE=2.7438, SMAPE=126.21%, R^2=0.5850\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.7149\n",
            "Epoch 2/25, Loss: 39.8771\n",
            "Epoch 3/25, Loss: 39.9065\n",
            "Epoch 4/25, Loss: 39.9183\n",
            "Epoch 5/25, Loss: 39.9258\n",
            "Epoch 6/25, Loss: 39.9383\n",
            "Epoch 7/25, Loss: 39.9408\n",
            "Epoch 8/25, Loss: 39.9427\n",
            "Epoch 9/25, Loss: 39.9439\n",
            "Epoch 10/25, Loss: 39.9450\n",
            "Epoch 11/25, Loss: 39.9455\n",
            "Epoch 12/25, Loss: 39.9399\n",
            "Epoch 13/25, Loss: 39.8812\n",
            "Epoch 14/25, Loss: 39.9482\n",
            "Epoch 15/25, Loss: 39.9415\n",
            "Epoch 16/25, Loss: 39.9147\n",
            "Epoch 17/25, Loss: 38.3131\n",
            "Epoch 18/25, Loss: 33.1512\n",
            "Epoch 19/25, Loss: 27.3749\n",
            "Epoch 20/25, Loss: 23.5285\n",
            "Epoch 21/25, Loss: 21.3634\n",
            "Epoch 22/25, Loss: 20.2710\n",
            "Epoch 23/25, Loss: 19.6799\n",
            "Epoch 24/25, Loss: 19.2637\n",
            "Epoch 25/25, Loss: 18.8369\n",
            "🔍 Validation at timestamp: 2021-01-09 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 5.7 11.2  0.   0.  20.8  0.  20.3 19.7 17.8 14.6  9.8  4.4  0.8  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.4  5.6 11.  15.5 18.2\n",
            " 19.4 19.7 19.8 19.  17.  14.1  9.6  4.3  0.6  0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.2  2.1 10.7 15.2 18.2 19.3 18.9 20.3 13.1\n",
            " 18.2 13.9  4.7  3.3  0.6  0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.5  5.3 10.6 15.2 18.  19.6 19.3 19.3 14.1 14.5 15.8  7.3  4.3\n",
            "  0.9  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.2]\n",
            "True Values (Rounded):\n",
            "[ 5.7 11.2  0.   0.  20.8  0.  20.3 19.7 17.8 14.6  9.8  4.4  0.8  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.4  5.6 11.  15.5 18.2\n",
            " 19.4 19.7 19.8 19.  17.  14.1  9.6  4.3  0.6  0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.2  2.1 10.7 15.2 18.2 19.3 18.9 20.3 13.1\n",
            " 18.2 13.9  4.7  3.3  0.6  0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.5  5.3 10.6 15.2 18.  19.6 19.3 19.3 14.1 14.5 15.8  7.3  4.3\n",
            "  0.9  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.2]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=96, CL=192, Model=LSTM\n",
            "Length of: RMSE=117, MAE=117, SMAPE=117, R2=117\n",
            "Mean of: RMSE=4.6992, MAE=3.3965, SMAPE=99.75%, R^2=-0.0053\n",
            "Median of: RMSE=4.6986, MAE=3.2723, SMAPE=90.65%, R^2=0.4979\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 44.4832\n",
            "Epoch 2/25, Loss: 39.9314\n",
            "Epoch 3/25, Loss: 39.9869\n",
            "Epoch 4/25, Loss: 40.0013\n",
            "Epoch 5/25, Loss: 40.0093\n",
            "Epoch 6/25, Loss: 40.0141\n",
            "Epoch 7/25, Loss: 40.0173\n",
            "Epoch 8/25, Loss: 40.0200\n",
            "Epoch 9/25, Loss: 40.0218\n",
            "Epoch 10/25, Loss: 40.0232\n",
            "Epoch 11/25, Loss: 40.0243\n",
            "Epoch 12/25, Loss: 40.0252\n",
            "Epoch 13/25, Loss: 40.0259\n",
            "Epoch 14/25, Loss: 40.0261\n",
            "Epoch 15/25, Loss: 40.0268\n",
            "Epoch 16/25, Loss: 40.0268\n",
            "Epoch 17/25, Loss: 40.0273\n",
            "Epoch 18/25, Loss: 40.0242\n",
            "Epoch 19/25, Loss: 40.0105\n",
            "Epoch 20/25, Loss: 39.7965\n",
            "Epoch 21/25, Loss: 39.6757\n",
            "Epoch 22/25, Loss: 38.0798\n",
            "Epoch 23/25, Loss: 34.3154\n",
            "Epoch 24/25, Loss: 30.1187\n",
            "Epoch 25/25, Loss: 26.3533\n",
            "🔍 Validation at timestamp: 2021-01-17 08:00:00\n",
            "Predicted Values (Rounded):\n",
            "[ 5.3  0.  16.  19.4 21.7  0.  21.7 21.2 18.6 15.2 10.3  4.6  0.9  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.1  5.2  0.  15.5 19.1\n",
            " 21.   0.   0.   0.  18.6 15.1 10.3  4.6  0.8  0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.1  5.3 11.  15.9 19.2 21.1  0.  20.7 20.2\n",
            " 18.1 15.2 10.3  4.5  0.8  0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.9  5.  10.7 15.6 18.9 21.  21.6 21.3 20.7  0.  15.  10.2  4.5\n",
            "  0.8  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.8]\n",
            "True Values (Rounded):\n",
            "[ 5.3  0.  16.  19.4 21.7  0.  21.7 21.2 18.6 15.2 10.3  4.6  0.9  0.\n",
            "  0.   0.   0.   0.   0.   0.   0.   0.   0.   1.1  5.2  0.  15.5 19.1\n",
            " 21.   0.   0.   0.  18.6 15.1 10.3  4.6  0.8  0.   0.   0.   0.   0.\n",
            "  0.   0.   0.   0.   0.   1.1  5.3 11.  15.9 19.2 21.1  0.  20.7 20.2\n",
            " 18.1 15.2 10.3  4.5  0.8  0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
            "  0.   0.9  5.  10.7 15.6 18.9 21.  21.6 21.3 20.7  0.  15.  10.2  4.5\n",
            "  0.8  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.8]\n",
            "\n",
            "✅ Validation Passed\n",
            "Metrics ---------------------------------------------------------\n",
            "PL=96, CL=384, Model=LSTM\n",
            "Length of: RMSE=115, MAE=115, SMAPE=115, R2=115\n",
            "Mean of: RMSE=4.8545, MAE=3.9370, SMAPE=140.58%, R^2=0.1935\n",
            "Median of: RMSE=4.8101, MAE=3.8659, SMAPE=138.84%, R^2=0.3677\n",
            "\n",
            "LSTM model - Main block\n",
            "Epoch 1/25, Loss: 43.4709\n",
            "Epoch 2/25, Loss: 39.5952\n",
            "Epoch 3/25, Loss: 39.6566\n",
            "Epoch 4/25, Loss: 39.6518\n",
            "Epoch 5/25, Loss: 39.5604\n",
            "Epoch 6/25, Loss: 38.6514\n",
            "Epoch 7/25, Loss: 36.8970\n",
            "Epoch 8/25, Loss: 34.1751\n"
          ]
        }
      ],
      "source": [
        "# Main block - run for several iterations\n",
        "\n",
        "rmse_list = []\n",
        "mae_list = []\n",
        "smape_list = []\n",
        "r2_list = []\n",
        "\n",
        "for prediction_length in sorted(PREDICTION_LENGTH_LIST):\n",
        "  for context_length_fold in sorted(CONTEXT_LENGTH_FOLD_LIST):\n",
        "    PREDICTION_LENGTH = prediction_length\n",
        "    CONTEXT_LENGTH = context_length_fold * PREDICTION_LENGTH\n",
        "\n",
        "    if MODEL_TYPE == \"LSTM\":\n",
        "      print(\"LSTM model - Main block\")\n",
        "\n",
        "      # Step 3: Create sequences\n",
        "      X_seq, y_seq = create_sequences(X_scaled, y, CONTEXT_LENGTH, PREDICTION_LENGTH)\n",
        "\n",
        "      # Step 4: Split data into training and testing sets\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, train_size=TRAIN_DATA_LAST_INDEX, random_state=None, shuffle=False)\n",
        "      # Convert data to PyTorch tensors\n",
        "      X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "      y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "      X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "      y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "      # Step 5: Create DataLoader for batch processing\n",
        "      train_dataset = TensorDataset(X_train, y_train)\n",
        "      test_dataset = TensorDataset(X_test, y_test)\n",
        "      train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "      test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "      # Step 6: Create the model\n",
        "      input_size = X_train.shape[2]  # Number of features -> 7 eg. 'air_temp', 'albedo', etc.\n",
        "      model = LSTMModel(input_size, HIDDEN_SIZE, NUM_LAYERS, output_size=PREDICTION_LENGTH)\n",
        "\n",
        "      # Step 7: Define the loss function and optimizer\n",
        "      criterion = nn.MSELoss()\n",
        "      optimizer = torch.optim.Adam(model.parameters(), LR)\n",
        "\n",
        "      # Step 8: Train the model\n",
        "      LOSS = train_LSTM_model(model, train_loader, criterion, optimizer, NUM_EPOCHS)\n",
        "\n",
        "      # Step 9: Evaluate the model\n",
        "      predictions, actuals = evaluate_LSTM_model(model, test_loader)\n",
        "      data_validation_LSTM(df, actuals, X_train, CONTEXT_LENGTH, PREDICTION_LENGTH, hour=START_HOUR)\n",
        "      INDEX, TIMESTAMP = get_index_and_timestamp_for_validation(START_HOUR, df, train_size=len(X_train) + CONTEXT_LENGTH)\n",
        "      TIMESTAMP_DATE = TIMESTAMP.split(\" \")[0]\n",
        "      actuals_daily = truncate_tensor_with_interval(actuals, INDEX, offset=PREDICTION_LENGTH)\n",
        "      predictions_daily = truncate_tensor_with_interval(predictions, INDEX, offset=PREDICTION_LENGTH)\n",
        "\n",
        "      # Reset list values\n",
        "      rmse_list = []\n",
        "      mae_list = []\n",
        "      smape_list = []\n",
        "      r2_list = []\n",
        "\n",
        "      # Step 10: Calc metrics\n",
        "      for i, row in enumerate(actuals_daily):  # Loop over days in actuals_daily rows\n",
        "        rmse = np.sqrt(np.mean((predictions_daily[i] - actuals_daily[i])**2))\n",
        "        rmse_list.append(rmse)\n",
        "        mae = mean_absolute_error(actuals_daily[i], predictions_daily[i])\n",
        "        mae_list.append(mae)\n",
        "        r2 = r2_score(actuals_daily[i], predictions_daily[i])\n",
        "        r2_list.append(r2)\n",
        "        smape = calc_smape(actuals_daily[i], predictions_daily[i])\n",
        "        smape_list.append(smape)\n",
        "        #print(f\"DATE={PREDICTION_START_DATETIME_STR}, RMSE={rmse:.4f}, MAE={mae:.4f}, SMAPE={smape:.2f}%, R^2={r2:.4f}\")\n",
        "      print(\"Metrics ---------------------------------------------------------\")\n",
        "      print(f\"PL={PREDICTION_LENGTH}, CL={CONTEXT_LENGTH}, Model={MODEL_TYPE}\")\n",
        "      print(f\"Length of: RMSE={len(rmse_list)}, MAE={len(mae_list)}, SMAPE={len(smape_list)}, R2={len(r2_list)}\")\n",
        "      print(f\"Mean of: RMSE={np.mean(rmse_list):.4f}, MAE={np.mean(mae_list):.4f}, SMAPE={np.mean(smape_list):.2f}%, R^2={np.mean(r2_list):.4f}\")\n",
        "      RMSE_MED = np.median(rmse_list)\n",
        "      MAE_MED = np.median(mae_list)\n",
        "      SMAPE_MED = np.median(smape_list)\n",
        "      R2_MED = np.median(r2_list)\n",
        "      print(f\"Median of: RMSE={RMSE_MED:.4f}, MAE={MAE_MED:.4f}, SMAPE={SMAPE_MED:.2f}%, R^2={R2_MED:.4f}\")\n",
        "      print(\"\")\n",
        "\n",
        "      # Step 11: Prepare Plot\n",
        "      # Plot\n",
        "      plt.plot(df[TIMESTAMP:][\"dc_power\"].iloc[:PREDICTION_LENGTH], color=\"black\", linestyle=\"--\", label=\"True values\")\n",
        "      plt.plot(df[TIMESTAMP:].iloc[:PREDICTION_LENGTH].index, predictions_daily[0], color=\"blue\", label=\"Predictions\")\n",
        "\n",
        "      plt.legend(loc=\"upper right\", fontsize=\"small\")\n",
        "      plt.ylabel(\"DC Power (kW)\")\n",
        "      plt.xlabel(\"Date\")\n",
        "      plt.title(f\"{MODEL_TYPE} Forecast (starts {TIMESTAMP})\")\n",
        "      plt.figtext(0.1, 0.9000,f\"RMSE={RMSE_MED:.4f}, MAE={MAE_MED:.4f}, SMAPE={SMAPE_MED:.2f}%, R^2={R2_MED:.4f} (median values over {len(rmse_list)} samples)\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8700,f\"Pre_L={PREDICTION_LENGTH}, Con_L={CONTEXT_LENGTH}, HIDDEN_SIZE={HIDDEN_SIZE}, BATCH={BATCH_SIZE}, LR={LR}, Epochs={NUM_EPOCHS}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8400,f\"Dataset={file_name_no_extension(FILEPATH)}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8100,f\"Features={features}\", fontsize=6, color='black')\n",
        "      #plt.figtext(0.1, 0.7800,f\"Past features={dataset.past_feat_dynamic_real}\", fontsize=6, color='black')\n",
        "\n",
        "      # Set y-axis limits\n",
        "      plt.ylim(0, 12.5)\n",
        "\n",
        "      # Set y-axis limits and x-axis formatting\n",
        "      ax = plt.gca()\n",
        "      ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n",
        "      ax.xaxis.set_major_locator(mdates.DayLocator())\n",
        "      plt.xticks(rotation=25)\n",
        "\n",
        "      # Adjust layout and save plot\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(f\"PL={PREDICTION_LENGTH}-CL={CONTEXT_LENGTH}-RMSE={RMSE_MED:.4f}-{TIMESTAMP_DATE}-{MODEL_TYPE}.pdf\")\n",
        "      #plt.show()\n",
        "\n",
        "    elif MODEL_TYPE == \"DeepAR\":\n",
        "      print(\"DeepAR model type stuff\")\n",
        "\n",
        "      # Step 3: Split data into training and testing sets\n",
        "      TESTDATA_LENGTH = len(df) - TRAIN_DATA_LAST_INDEX\n",
        "\n",
        "      # Step 4: Create test sequences for time-series prediction\n",
        "      training_data, test_gen = split(dataset, offset=-TESTDATA_LENGTH)\n",
        "      test_data = test_gen.generate_instances(prediction_length=PREDICTION_LENGTH)\n",
        "      # Helper\n",
        "      # Calculate timestamp for Test Data starting point\n",
        "      timestamp_test_data = df[len(df)-TESTDATA_LENGTH:].iloc[:1].index.item()\n",
        "      TEST_DATA_START_DATETIME = timestamp_test_data.strftime('%Y-%m-%d %H:%M:%S')\n",
        "      TEST_DATA_START_DATE_STR = TEST_DATA_START_DATETIME.split(\" \")[0]\n",
        "\n",
        "      # Step5: Define and Train the model\n",
        "      model = DeepAREstimator(\n",
        "          prediction_length=PREDICTION_LENGTH, context_length=CONTEXT_LENGTH, freq=dataset.freq,\n",
        "          trainer_kwargs={\"max_epochs\": NUM_EPOCHS},\n",
        "          num_feat_dynamic_real=dataset.num_feat_dynamic_real,\n",
        "          num_layers=NUM_LAYERS,\n",
        "          hidden_size=HIDDEN_SIZE,\n",
        "          lr=LR,\n",
        "      ).train(training_data)\n",
        "\n",
        "      # Reset list values\n",
        "      rmse_list = []\n",
        "      mae_list = []\n",
        "      smape_list = []\n",
        "      r2_list = []\n",
        "      dates = get_values_by_day(df[TRAIN_DATA_LAST_INDEX:], offset=PREDICTION_LENGTH)\n",
        "      INITIAL_DATETIME = sorted(dates)[0]\n",
        "      INITIAL_DATE = INITIAL_DATETIME.split(\" \")[0]\n",
        "\n",
        "      # Loop over all dates in testdata\n",
        "      for date in sorted(dates):\n",
        "        # Step 3: Split data into training and testing sets\n",
        "        # Step 4: Create test sequences for time-series prediction\n",
        "        PREDICTION_START_DATETIME_STR = date\n",
        "        PREDICTION_START_DATE_STR = PREDICTION_START_DATETIME_STR.split(\" \")[0]\n",
        "        training_data, test_gen = split(dataset, date=pd.Period(PREDICTION_START_DATETIME_STR, freq=\"h\"))\n",
        "        test_data = test_gen.generate_instances(prediction_length=PREDICTION_LENGTH, windows=1)\n",
        "\n",
        "        # Step 6: Get probabilistic predictions\n",
        "        forecasts = list(model.predict(test_data.input))\n",
        "\n",
        "        # Step 7: Get point predictions\n",
        "        # Get predictions from forecasts\n",
        "        predictions = forecasts[0].mean_ts\n",
        "\n",
        "        # Feature added here!!!\n",
        "        # Truncate negative predictions to 0\n",
        "        predictions = np.maximum(predictions, 0)\n",
        "\n",
        "        # Get actuals for metric calculations later\n",
        "        actuals = df[timestamp_test_data:].iloc[:PREDICTION_LENGTH][\"dc_power\"]\n",
        "\n",
        "        # Change type\n",
        "        predictions = predictions.to_numpy()\n",
        "        actuals = actuals.to_numpy()\n",
        "\n",
        "        # Step 8: Do evaluations\n",
        "        rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
        "        rmse_list.append(rmse)\n",
        "        mae = mean_absolute_error(actuals, predictions)\n",
        "        mae_list.append(mae)\n",
        "        r2 = r2_score(actuals, predictions)\n",
        "        r2_list.append(r2)\n",
        "        smape = calc_smape(actuals, predictions)\n",
        "        smape_list.append(smape)\n",
        "        #print(f\"DATE={PREDICTION_START_DATETIME_STR}, RMSE={rmse:.4f}, MAE={mae:.4f}, SMAPE={smape:.2f}%, R^2={r2:.4f}\")\n",
        "\n",
        "      print(f\"PL={PREDICTION_LENGTH}, CL={CONTEXT_LENGTH}, Model={MODEL_TYPE}\")\n",
        "      print(f\"Length of: RMSE={len(rmse_list)}, MAE={len(mae_list)}, SMAPE={len(smape_list)}, R2={len(r2_list)}\")\n",
        "      print(f\"Mean of: RMSE={np.mean(rmse_list):.4f}, MAE={np.mean(mae_list):.4f}, SMAPE={np.mean(smape_list):.2f}%, R^2={np.mean(r2_list):.4f}\")\n",
        "      RMSE_MED = np.median(rmse_list)\n",
        "      MAE_MED = np.median(mae_list)\n",
        "      SMAPE_MED = np.median(smape_list)\n",
        "      R2_MED = np.median(r2_list)\n",
        "      print(f\"Median of: RMSE={RMSE_MED:.4f}, MAE={MAE_MED:.4f}, SMAPE={SMAPE_MED:.2f}%, R^2={R2_MED:.4f}\")\n",
        "      print(\"\")\n",
        "\n",
        "      # Prepare Plot\n",
        "      # Get prediction intervals\n",
        "      lower_50 = forecasts[0].quantile(0.25)  # 25th percentile (lower bound)\n",
        "      upper_50 = forecasts[0].quantile(0.75)  # 75th percentile (upper bound)\n",
        "      lower_90 = forecasts[0].quantile(0.05)  # 5th percentile (lower bound)\n",
        "      upper_90 = forecasts[0].quantile(0.95)  # 95th percentile (upper bound)\n",
        "\n",
        "      # Plot filtered data\n",
        "      plt.plot(df[INITIAL_DATETIME:][\"dc_power\"].iloc[:PREDICTION_LENGTH], color=\"black\", linestyle=\"--\", label=\"True values\")\n",
        "      plt.plot(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH].index, predictions[len(predictions)-len(df[PREDICTION_START_DATETIME_STR:].iloc[:PREDICTION_LENGTH]):], color=\"blue\", label=\"Predictions\")\n",
        "\n",
        "      # Add 50% Prediction Interval (Shaded)\n",
        "      plt.fill_between(\n",
        "          df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH].index,\n",
        "          lower_50[len(lower_50)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          upper_50[len(upper_50)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          color=\"blue\", alpha=0.2, label=\"50% Prediction Interval\"\n",
        "      )\n",
        "\n",
        "      # Add 90% Prediction Interval (Shaded)\n",
        "      plt.fill_between(\n",
        "          df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH].index,\n",
        "          lower_90[len(lower_90)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          upper_90[len(upper_90)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          color=\"blue\", alpha=0.075, label=\"90% Prediction Interval\"\n",
        "      )\n",
        "\n",
        "      plt.legend(loc=\"upper right\", fontsize=\"small\")\n",
        "      plt.ylabel(\"DC Power (kW)\")\n",
        "      plt.xlabel(\"Date\")\n",
        "      plt.title(f\"{MODEL_TYPE} Forecast (starts {INITIAL_DATETIME})\")\n",
        "      plt.figtext(0.1, 0.9000,f\"RMSE={RMSE_MED:.4f}, MAE={MAE_MED:.4f}, SMAPE={SMAPE_MED:.2f}%, R^2={R2_MED:.4f} (median values over {len(rmse_list)} samples)\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8700,f\"Pre_L={PREDICTION_LENGTH}, Con_L={CONTEXT_LENGTH}, HIDDEN_SIZE={HIDDEN_SIZE}, RNN_LAYERS={NUM_LAYERS}, LR={LR}, Epochs={NUM_EPOCHS}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8400,f\"Dataset={file_name_no_extension(FILEPATH)}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8100,f\"Features={dataset.feat_dynamic_real}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.7800,f\"Past features={dataset.past_feat_dynamic_real}\", fontsize=6, color='black')\n",
        "\n",
        "      # Set y-axis limits\n",
        "      plt.ylim(0, 12.5)\n",
        "\n",
        "      # Set y-axis limits and x-axis formatting\n",
        "      ax = plt.gca()\n",
        "      ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n",
        "      ax.xaxis.set_major_locator(mdates.DayLocator())\n",
        "      plt.xticks(rotation=25)\n",
        "\n",
        "      # Adjust layout and save plot\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(f\"PL={PREDICTION_LENGTH}-CL={CONTEXT_LENGTH}-RMSE={RMSE_MED:.4f}-{INITIAL_DATE}-{MODEL_TYPE}.pdf\")\n",
        "      #plt.show()\n",
        "\n",
        "    elif MODEL_TYPE == \"MOIRAI\":\n",
        "      print(\"MOIRAI model type stuff\")\n",
        "\n",
        "      # Step 2.1: Prepare pre-trained model by downloading model weights from huggingface hub\n",
        "      if \"moirai-moe\" in MOIRAI_MODEL_STR:\n",
        "        model = MoiraiMoEForecast(\n",
        "          module=MoiraiMoEModule.from_pretrained(MOIRAI_MODEL_STR),\n",
        "          prediction_length=PREDICTION_LENGTH,\n",
        "          context_length=CONTEXT_LENGTH,\n",
        "          patch_size=PATCH_SIZE,\n",
        "          num_samples=100,\n",
        "          target_dim=1,\n",
        "          feat_dynamic_real_dim=dataset.num_feat_dynamic_real,\n",
        "          past_feat_dynamic_real_dim=dataset.num_past_feat_dynamic_real,\n",
        "        )\n",
        "        #print(\"MOE used\")\n",
        "      else:\n",
        "        model = MoiraiForecast(\n",
        "          module=MoiraiModule.from_pretrained(MOIRAI_MODEL_STR),\n",
        "          prediction_length=PREDICTION_LENGTH,\n",
        "          context_length=CONTEXT_LENGTH,\n",
        "          patch_size=PATCH_SIZE,\n",
        "          num_samples=100,\n",
        "          target_dim=1,\n",
        "          feat_dynamic_real_dim=dataset.num_feat_dynamic_real,\n",
        "          past_feat_dynamic_real_dim=dataset.num_past_feat_dynamic_real,\n",
        "        )\n",
        "        #print(\"No MOE used\")\n",
        "\n",
        "      '''\n",
        "\n",
        "      # Step 2.2: Prepare fine-tuned model by loading model weights from ckpt file\n",
        "      checkpoint_path = \"/content/last.ckpt\"\n",
        "\n",
        "      # Load the model\n",
        "      model = MoiraiForecast.load_from_checkpoint(checkpoint_path,\n",
        "      prediction_length=PREDICTION_LENGTH,\n",
        "      context_length=CONTEXT_LENGTH,\n",
        "      patch_size=PATCH_SIZE,\n",
        "      num_samples=100,\n",
        "      target_dim=1,\n",
        "      feat_dynamic_real_dim=dataset.num_feat_dynamic_real,\n",
        "      past_feat_dynamic_real_dim=dataset.num_past_feat_dynamic_real)\n",
        "      '''\n",
        "\n",
        "      # Reset list values\n",
        "      rmse_list = []\n",
        "      mae_list = []\n",
        "      smape_list = []\n",
        "      r2_list = []\n",
        "      dates = get_values_by_day(df, offset=PREDICTION_LENGTH)\n",
        "      INITIAL_DATETIME = sorted(dates)[0]\n",
        "      INITIAL_DATE = INITIAL_DATETIME.split(\" \")[0]\n",
        "\n",
        "      # Loop over all dates in testdata\n",
        "      for date in sorted(dates):\n",
        "        # Step 3: Split data into training and testing sets\n",
        "        # Step 4: Create test sequences for time-series prediction\n",
        "        PREDICTION_START_DATETIME_STR = date\n",
        "        PREDICTION_START_DATE_STR = PREDICTION_START_DATETIME_STR.split(\" \")[0]\n",
        "        training_data, test_gen = split(dataset, date=pd.Period(PREDICTION_START_DATETIME_STR, freq=\"h\"))\n",
        "        test_data = test_gen.generate_instances(prediction_length=PREDICTION_LENGTH, windows=1)\n",
        "\n",
        "        # Step 6: Get probabilistic predictions\n",
        "        predictor = model.create_predictor(batch_size=BATCH_SIZE)\n",
        "        forecasts = list(predictor.predict(test_data.input))\n",
        "\n",
        "        # Step 7: Get point predictions\n",
        "        # Get predictions from forecasts\n",
        "        predictions = forecasts[0].mean_ts\n",
        "        # Feature added here!!!\n",
        "        # Truncate negative predictions to 0\n",
        "        predictions = np.maximum(predictions, 0)\n",
        "\n",
        "        # Get actuals for metric calculations later\n",
        "        actuals = df.loc[PREDICTION_START_DATETIME_STR:].iloc[:PREDICTION_LENGTH][\"dc_power\"]\n",
        "\n",
        "        # Change type\n",
        "        predictions = predictions.to_numpy()\n",
        "        actuals = actuals.to_numpy()\n",
        "\n",
        "        # Step 8: Do evaluations\n",
        "        rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
        "        rmse_list.append(rmse)\n",
        "        mae = mean_absolute_error(actuals, predictions)\n",
        "        mae_list.append(mae)\n",
        "        r2 = r2_score(actuals, predictions)\n",
        "        r2_list.append(r2)\n",
        "        smape = calc_smape(actuals, predictions)\n",
        "        smape_list.append(smape)\n",
        "        #print(f\"DATE={PREDICTION_START_DATETIME_STR}, RMSE={rmse:.4f}, MAE={mae:.4f}, SMAPE={smape:.2f}%, R^2={r2:.4f}\")\n",
        "\n",
        "      print(f\"PL={PREDICTION_LENGTH}, CL={CONTEXT_LENGTH}, Model={MOIRAI_MODEL_PATH_SAFE}\")\n",
        "      print(f\"Length of: RMSE={len(rmse_list)}, MAE={len(mae_list)}, SMAPE={len(smape_list)}, R2={len(r2_list)}\")\n",
        "      print(f\"Mean of: RMSE={np.mean(rmse_list):.4f}, MAE={np.mean(mae_list):.4f}, SMAPE={np.mean(smape_list):.2f}%, R^2={np.mean(r2_list):.4f}\")\n",
        "      RMSE_MED = np.median(rmse_list)\n",
        "      MAE_MED = np.median(mae_list)\n",
        "      SMAPE_MED = np.median(smape_list)\n",
        "      R2_MED = np.median(r2_list)\n",
        "      print(f\"Median of: RMSE={RMSE_MED:.4f}, MAE={MAE_MED:.4f}, SMAPE={SMAPE_MED:.2f}%, R^2={R2_MED:.4f}\")\n",
        "      print(\"\")\n",
        "\n",
        "\n",
        "      # Prepare Plot\n",
        "      # Get prediction intervals\n",
        "      lower_50 = forecasts[0].quantile(0.25)  # 25th percentile (lower bound)\n",
        "      upper_50 = forecasts[0].quantile(0.75)  # 75th percentile (upper bound)\n",
        "      lower_90 = forecasts[0].quantile(0.05)  # 5th percentile (lower bound)\n",
        "      upper_90 = forecasts[0].quantile(0.95)  # 95th percentile (upper bound)\n",
        "\n",
        "      # Plot filtered data\n",
        "      plt.plot(df[INITIAL_DATETIME:][\"dc_power\"].iloc[:PREDICTION_LENGTH], color=\"black\", linestyle=\"--\", label=\"True values\")\n",
        "      plt.plot(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH].index, predictions[len(predictions)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):], color=\"blue\", label=\"Predictions\")\n",
        "\n",
        "      # Add 50% Prediction Interval (Shaded)\n",
        "      plt.fill_between(\n",
        "          df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH].index,\n",
        "          lower_50[len(lower_50)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          upper_50[len(upper_50)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          color=\"blue\", alpha=0.2, label=\"50% Prediction Interval\"\n",
        "      )\n",
        "\n",
        "      # Add 90% Prediction Interval (Shaded)\n",
        "      plt.fill_between(\n",
        "          df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH].index,\n",
        "          lower_90[len(lower_90)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          upper_90[len(upper_90)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          color=\"blue\", alpha=0.075, label=\"90% Prediction Interval\"\n",
        "      )\n",
        "\n",
        "      plt.legend(loc=\"upper right\", fontsize=\"small\")\n",
        "      plt.ylabel(\"DC Power (kW)\")\n",
        "      plt.xlabel(\"Date\")\n",
        "      plt.title(f\"{MOIRAI_MODEL_PATH_SAFE} Forecast (starts {INITIAL_DATETIME})\")\n",
        "      plt.figtext(0.1, 0.9000,f\"RMSE={RMSE_MED:.4f}, MAE={MAE_MED:.4f}, SMAPE={SMAPE_MED:.2f}%, R^2={R2_MED:.4f} (median values over {len(rmse_list)} samples)\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8700,f\"Pre_L={PREDICTION_LENGTH}, Con_L={CONTEXT_LENGTH}, PATCH={PATCH_SIZE}, BATCH={BATCH_SIZE}, LR={LR}, Epochs={NUM_EPOCHS}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8400,f\"Dataset={file_name_no_extension(FILEPATH)}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8100,f\"Features={dataset.feat_dynamic_real}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.7800,f\"Past features={dataset.past_feat_dynamic_real}\", fontsize=6, color='black')\n",
        "\n",
        "      # Set y-axis limits\n",
        "      plt.ylim(0, 12.5)\n",
        "\n",
        "      # Set y-axis limits and x-axis formatting\n",
        "      ax = plt.gca()\n",
        "      ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n",
        "      ax.xaxis.set_major_locator(mdates.DayLocator())\n",
        "      plt.xticks(rotation=25)\n",
        "\n",
        "      # Adjust layout and save plot\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(f\"PL={PREDICTION_LENGTH}-CL={CONTEXT_LENGTH}-RMSE={RMSE_MED:.4f}-{INITIAL_DATE}-{MOIRAI_MODEL_PATH_SAFE}.pdf\")\n",
        "      #plt.show()\n",
        "\n",
        "    else:\n",
        "      print(\"Invalid model type\")\n",
        "\n",
        "    # Reset plot before next round\n",
        "    plt.figure().clear()\n",
        "    plt.close()\n",
        "    plt.cla()\n",
        "    plt.clf()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5WvOywrXWbW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Step5.2: Prepare fine-tuned model by loading model weights from ckpt file\n",
        "checkpoint_path = \"/content/multirun/2025-02-11/15-40-14/0/checkpoints/epoch=13-step=1400.ckpt\"\n",
        "\n",
        "# Load the model\n",
        "model = MoiraiForecast.load_from_checkpoint(checkpoint_path,\n",
        "      prediction_length=PREDICTION_LENGTH,\n",
        "      context_length=CONTEXT_LENGTH,\n",
        "      patch_size=PATCH_SIZE,\n",
        "      num_samples=100,\n",
        "      target_dim=1,\n",
        "      feat_dynamic_real_dim=dataset.num_feat_dynamic_real,\n",
        "      past_feat_dynamic_real_dim=dataset.num_past_feat_dynamic_real)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN6y68zkhZS2"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning in regards to https://github.com/SalesforceAIResearch/uni2ts/blob/main/README.md#fine-tuning\n",
        "# Step 1 Set Data Path Directory\n",
        "!echo \"CUSTOM_DATA_PATH=/content/uni2ts/\" >> .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es3PtupPF-OB"
      },
      "outputs": [],
      "source": [
        "!echo \"PYTHONPATH=/content/uni2ts\" >> .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9NAOCxbYZwC",
        "outputId": "d60a0ab3-f3cd-41e4-873d-ec18a6e01367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUSTOM_DATA_PATH=/content/uni2ts/\n",
            "PYTHONPATH=/content/uni2ts\n"
          ]
        }
      ],
      "source": [
        "!cat .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogaNfoMChlvJ",
        "outputId": "a4ae6c56-0db1-4c82-e8b5-2e04a9bbb5de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inferred frequency: H. Using this value for the 'freq' parameter.\n",
            "Generating train split: 1 examples [00:00,  4.58 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 1/1 [00:00<00:00, 263.00 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 2.1 process dataset\n",
        "!python -m uni2ts.data.builder.simple customdataset /content/uni2ts/230401_250108_PT1H_Solcast_reduced_features.csv --dataset_type wide_multivariate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsF2EKXEktSI",
        "outputId": "048f19bc-a972-42e2-f7df-72612a5907b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     dc_power  air_temp  azimuth  cloud_opacity  dhi  dni  ghi  zenith\n",
            "date                                                                                  \n",
            "2023-04-01 01:00:00       0.0         8      -27           37.4    0    0    0     125\n",
            "2023-04-01 02:00:00       0.0         8      -43           46.6    0    0    0     120\n",
            "2023-04-01 03:00:00       0.0         8      -57           40.6    0    0    0     112\n",
            "2023-04-01 04:00:00       0.0         7      -70           47.6    0    0    0     103\n",
            "2023-04-01 05:00:00       0.0         7      -81           58.2    0    0    0      93\n",
            "...                       ...       ...      ...            ...  ...  ...  ...     ...\n",
            "2024-08-31 03:00:00       0.0        19      -55            0.0    0    0    0     107\n",
            "2024-08-31 04:00:00       0.0        19      -67           16.5    0    0    0      98\n",
            "2024-08-31 05:00:00       0.0        19      -79           39.8    7    0    7      88\n",
            "2024-08-31 06:00:00       0.0        19      -90           23.9   94   47  104      78\n",
            "2024-08-31 07:00:00       0.1        21     -101            0.0   89  575  303      68\n",
            "\n",
            "[12439 rows x 8 columns]\n",
            "Inferred frequency: H. Using this value for the 'freq' parameter.\n",
            "Generating train split: 8 examples [00:00, 49.89 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 8/8 [00:00<00:00, 2004.21 examples/s]\n",
            "                     dc_power  air_temp  azimuth  cloud_opacity  dhi  dni  ghi  zenith\n",
            "date                                                                                  \n",
            "2023-04-01 01:00:00       0.0         8      -27           37.4    0    0    0     125\n",
            "2023-04-01 02:00:00       0.0         8      -43           46.6    0    0    0     120\n",
            "2023-04-01 03:00:00       0.0         8      -57           40.6    0    0    0     112\n",
            "2023-04-01 04:00:00       0.0         7      -70           47.6    0    0    0     103\n",
            "2023-04-01 05:00:00       0.0         7      -81           58.2    0    0    0      93\n",
            "...                       ...       ...      ...            ...  ...  ...  ...     ...\n",
            "2025-01-07 20:00:00       0.0         1       79           32.1    0    0    0     131\n",
            "2025-01-07 21:00:00       0.0         1       65           73.2    0    0    0     140\n",
            "2025-01-07 22:00:00       0.0         2       45           33.5    0    0    0     149\n",
            "2025-01-07 23:00:00       0.0         1       18           40.6    0    0    0     154\n",
            "2025-01-08 00:00:00       0.0         2      -14           51.0    0    0    0     155\n",
            "\n",
            "[15552 rows x 8 columns]\n",
            "Inferred frequency: H. Using this value for the 'freq' parameter.\n",
            "Generating train split: 8 examples [00:00, 52.27 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 8/8 [00:00<00:00, 2196.11 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 2.2 Set offset\n",
        "!python -m uni2ts.data.builder.simple customdataset /content/uni2ts/230401_250108_PT1H_Solcast_reduced_features.csv --date_offset '2024-08-31 07:00:00'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "903wnpN-PDG2"
      },
      "outputs": [],
      "source": [
        "#!mv content/uni2ts/cli content/uni2ts/src/uni2ts/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqmjyQbeCqjU"
      },
      "outputs": [],
      "source": [
        "# Step 3 (move cli dir to src/uni2ts first!!!!)\n",
        "# Set Batch size here /content/uni2ts/src/uni2ts/cli/conf/finetune/default.yaml to lower value\n",
        "# For moirai large with A100 40GB RAM use 16 as batch size in val_dataloader and train_dataloader section\n",
        "\n",
        "#!python -m uni2ts.cli.train -cp conf/finetune run_name=example_run model=moirai_1.1_R_large data=etth1 val_data=etth1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z-quJtw5FZL",
        "outputId": "32c2b9eb-5eda-409b-b738-0c7dfc50fa08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-02-11 15:40:14,959][HYDRA] Launching 1 jobs locally\n",
            "[2025-02-11 15:40:14,959][HYDRA] \t#0 : run_name=example_run model=moirai_1.1_R_base data=customdataset val_data=customdataset model.module_kwargs.dropout_p=0.2 trainer.max_epochs=25 model.lr=1e-07 train_dataloader.batch_size=24 val_dataloader.batch_size=24\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "[2025-02-11 15:40:16,903][datasets][INFO] - PyTorch version 2.4.1 available.\n",
            "[2025-02-11 15:40:16,903][datasets][INFO] - TensorFlow version 2.18.0 available.\n",
            "[2025-02-11 15:40:16,904][datasets][INFO] - JAX version 0.4.33 available.\n",
            "Seed set to 0\n",
            "2025-02-11 15:40:17.397359: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739288417.422296   43645 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739288417.429823   43645 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name   | Type         | Params | Mode \n",
            "------------------------------------------------\n",
            "0 | module | MoiraiModule | 91.4 M | train\n",
            "------------------------------------------------\n",
            "91.4 M    Trainable params\n",
            "0         Non-trainable params\n",
            "91.4 M    Total params\n",
            "365.431   Total estimated model params size (MB)\n",
            "253       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Epoch 0: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.360, train/PackedNLLLoss=3.230]Metric val/PackedNLLLoss improved. New best score: 2.360\n",
            "Epoch 1: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.360, train/PackedNLLLoss=3.230]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.300, train/PackedNLLLoss=3.120]Metric val/PackedNLLLoss improved by 0.059 >= min_delta = 0.0. New best score: 2.302\n",
            "Epoch 2: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.300, train/PackedNLLLoss=3.120]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.270, train/PackedNLLLoss=3.050]Metric val/PackedNLLLoss improved by 0.033 >= min_delta = 0.0. New best score: 2.268\n",
            "Epoch 3: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.270, train/PackedNLLLoss=3.050]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.250, train/PackedNLLLoss=3.000]Metric val/PackedNLLLoss improved by 0.022 >= min_delta = 0.0. New best score: 2.247\n",
            "Epoch 4: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.250, train/PackedNLLLoss=3.000]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.220, train/PackedNLLLoss=2.980]Metric val/PackedNLLLoss improved by 0.022 >= min_delta = 0.0. New best score: 2.225\n",
            "Epoch 5: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.220, train/PackedNLLLoss=2.980]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.210, train/PackedNLLLoss=2.950]Metric val/PackedNLLLoss improved by 0.012 >= min_delta = 0.0. New best score: 2.213\n",
            "Epoch 6: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.210, train/PackedNLLLoss=2.950]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.200, train/PackedNLLLoss=2.930]Metric val/PackedNLLLoss improved by 0.014 >= min_delta = 0.0. New best score: 2.199\n",
            "Epoch 7: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.200, train/PackedNLLLoss=2.930]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.190, train/PackedNLLLoss=2.900]Metric val/PackedNLLLoss improved by 0.013 >= min_delta = 0.0. New best score: 2.186\n",
            "Epoch 8: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.190, train/PackedNLLLoss=2.900]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.170, train/PackedNLLLoss=2.890]Metric val/PackedNLLLoss improved by 0.012 >= min_delta = 0.0. New best score: 2.174\n",
            "Epoch 9: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.170, train/PackedNLLLoss=2.890]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.170, train/PackedNLLLoss=2.870]Metric val/PackedNLLLoss improved by 0.006 >= min_delta = 0.0. New best score: 2.167\n",
            "Epoch 10: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.170, train/PackedNLLLoss=2.870]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.160, train/PackedNLLLoss=2.860]Metric val/PackedNLLLoss improved by 0.004 >= min_delta = 0.0. New best score: 2.163\n",
            "Epoch 11: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.160, train/PackedNLLLoss=2.860]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.160, train/PackedNLLLoss=2.850]Metric val/PackedNLLLoss improved by 0.008 >= min_delta = 0.0. New best score: 2.156\n",
            "Epoch 12: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.160, train/PackedNLLLoss=2.850]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.850]Metric val/PackedNLLLoss improved by 0.004 >= min_delta = 0.0. New best score: 2.152\n",
            "Epoch 13: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.850]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.840]Metric val/PackedNLLLoss improved by 0.004 >= min_delta = 0.0. New best score: 2.148\n",
            "Epoch 14: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.840]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.830]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.830]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.820]Monitored metric val/PackedNLLLoss did not improve in the last 3 records. Best score: 2.148. Signaling Trainer to stop.\n",
            "Epoch 16: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.820]\n",
            "/usr/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 22 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n"
          ]
        }
      ],
      "source": [
        "# Use Hydra's Sweeping Feature for Hyperparameter Search\n",
        "!python -m uni2ts.cli.train --multirun -cp conf/finetune run_name=example_run model=moirai_1.1_R_base data=customdataset val_data=customdataset \\\n",
        "  model.module_kwargs.dropout_p=0.2 \\\n",
        "  trainer.max_epochs=25 \\\n",
        "  model.lr=1e-7 \\\n",
        "  train_dataloader.batch_size=24 \\\n",
        "  val_dataloader.batch_size=24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMAEx64Har1x"
      },
      "outputs": [],
      "source": [
        "# Show Tensorboard with results\n",
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir /content/multirun\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppn9_O3-WJJ0"
      },
      "outputs": [],
      "source": [
        "#Export outputs\n",
        "#!zip -r outputs.zip /content/outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_B_t-0YOvLq"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "#!python -m uni2ts.cli.eval run_name=example_eval_1 model=moirai_1.0_R_small model.patch_size=32 model.context_length=1000 data=etth1_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwrqN3dk8vB2"
      },
      "outputs": [],
      "source": [
        "#import gc\n",
        "\n",
        "# Invoke garbage collector\n",
        "#gc.collect()\n",
        "\n",
        "# Clear GPU cache\n",
        "#torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}