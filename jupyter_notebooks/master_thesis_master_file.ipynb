{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MnSuayJCd541"
      },
      "outputs": [],
      "source": [
        "MODEL_TYPE = \"MOIRAI\" # Choose between LSTM, DeepAR and MOIRAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "N023cFtseXHa",
        "outputId": "817fb472-5d94-4b3e-c96d-b1efdde03df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MOIRAI model type stuff\n",
            "Cloning into 'uni2ts'...\n",
            "remote: Enumerating objects: 959, done.\u001b[K\n",
            "remote: Counting objects: 100% (447/447), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 959 (delta 315), reused 260 (delta 260), pack-reused 512 (from 2)\u001b[K\n",
            "Receiving objects: 100% (959/959), 8.25 MiB | 5.55 MiB/s, done.\n",
            "Resolving deltas: 100% (458/458), done.\n",
            "/content/uni2ts\n",
            "Obtaining file:///content/uni2ts\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets~=2.17.1 (from uni2ts==1.2.0)\n",
            "  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting einops==0.7.* (from uni2ts==1.2.0)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting gluonts~=0.14.3 (from uni2ts==1.2.0)\n",
            "  Downloading gluonts-0.14.4-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (0.28.1)\n",
            "Collecting hydra-core==1.3 (from uni2ts==1.2.0)\n",
            "  Downloading hydra_core-1.3.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: jax[cpu] in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (0.4.33)\n",
            "Collecting jaxtyping~=0.2.24 (from uni2ts==1.2.0)\n",
            "  Downloading jaxtyping-0.2.38-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting lightning>=2.0 (from uni2ts==1.2.0)\n",
            "  Downloading lightning-2.5.0.post0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from uni2ts==1.2.0)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: numpy~=1.26.0 in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (1.26.4)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (3.10.15)\n",
            "Collecting python-dotenv==1.0.0 (from uni2ts==1.2.0)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (0.5.2)\n",
            "Collecting scipy~=1.11.3 (from uni2ts==1.2.0)\n",
            "  Downloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (2.18.0)\n",
            "Collecting torch<2.5,>=2.1 (from uni2ts==1.2.0)\n",
            "  Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting omegaconf~=2.2 (from hydra-core==1.3->uni2ts==1.2.0)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core==1.3->uni2ts==1.2.0)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hydra-core==1.3->uni2ts==1.2.0) (24.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (7.7.1)\n",
            "Collecting jupyter (from uni2ts==1.2.0)\n",
            "  Downloading jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (17.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets~=2.17.1->uni2ts==1.2.0)\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets~=2.17.1->uni2ts==1.2.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (4.67.1)\n",
            "Collecting xxhash (from datasets~=2.17.1->uni2ts==1.2.0)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets~=2.17.1->uni2ts==1.2.0)\n",
            "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (3.11.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (6.0.2)\n",
            "Collecting pandas (from datasets~=2.17.1->uni2ts==1.2.0)\n",
            "  Downloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.11/dist-packages (from gluonts~=0.14.3->uni2ts==1.2.0) (2.10.6)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.11/dist-packages (from gluonts~=0.14.3->uni2ts==1.2.0) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gluonts~=0.14.3->uni2ts==1.2.0) (4.12.2)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping~=0.2.24->uni2ts==1.2.0)\n",
            "  Downloading wadler_lindig-0.1.3-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning>=2.0->uni2ts==1.2.0)\n",
            "  Downloading lightning_utilities-0.12.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning>=2.0->uni2ts==1.2.0)\n",
            "  Downloading torchmetrics-1.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting pytorch-lightning (from lightning>=2.0->uni2ts==1.2.0)\n",
            "  Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<2.5,>=2.1->uni2ts==1.2.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.5,>=2.1->uni2ts==1.2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.5,>=2.1->uni2ts==1.2.0) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.5,>=2.1->uni2ts==1.2.0) (12.5.82)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->uni2ts==1.2.0) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->uni2ts==1.2.0) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->uni2ts==1.2.0) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->uni2ts==1.2.0) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->uni2ts==1.2.0) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->uni2ts==1.2.0) (3.0.13)\n",
            "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /usr/local/lib/python3.11/dist-packages (from jax[cpu]->uni2ts==1.2.0) (0.4.33)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax[cpu]->uni2ts==1.2.0) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax[cpu]->uni2ts==1.2.0) (3.4.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->uni2ts==1.2.0) (6.5.5)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter->uni2ts==1.2.0) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter->uni2ts==1.2.0) (7.16.6)\n",
            "Collecting jupyterlab (from jupyter->uni2ts==1.2.0)\n",
            "  Downloading jupyterlab-4.3.5-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (2.8.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from uni2ts==1.2.0)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (1.70.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (4.25.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (3.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (1.18.3)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (6.4.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (4.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets~=2.17.1->uni2ts==1.2.0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets~=2.17.1->uni2ts==1.2.0) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts~=0.14.3->uni2ts==1.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts~=0.14.3->uni2ts==1.2.0) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets~=2.17.1->uni2ts==1.2.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets~=2.17.1->uni2ts==1.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets~=2.17.1->uni2ts==1.2.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets~=2.17.1->uni2ts==1.2.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->uni2ts==1.2.0) (3.0.2)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (5.10.4)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (1.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->uni2ts==1.2.0) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->uni2ts==1.2.0) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->uni2ts==1.2.0) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->uni2ts==1.2.0) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->uni2ts==1.2.0) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->uni2ts==1.2.0) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->uni2ts==1.2.0) (1.5.1)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading async_lru-2.0.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->uni2ts==1.2.0) (0.28.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->uni2ts==1.2.0) (0.2.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<2.5,>=2.1->uni2ts==1.2.0) (1.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->uni2ts==1.2.0) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->uni2ts==1.2.0) (1.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->uni2ts==1.2.0) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->uni2ts==1.2.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->uni2ts==1.2.0) (0.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook->jupyter->uni2ts==1.2.0) (4.3.6)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0)\n",
            "  Downloading jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter->uni2ts==1.2.0) (21.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (0.4)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->uni2ts==1.2.0) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->uni2ts==1.2.0) (4.23.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook->jupyter->uni2ts==1.2.0) (2.21.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter->uni2ts==1.2.0) (2.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->uni2ts==1.2.0) (1.3.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->uni2ts==1.2.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->uni2ts==1.2.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->uni2ts==1.2.0) (0.22.3)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading python_json_logger-3.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->uni2ts==1.2.0) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->uni2ts==1.2.0) (2.22)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.0-py3-none-any.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.8/153.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gluonts-0.14.4-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxtyping-0.2.38-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.4/56.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.0.post0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m874.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.12.0-py3-none-any.whl (28 kB)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.1-py3-none-any.whl (927 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m927.3/927.3 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.3-py3-none-any.whl (20 kB)\n",
            "Downloading jupyterlab-4.3.5-py3-none-any.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading pytorch_lightning-2.5.0.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.3/819.3 kB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.4-py3-none-any.whl (6.1 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-3.2.1-py3-none-any.whl (14 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: uni2ts, antlr4-python3-runtime\n",
            "  Building editable for uni2ts (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uni2ts: filename=uni2ts-1.2.0-py3-none-any.whl size=14551 sha256=b85b0eb983365fc98a02c6147d824cc69a48de3904986b020574455762232511\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_wgbzwie/wheels/e4/09/f3/135225f0ca94e25a20e8e24c54f78dd98d712c8760e0d0900a\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=02697bb5c625aeaaf7e6c318892a07d3fd5d97a86e2e2268f7f3253f818fa41c\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built uni2ts antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, xxhash, wadler-lindig, uri-template, types-python-dateutil, triton, scipy, rfc3986-validator, rfc3339-validator, python-json-logger, python-dotenv, pyarrow-hotfix, overrides, omegaconf, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, json5, jedi, fsspec, fqdn, einops, dill, async-lru, pandas, nvidia-cusolver-cu12, nvidia-cudnn-cu12, multiprocess, jupyter-server-terminals, jupyter-client, jaxtyping, hydra-core, arrow, torch, isoduration, gluonts, torchmetrics, datasets, pytorch-lightning, jupyter-events, lightning, uni2ts, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.1\n",
            "    Uninstalling einops-0.8.1:\n",
            "      Successfully uninstalled einops-0.8.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.24.0\n",
            "    Uninstalling jupyter-server-1.24.0:\n",
            "      Successfully uninstalled jupyter-server-1.24.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.10.0 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.4.1 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.4.1 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 arrow-1.3.0 async-lru-2.0.4 datasets-2.17.1 dill-0.3.8 einops-0.7.0 fqdn-1.5.1 fsspec-2023.10.0 gluonts-0.14.4 hydra-core-1.3.0 isoduration-20.11.0 jaxtyping-0.2.38 jedi-0.19.2 json5-0.10.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.3.5 jupyterlab-server-2.27.3 lightning-2.5.0.post0 lightning-utilities-0.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 overrides-7.7.0 pandas-2.1.4 pyarrow-hotfix-0.6 python-dotenv-1.0.0 python-json-logger-3.2.1 pytorch-lightning-2.5.0.post0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 scipy-1.11.4 torch-2.4.1 torchmetrics-1.6.1 triton-3.0.0 types-python-dateutil-2.9.0.20241206 uni2ts-1.2.0 uri-template-1.3.0 wadler-lindig-0.1.3 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "e80e2e4ee7874ff1a62f3172044c6330"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initialize Directories and Configurations\n",
        "if MODEL_TYPE == \"LSTM\":\n",
        "  print(\"LSTM model type stuff\")\n",
        "\n",
        "elif MODEL_TYPE == \"DeepAR\":\n",
        "  print(\"DeepAR model type stuff\")\n",
        "  # install with support for torch models\n",
        "  !pip install \"gluonts[torch]\"\n",
        "  # install with support for mxnet models\n",
        "  !pip install \"gluonts[mxnet]\"\n",
        "\n",
        "elif MODEL_TYPE == \"MOIRAI\":\n",
        "  print(\"MOIRAI model type stuff\")\n",
        "  !git clone https://github.com/taschoebli/uni2ts.git\n",
        "  %cd uni2ts\n",
        "  !pip install -e '.[notebook]'\n",
        "  #!pip install uni2ts\n",
        "  !touch .env\n",
        "\n",
        "else:\n",
        "  print(\"Invalid model type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6YkS5WEYYYx",
        "outputId": "02e18761-7cb5-4877-ea20-94d897e2b34e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "General imports loaded\n",
            "MOIRAI specific imports loaded\n"
          ]
        }
      ],
      "source": [
        "#General imports\n",
        "print(\"General imports loaded\")\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "if MODEL_TYPE == \"LSTM\":\n",
        "  print(\"LSTM specific imports loaded\")\n",
        "  from sklearn.preprocessing import StandardScaler\n",
        "  from sklearn.preprocessing import MinMaxScaler\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "elif MODEL_TYPE == \"DeepAR\":\n",
        "  print(\"DeepAR specific imports loaded\")\n",
        "  from gluonts.torch import DeepAREstimator\n",
        "  from gluonts.dataset.pandas import PandasDataset\n",
        "  from gluonts.dataset.split import split\n",
        "\n",
        "elif MODEL_TYPE == \"MOIRAI\":\n",
        "  print(\"MOIRAI specific imports loaded\")\n",
        "  from huggingface_hub import hf_hub_download\n",
        "  from uni2ts.model.moirai import MoiraiForecast, MoiraiModule\n",
        "  from uni2ts.model.moirai_moe import MoiraiMoEForecast, MoiraiMoEModule\n",
        "  from gluonts.dataset.pandas import PandasDataset\n",
        "  from gluonts.dataset.split import split\n",
        "\n",
        "else:\n",
        "  print(\"Invalid model type\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "af2SkhDQZ0Bv"
      },
      "outputs": [],
      "source": [
        "# Functions, Classes and other Helpers\n",
        "\n",
        "# General ----------------------------------------------------------------------\n",
        "\n",
        "# Load data from csv\n",
        "def load_data(filepath):\n",
        "    data = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "    return data\n",
        "\n",
        "# Filename extractor\n",
        "def file_name_no_extension(filepath):\n",
        "  # Extract the filename without the folder path\n",
        "  filename_with_extension = os.path.basename(filepath)\n",
        "\n",
        "  # Remove the `.csv` extension\n",
        "  filename_without_extension = os.path.splitext(filename_with_extension)[0]\n",
        "\n",
        "  return filename_without_extension\n",
        "\n",
        "\n",
        "# Day extractor\n",
        "def get_values_by_day(df_internal, offset=0):\n",
        "  # Ensure index is datetime\n",
        "  if not isinstance(df_internal.index, pd.DatetimeIndex):\n",
        "      df_internal.index = pd.to_datetime(df_internal.index)\n",
        "\n",
        "  # Apply offset by removing the last 'offset' rows\n",
        "  if offset > 0:\n",
        "      df_internal = df_internal.iloc[:-offset]\n",
        "\n",
        "  # Filter timestamps to only include those at exactly 08:00\n",
        "  df_filtered = df_internal[df_internal.index.hour == 8]\n",
        "\n",
        "  # Group by date and store in a dictionary\n",
        "  #result = {}\n",
        "  #for date, group in df_filtered.groupby(df_filtered.index.date):\n",
        "      #result[str(date)] = group.index.tolist()\n",
        "  # Extract timestamps as strings instead of lists\n",
        "  result = {group.index[0].strftime('%Y-%m-%d %H:%M:%S') for date, group in df_filtered.groupby(df_filtered.index.date)}\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "# Calculate SMAPE metric\n",
        "def calc_smape(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculate the Symmetric Mean Absolute Percentage Error (SMAPE).\n",
        "  y_true: array-like, actual values\n",
        "  y_pred: array-like, predicted values\n",
        "  \"\"\"\n",
        "  epsilon = 1e-10  # Small constant to avoid division by zero\n",
        "  denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
        "  smape_values = np.abs(y_true - y_pred) / (denominator + epsilon)\n",
        "  smape_values = np.nan_to_num(smape_values, nan=0.0, posinf=0.0, neginf=0.0)  # Handle division by zero\n",
        "\n",
        "  return np.mean(smape_values) * 100  # Convert to percentage\n",
        "\n",
        "def optimize_memory():\n",
        "  # Memory optimization\n",
        "  # Set CUDA memory management configuration to avoid fragmentation\n",
        "  # https://pytorch.org/docs/stable/notes/cuda.html#using-custom-memory-allocators-for-cuda\n",
        "  os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "  # Check if CUDA is available\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"Using device: {device}\")\n",
        "  torch.cuda.empty_cache()\n",
        "  os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\n",
        "\n",
        "\n",
        "# LSTM specific ----------------------------------------------------------------\n",
        "\n",
        "# Create sequences for time-series prediction\n",
        "def create_sequences(features, target, context_length, prediction_length):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(features) - context_length - prediction_length + 1):\n",
        "        X_seq.append(features[i:i+context_length])  # Past `context_length` values\n",
        "        y_seq.append(target[i+context_length:i+context_length+prediction_length])  # Next `prediction_length` values\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Define the LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        out = self.fc(hidden[-1])  # Use the last hidden state\n",
        "        return out\n",
        "\n",
        "def train_LSTM_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}')\n",
        "    return epoch_loss/len(train_loader)\n",
        "\n",
        "def evaluate_LSTM_model(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            predictions.extend(outputs.tolist())\n",
        "            actuals.extend(y_batch.tolist())\n",
        "    # Feature added here!!!\n",
        "    # Truncate negative predictions to 0\n",
        "    predictions = np.maximum(predictions, 0)\n",
        "    return np.array(predictions), np.array(actuals)\n",
        "\n",
        "def get_index_and_timestamp_for_validation(hour, original_df, train_size):\n",
        "    \"\"\"\n",
        "    Finds the first available testdata tensor index that matches the given hour.\n",
        "\n",
        "    Parameters:\n",
        "    - hour (int): The desired hour (0-23).\n",
        "    - original_df (pd.DataFrame): The original dataset with a datetime index.\n",
        "    - train_size (int): The number of samples in the training dataset + Context Length.\n",
        "\n",
        "    Returns:\n",
        "    - test_index (int): The index relative to testdata tensor.\n",
        "    - timestamp (str): The corresponding datetime string.\n",
        "    \"\"\"\n",
        "    # Ensure datetime format\n",
        "    original_df.index = pd.to_datetime(original_df.index)\n",
        "\n",
        "    # Extract test dataset portion\n",
        "    test_df = original_df.iloc[train_size:]  # This contains only the test set\n",
        "\n",
        "    # Find the first row where the hour matches\n",
        "    for i, timestamp in enumerate(test_df.index):\n",
        "        if timestamp.hour == hour:\n",
        "            test_index = i  # Relative index in testdata tensor\n",
        "            return test_index, timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    raise ValueError(f\"No test data found at hour {hour}:00.\")\n",
        "\n",
        "\n",
        "def truncate_tensor_with_interval(tensor, start_index, offset=1):\n",
        "    \"\"\"\n",
        "    Truncates the given tensor along the first dimension, starting at start_index,\n",
        "    and selects every `offset`-th row.\n",
        "\n",
        "    Parameters:\n",
        "    - tensor (torch.Tensor): The input tensor (shape: [X, Y]).\n",
        "    - start_index (int): The base index from which truncation starts.\n",
        "    - offset (int): Step interval to select elements (e.g., every 4th, 12th, etc.).\n",
        "\n",
        "    Returns:\n",
        "    - truncated_tensor (torch.Tensor): The truncated tensor with selected intervals.\n",
        "    \"\"\"\n",
        "    if start_index >= tensor.shape[0]:\n",
        "        raise ValueError(\"Start index exceeds tensor dimensions.\")\n",
        "\n",
        "    # Make sure that below 24 hours, offset is set to one day!\n",
        "    if offset < 24:\n",
        "      offset = 24\n",
        "\n",
        "    return tensor[start_index::offset]  # Truncate and select every `offset` step\n",
        "\n",
        "def data_validation_LSTM(df, actuals, X_train, CONTEXT_LENGTH, PREDICTION_LENGTH, hour=8):\n",
        "    \"\"\"\n",
        "    Validates the actual LSTM predictions against the true values in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The original dataset with a datetime index.\n",
        "    - actuals (torch.Tensor): The model's predicted values (shape: [X, Y]).\n",
        "    - X_train (numpy array or tensor): The training dataset.\n",
        "    - CONTEXT_LENGTH (int): The number of past timesteps used for forecasting.\n",
        "    - PREDICTION_LENGTH (int): The number of future timesteps predicted.\n",
        "    - hour (int): The desired hour (default: 8 AM).\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Get the test index and timestamp\n",
        "    INDEX, timestamp = get_index_and_timestamp_for_validation(hour, df, train_size=len(X_train) + CONTEXT_LENGTH)\n",
        "\n",
        "    # Step 2: Extract predictions using the computed index\n",
        "    actuals_daily = truncate_tensor_with_interval(actuals, INDEX, offset=PREDICTION_LENGTH)\n",
        "\n",
        "    # Step 3: Retrieve true values from `df['dc_power']`\n",
        "    start_index = len(X_train) + CONTEXT_LENGTH + INDEX\n",
        "    true_values = df['dc_power'].iloc[start_index:start_index + PREDICTION_LENGTH].values  # Convert to NumPy array\n",
        "\n",
        "    # Step 4: Format values for comparison\n",
        "    formatted_actuals = np.round(actuals_daily[0], 1)  # Convert tensor to NumPy & round\n",
        "    formatted_true_values = np.round(true_values, 1)  # Round true values for comparison\n",
        "\n",
        "    # Step 5: Print Debugging Information\n",
        "    print(\"🔍 Validation at timestamp:\", timestamp)\n",
        "    print(\"Predicted Values (Rounded):\")\n",
        "    print(f\"{formatted_actuals}\")\n",
        "    print(\"True Values (Rounded):\")\n",
        "    print(f\"{formatted_true_values}\")\n",
        "\n",
        "    # Step 6: Perform Validation Check\n",
        "    validation_passed = np.array_equal(formatted_actuals, formatted_true_values)\n",
        "\n",
        "    print(\"\\n✅ Validation Passed\" if validation_passed else \"\\n❌ Validation Failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b_AgDYt-8_z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7oYYV-DYpgB",
        "outputId": "5172fdf5-5136-48b8-c54a-c1c663787df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Definitions and Constants\n",
        "FILEPATH = '/content/sample_data/230401_250108_PT1H_Solcast_reduced_features.csv'\n",
        "PREDICTION_LENGTH_LIST = [4, 12, 24, 48, 96, 336] # Use the past 4, 12, 24, 48, 96, 336 time steps for prediction (4 hours to 14 days)\n",
        "CONTEXT_LENGTH_FOLD_LIST = [1, 2, 4, 8] # Use 1, 2, 4 and 8 fold PREDICTION_LENGTH\n",
        "START_HOUR = 8 # Use to start plots at this hour\n",
        "TRAIN_DATA_LAST_INDEX = 8784 # Corresponds to '2024-04-01 00:00:00', used for LSTM and DeepAR\n",
        "\n",
        "if MODEL_TYPE == \"LSTM\":\n",
        "  print(\"LSTM model definitions and constants loaded\")\n",
        "  HIDDEN_SIZE = 64  # Number of hidden units -> # of neurons in the LSTM's hidden layer\n",
        "  BATCH_SIZE = 64  # Batch size -> # of samples processed in parallel\n",
        "  NUM_LAYERS = 2  # Number of LSTM layers -> # of stacked LSTM layers\n",
        "  LR = 1e-3\n",
        "  NUM_EPOCHS = 25\n",
        "\n",
        "elif MODEL_TYPE == \"DeepAR\":\n",
        "  print(\"DeepAR model type stuff\")\n",
        "  HIDDEN_SIZE = 64 # Number of RNN cells for each layer (default: 40)\n",
        "  NUM_LAYERS = 2 # Number of RNN layers (default: 2)\n",
        "  LR = 1e-3\n",
        "  NUM_EPOCHS = 25\n",
        "  optimize_memory()\n",
        "\n",
        "elif MODEL_TYPE == \"MOIRAI\":\n",
        "  PATCH_SIZE = 32 # patch size, Number of samples for each layer or sequence, 32 or 64 recommended for hourly\n",
        "  BATCH_SIZE = 16 # batch size, samples processed in parallel\n",
        "  LR = \"nA\"\n",
        "  NUM_EPOCHS = \"nA\"\n",
        "  LOSS = \"nA\"\n",
        "  DROPOUT_P = \"nA\"\n",
        "  MODEL = \"moirai-1.1-R\"  # model name: choose from {'moirai-1.1-R', 'moirai-moe-1.0-R'}\n",
        "  SIZE = \"small\"  # model size: choose from {'small', 'base', 'large'}\n",
        "  MOIRAI_MODEL_STR = f\"Salesforce/{MODEL}-{SIZE}\"\n",
        "  MOIRAI_MODEL_PATH_SAFE = MOIRAI_MODEL_STR.replace(\"Salesforce/\", \"\").replace(\"/\", \"-\").replace(\".\", \"-\")\n",
        "  optimize_memory()\n",
        "\n",
        "else:\n",
        "  print(\"Invalid model type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iwqJv1T9cjfL"
      },
      "outputs": [],
      "source": [
        "if MODEL_TYPE == \"LSTM\":\n",
        "  print(\"LSTM model type stuff\")\n",
        "\n",
        "elif MODEL_TYPE == \"DeepAR\":\n",
        "\n",
        "  print(\"DeepAR model type stuff\")\n",
        "\n",
        "elif MODEL_TYPE == \"MOIRAI\":\n",
        "\n",
        "  print(\"MOIRAI model type stuff\")\n",
        "\n",
        "else:\n",
        "  print(\"Invalid model type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eOuzkZPrXDYf"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load data from csv\n",
        "df = load_data(FILEPATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwyPxPrUcrFi",
        "outputId": "114abe18-bba7-4191-ab28-b0bdf7afd672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepAR OR MOIRAI feature scaling\n"
          ]
        }
      ],
      "source": [
        "## Step 2: Feature Scaling\n",
        "if MODEL_TYPE == \"LSTM\":\n",
        "  print(\"LSTM feature scaling\")\n",
        "  # Separate features and target\n",
        "  target_column = 'dc_power'\n",
        "  features = [\n",
        "      'air_temp', 'albedo', 'azimuth', 'clearsky_dhi', 'clearsky_dni', 'clearsky_ghi', 'clearsky_gti',\n",
        "      'cloud_opacity', 'dewpoint_temp', 'dhi', 'dni', 'ghi', 'gti', 'precipitable_water',\n",
        "      'precipitation_rate', 'relative_humidity', 'surface_pressure', 'snow_depth',\n",
        "      'snow_water_equivalent', 'snow_soiling_rooftop', 'snow_soiling_ground',\n",
        "      'wind_direction_100m', 'wind_direction_10m', 'wind_speed_100m', 'wind_speed_10m', 'zenith'\n",
        "  ]\n",
        "  features = [\n",
        "      'air_temp', 'azimuth', 'cloud_opacity', 'dhi', 'dni', 'ghi', 'zenith'\n",
        "  ]\n",
        "  X = df[features]\n",
        "  y = df[target_column]\n",
        "\n",
        "  # Standardize the features\n",
        "  scaler = StandardScaler()\n",
        "  #scaler = MinMaxScaler()\n",
        "  X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "  # Convert target to a numpy array\n",
        "  y = y.to_numpy()\n",
        "\n",
        "elif MODEL_TYPE == \"DeepAR\" or \"MOIRAI\":\n",
        "  print(\"DeepAR OR MOIRAI feature scaling\")\n",
        "  # https://ts.gluon.ai/dev/api/gluonts/gluonts.dataset.pandas.html\n",
        "  # Convert into GluonTS dataset with features\n",
        "  dataset = PandasDataset(df, target=\"dc_power\",\n",
        "                        past_feat_dynamic_real=['dc_power','air_temp', 'azimuth', 'cloud_opacity', 'dhi', 'dni', 'ghi', 'zenith'],\n",
        "                        feat_dynamic_real=['air_temp', 'azimuth', 'cloud_opacity', 'dhi', 'dni', 'ghi', 'zenith'])\n",
        "\n",
        "elif MODEL_TYPE == \"MOIRAI\":\n",
        "  print(\"MOIRAI feature scaling\")\n",
        "\n",
        "else:\n",
        "  print(\"Invalid model type - feature scaling failed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220,
          "referenced_widgets": [
            "9a4f780ab2f34058ad13df98379b3968",
            "17dcd2647d5f4d5bb3d345112f2c0573",
            "01c89da94d494969a4fc42b5d325d3e9",
            "ed5d7191b7424da6978121b053232ee5",
            "bb926f24e5c044c88edf3f8aa35d9dd5",
            "e8179b7b9b5d4af5932c9f3a8ca7c3f6",
            "11b1247a206b47b98a23ba6417f61c14",
            "518dbb7452584a149145ed13f89a96d5",
            "f12af9cfc96849ca9541cf83d6f193b2",
            "898c5380e2ae46fdb4eaf0d0aa73f5fc",
            "ca177609a13e4659b5636c57322987fe",
            "a0b726e5623a41a8b86717f5f594bd7b",
            "b7010df5bb5340a8afb4a7c9cab4357b",
            "731bc151e2484e92a05eeda2ef19cb24",
            "3eb7ad77e1e6438fbc049831be45386a",
            "f8a6731b9a2143a9a1ae658febeee7a4",
            "e9920737715b45c5a9d94f968764cf4b",
            "106422f6b78e4053bfb81bb84c1f2012",
            "4226e758be2c428a8b120db8289583c3",
            "e9e5d3ec35034839a5cb8c62e0990639",
            "d9794fa75a5145b49d54e546d85174f1",
            "adcab91c97794e82934de361c7574494"
          ]
        },
        "id": "HD2g7m_5hOaq",
        "outputId": "912a8c97-bb73-47af-ad9e-7251563b59f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MOIRAI model type stuff\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/682 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a4f780ab2f34058ad13df98379b3968"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/55.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a0b726e5623a41a8b86717f5f594bd7b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Main block - run for several iterations\n",
        "\n",
        "rmse_list = []\n",
        "mae_list = []\n",
        "smape_list = []\n",
        "r2_list = []\n",
        "\n",
        "for prediction_length in sorted(PREDICTION_LENGTH_LIST):\n",
        "  for context_length_fold in sorted(CONTEXT_LENGTH_FOLD_LIST):\n",
        "    PREDICTION_LENGTH = prediction_length\n",
        "    CONTEXT_LENGTH = context_length_fold * PREDICTION_LENGTH\n",
        "\n",
        "    if MODEL_TYPE == \"LSTM\":\n",
        "      print(\"LSTM model - Main block\")\n",
        "\n",
        "      # Step 3: Create sequences\n",
        "      X_seq, y_seq = create_sequences(X_scaled, y, CONTEXT_LENGTH, PREDICTION_LENGTH)\n",
        "\n",
        "      # Step 4: Split data into training and testing sets\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, train_size=TRAIN_DATA_LAST_INDEX, random_state=None, shuffle=False)\n",
        "      # Convert data to PyTorch tensors\n",
        "      X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "      y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "      X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "      y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "      # Step 5: Create DataLoader for batch processing\n",
        "      train_dataset = TensorDataset(X_train, y_train)\n",
        "      test_dataset = TensorDataset(X_test, y_test)\n",
        "      train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "      test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "      # Step 6: Create the model\n",
        "      input_size = X_train.shape[2]  # Number of features -> 7 eg. 'air_temp', 'albedo', etc.\n",
        "      model = LSTMModel(input_size, HIDDEN_SIZE, NUM_LAYERS, output_size=PREDICTION_LENGTH)\n",
        "\n",
        "      # Step 7: Define the loss function and optimizer\n",
        "      criterion = nn.MSELoss()\n",
        "      optimizer = torch.optim.Adam(model.parameters(), LR)\n",
        "\n",
        "      # Step 8: Train the model\n",
        "      LOSS = train_LSTM_model(model, train_loader, criterion, optimizer, NUM_EPOCHS)\n",
        "\n",
        "      # Step 9: Evaluate the model\n",
        "      predictions, actuals = evaluate_LSTM_model(model, test_loader)\n",
        "      data_validation_LSTM(df, actuals, X_train, CONTEXT_LENGTH, PREDICTION_LENGTH, hour=START_HOUR)\n",
        "      INDEX, TIMESTAMP = get_index_and_timestamp_for_validation(START_HOUR, df, train_size=len(X_train) + CONTEXT_LENGTH)\n",
        "      TIMESTAMP_DATE = TIMESTAMP.split(\" \")[0]\n",
        "      actuals_daily = truncate_tensor_with_interval(actuals, INDEX, offset=PREDICTION_LENGTH)\n",
        "      predictions_daily = truncate_tensor_with_interval(predictions, INDEX, offset=PREDICTION_LENGTH)\n",
        "\n",
        "      # Reset list values\n",
        "      rmse_list = []\n",
        "      mae_list = []\n",
        "      smape_list = []\n",
        "      r2_list = []\n",
        "\n",
        "      # Step 10: Calc metrics\n",
        "      for i, row in enumerate(actuals_daily):  # Loop over days in actuals_daily rows\n",
        "        rmse = np.sqrt(np.mean((predictions_daily[i] - actuals_daily[i])**2))\n",
        "        rmse_list.append(rmse)\n",
        "        mae = mean_absolute_error(actuals_daily[i], predictions_daily[i])\n",
        "        mae_list.append(mae)\n",
        "        r2 = r2_score(actuals_daily[i], predictions_daily[i])\n",
        "        r2_list.append(r2)\n",
        "        smape = calc_smape(actuals_daily[i], predictions_daily[i])\n",
        "        smape_list.append(smape)\n",
        "        #print(f\"DATE={PREDICTION_START_DATETIME_STR}, RMSE={rmse:.4f}, MAE={mae:.4f}, SMAPE={smape:.2f}%, R^2={r2:.4f}\")\n",
        "      print(\"Metrics ---------------------------------------------------------\")\n",
        "      print(f\"PL={PREDICTION_LENGTH}, CL={CONTEXT_LENGTH}, Model={MODEL_TYPE}\")\n",
        "      print(f\"Length of: RMSE={len(rmse_list)}, MAE={len(mae_list)}, SMAPE={len(smape_list)}, R2={len(r2_list)}\")\n",
        "      print(f\"Mean of: RMSE={np.mean(rmse_list):.4f}, MAE={np.mean(mae_list):.4f}, SMAPE={np.mean(smape_list):.2f}%, R^2={np.mean(r2_list):.4f}\")\n",
        "      RMSE_MED = np.median(rmse_list)\n",
        "      MAE_MED = np.median(mae_list)\n",
        "      SMAPE_MED = np.median(smape_list)\n",
        "      R2_MED = np.median(r2_list)\n",
        "      print(f\"Median of: RMSE={RMSE_MED:.4f}, MAE={MAE_MED:.4f}, SMAPE={SMAPE_MED:.2f}%, R^2={R2_MED:.4f}\")\n",
        "      print(\"\")\n",
        "\n",
        "      # Step 11: Prepare Plot\n",
        "      # Plot\n",
        "      plt.plot(df[TIMESTAMP:][\"dc_power\"].iloc[:PREDICTION_LENGTH], color=\"black\", linestyle=\"--\", label=\"True values\")\n",
        "      plt.plot(df[TIMESTAMP:].iloc[:PREDICTION_LENGTH].index, predictions_daily[0], color=\"blue\", label=\"Predictions\")\n",
        "\n",
        "      plt.legend(loc=\"upper right\", fontsize=\"small\")\n",
        "      plt.ylabel(\"DC Power (kW)\")\n",
        "      plt.xlabel(\"Date\")\n",
        "      plt.title(f\"{MODEL_TYPE} Forecast (starts {TIMESTAMP})\")\n",
        "      plt.figtext(0.1, 0.9000,f\"RMSE={RMSE_MED:.4f}, MAE={MAE_MED:.4f}, SMAPE={SMAPE_MED:.2f}%, R^2={R2_MED:.4f} (median values over {len(rmse_list)} samples)\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8700,f\"Pre_L={PREDICTION_LENGTH}, Con_L={CONTEXT_LENGTH}, HIDDEN_SIZE={HIDDEN_SIZE}, BATCH={BATCH_SIZE}, LR={LR}, Epochs={NUM_EPOCHS}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8400,f\"Dataset={file_name_no_extension(FILEPATH)}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8100,f\"Features={features}\", fontsize=6, color='black')\n",
        "      #plt.figtext(0.1, 0.7800,f\"Past features={dataset.past_feat_dynamic_real}\", fontsize=6, color='black')\n",
        "\n",
        "      # Set y-axis limits\n",
        "      plt.ylim(0, 12.5)\n",
        "\n",
        "      # Set y-axis limits and x-axis formatting\n",
        "      ax = plt.gca()\n",
        "      ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n",
        "      ax.xaxis.set_major_locator(mdates.DayLocator())\n",
        "      plt.xticks(rotation=25)\n",
        "\n",
        "      # Adjust layout and save plot\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(f\"PL={PREDICTION_LENGTH}-CL={CONTEXT_LENGTH}-RMSE={RMSE_MED:.4f}-{TIMESTAMP_DATE}-{MODEL_TYPE}.pdf\")\n",
        "      #plt.show()\n",
        "\n",
        "    elif MODEL_TYPE == \"DeepAR\":\n",
        "      print(\"DeepAR model type stuff\")\n",
        "\n",
        "      # Step 3: Split data into training and testing sets\n",
        "      TESTDATA_LENGTH = len(df) - TRAIN_DATA_LAST_INDEX\n",
        "\n",
        "      # Step 4: Create test sequences for time-series prediction\n",
        "      training_data, test_gen = split(dataset, offset=-TESTDATA_LENGTH)\n",
        "      test_data = test_gen.generate_instances(prediction_length=PREDICTION_LENGTH)\n",
        "      # Helper\n",
        "      # Calculate timestamp for Test Data starting point\n",
        "      timestamp_test_data = df[len(df)-TESTDATA_LENGTH:].iloc[:1].index.item()\n",
        "      TEST_DATA_START_DATETIME = timestamp_test_data.strftime('%Y-%m-%d %H:%M:%S')\n",
        "      TEST_DATA_START_DATE_STR = TEST_DATA_START_DATETIME.split(\" \")[0]\n",
        "\n",
        "      # Step5: Define and Train the model\n",
        "      model = DeepAREstimator(\n",
        "          prediction_length=PREDICTION_LENGTH, context_length=CONTEXT_LENGTH, freq=dataset.freq,\n",
        "          trainer_kwargs={\"max_epochs\": NUM_EPOCHS},\n",
        "          num_feat_dynamic_real=dataset.num_feat_dynamic_real,\n",
        "          num_layers=NUM_LAYERS,\n",
        "          hidden_size=HIDDEN_SIZE,\n",
        "          lr=LR,\n",
        "      ).train(training_data)\n",
        "\n",
        "      # Reset list values\n",
        "      rmse_list = []\n",
        "      mae_list = []\n",
        "      smape_list = []\n",
        "      r2_list = []\n",
        "      dates = get_values_by_day(df[TRAIN_DATA_LAST_INDEX:], offset=PREDICTION_LENGTH)\n",
        "      INITIAL_DATETIME = sorted(dates)[0]\n",
        "      INITIAL_DATE = INITIAL_DATETIME.split(\" \")[0]\n",
        "\n",
        "      # Loop over all dates in testdata\n",
        "      for date in sorted(dates):\n",
        "        # Step 3: Split data into training and testing sets\n",
        "        # Step 4: Create test sequences for time-series prediction\n",
        "        PREDICTION_START_DATETIME_STR = date\n",
        "        PREDICTION_START_DATE_STR = PREDICTION_START_DATETIME_STR.split(\" \")[0]\n",
        "        training_data, test_gen = split(dataset, date=pd.Period(PREDICTION_START_DATETIME_STR, freq=\"h\"))\n",
        "        test_data = test_gen.generate_instances(prediction_length=PREDICTION_LENGTH, windows=1)\n",
        "\n",
        "        # Step 6: Get probabilistic predictions\n",
        "        forecasts = list(model.predict(test_data.input))\n",
        "\n",
        "        # Step 7: Get point predictions\n",
        "        # Get predictions from forecasts\n",
        "        predictions = forecasts[0].mean_ts\n",
        "\n",
        "        # Feature added here!!!\n",
        "        # Truncate negative predictions to 0\n",
        "        predictions = np.maximum(predictions, 0)\n",
        "\n",
        "        # Get actuals for metric calculations later\n",
        "        actuals = df[timestamp_test_data:].iloc[:PREDICTION_LENGTH][\"dc_power\"]\n",
        "\n",
        "        # Change type\n",
        "        predictions = predictions.to_numpy()\n",
        "        actuals = actuals.to_numpy()\n",
        "\n",
        "        # Step 8: Do evaluations\n",
        "        rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
        "        rmse_list.append(rmse)\n",
        "        mae = mean_absolute_error(actuals, predictions)\n",
        "        mae_list.append(mae)\n",
        "        r2 = r2_score(actuals, predictions)\n",
        "        r2_list.append(r2)\n",
        "        smape = calc_smape(actuals, predictions)\n",
        "        smape_list.append(smape)\n",
        "        #print(f\"DATE={PREDICTION_START_DATETIME_STR}, RMSE={rmse:.4f}, MAE={mae:.4f}, SMAPE={smape:.2f}%, R^2={r2:.4f}\")\n",
        "\n",
        "      print(f\"PL={PREDICTION_LENGTH}, CL={CONTEXT_LENGTH}, Model={MODEL_TYPE}\")\n",
        "      print(f\"Length of: RMSE={len(rmse_list)}, MAE={len(mae_list)}, SMAPE={len(smape_list)}, R2={len(r2_list)}\")\n",
        "      print(f\"Mean of: RMSE={np.mean(rmse_list):.4f}, MAE={np.mean(mae_list):.4f}, SMAPE={np.mean(smape_list):.2f}%, R^2={np.mean(r2_list):.4f}\")\n",
        "      RMSE_MED = np.median(rmse_list)\n",
        "      MAE_MED = np.median(mae_list)\n",
        "      SMAPE_MED = np.median(smape_list)\n",
        "      R2_MED = np.median(r2_list)\n",
        "      print(f\"Median of: RMSE={RMSE_MED:.4f}, MAE={MAE_MED:.4f}, SMAPE={SMAPE_MED:.2f}%, R^2={R2_MED:.4f}\")\n",
        "      print(\"\")\n",
        "\n",
        "      # Prepare Plot\n",
        "      # Get prediction intervals\n",
        "      lower_50 = forecasts[0].quantile(0.25)  # 25th percentile (lower bound)\n",
        "      upper_50 = forecasts[0].quantile(0.75)  # 75th percentile (upper bound)\n",
        "      lower_90 = forecasts[0].quantile(0.05)  # 5th percentile (lower bound)\n",
        "      upper_90 = forecasts[0].quantile(0.95)  # 95th percentile (upper bound)\n",
        "\n",
        "      # Plot filtered data\n",
        "      plt.plot(df[INITIAL_DATETIME:][\"dc_power\"].iloc[:PREDICTION_LENGTH], color=\"black\", linestyle=\"--\", label=\"True values\")\n",
        "      plt.plot(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH].index, predictions[len(predictions)-len(df[PREDICTION_START_DATETIME_STR:].iloc[:PREDICTION_LENGTH]):], color=\"blue\", label=\"Predictions\")\n",
        "\n",
        "      # Add 50% Prediction Interval (Shaded)\n",
        "      plt.fill_between(\n",
        "          df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH].index,\n",
        "          lower_50[len(lower_50)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          upper_50[len(upper_50)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          color=\"blue\", alpha=0.2, label=\"50% Prediction Interval\"\n",
        "      )\n",
        "\n",
        "      # Add 90% Prediction Interval (Shaded)\n",
        "      plt.fill_between(\n",
        "          df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH].index,\n",
        "          lower_90[len(lower_90)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          upper_90[len(upper_90)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          color=\"blue\", alpha=0.075, label=\"90% Prediction Interval\"\n",
        "      )\n",
        "\n",
        "      plt.legend(loc=\"upper right\", fontsize=\"small\")\n",
        "      plt.ylabel(\"DC Power (kW)\")\n",
        "      plt.xlabel(\"Date\")\n",
        "      plt.title(f\"{MODEL_TYPE} Forecast (starts {INITIAL_DATETIME})\")\n",
        "      plt.figtext(0.1, 0.9000,f\"RMSE={RMSE_MED:.4f}, MAE={MAE_MED:.4f}, SMAPE={SMAPE_MED:.2f}%, R^2={R2_MED:.4f} (median values over {len(rmse_list)} samples)\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8700,f\"Pre_L={PREDICTION_LENGTH}, Con_L={CONTEXT_LENGTH}, HIDDEN_SIZE={HIDDEN_SIZE}, RNN_LAYERS={NUM_LAYERS}, LR={LR}, Epochs={NUM_EPOCHS}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8400,f\"Dataset={file_name_no_extension(FILEPATH)}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8100,f\"Features={dataset.feat_dynamic_real}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.7800,f\"Past features={dataset.past_feat_dynamic_real}\", fontsize=6, color='black')\n",
        "\n",
        "      # Set y-axis limits\n",
        "      plt.ylim(0, 12.5)\n",
        "\n",
        "      # Set y-axis limits and x-axis formatting\n",
        "      ax = plt.gca()\n",
        "      ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n",
        "      ax.xaxis.set_major_locator(mdates.DayLocator())\n",
        "      plt.xticks(rotation=25)\n",
        "\n",
        "      # Adjust layout and save plot\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(f\"PL={PREDICTION_LENGTH}-CL={CONTEXT_LENGTH}-RMSE={RMSE_MED:.4f}-{INITIAL_DATE}-{MODEL_TYPE}.pdf\")\n",
        "      #plt.show()\n",
        "\n",
        "    elif MODEL_TYPE == \"MOIRAI\":\n",
        "      print(\"MOIRAI model type stuff\")\n",
        "\n",
        "      # Step 2.1: Prepare pre-trained model by downloading model weights from huggingface hub\n",
        "      if \"moirai-moe\" in MOIRAI_MODEL_STR:\n",
        "        model = MoiraiMoEForecast(\n",
        "          module=MoiraiMoEModule.from_pretrained(MOIRAI_MODEL_STR),\n",
        "          prediction_length=PREDICTION_LENGTH,\n",
        "          context_length=CONTEXT_LENGTH,\n",
        "          patch_size=PATCH_SIZE,\n",
        "          num_samples=100,\n",
        "          target_dim=1,\n",
        "          feat_dynamic_real_dim=dataset.num_feat_dynamic_real,\n",
        "          past_feat_dynamic_real_dim=dataset.num_past_feat_dynamic_real,\n",
        "        )\n",
        "        #print(\"MOE used\")\n",
        "      else:\n",
        "        model = MoiraiForecast(\n",
        "          module=MoiraiModule.from_pretrained(MOIRAI_MODEL_STR),\n",
        "          prediction_length=PREDICTION_LENGTH,\n",
        "          context_length=CONTEXT_LENGTH,\n",
        "          patch_size=PATCH_SIZE,\n",
        "          num_samples=100,\n",
        "          target_dim=1,\n",
        "          feat_dynamic_real_dim=dataset.num_feat_dynamic_real,\n",
        "          past_feat_dynamic_real_dim=dataset.num_past_feat_dynamic_real,\n",
        "        )\n",
        "        #print(\"No MOE used\")\n",
        "\n",
        "      '''\n",
        "\n",
        "      # Step 2.2: Prepare fine-tuned model by loading model weights from ckpt file\n",
        "      checkpoint_path = \"/content/last.ckpt\"\n",
        "\n",
        "      # Load the model\n",
        "      model = MoiraiForecast.load_from_checkpoint(checkpoint_path,\n",
        "      prediction_length=PREDICTION_LENGTH,\n",
        "      context_length=CONTEXT_LENGTH,\n",
        "      patch_size=PATCH_SIZE,\n",
        "      num_samples=100,\n",
        "      target_dim=1,\n",
        "      feat_dynamic_real_dim=dataset.num_feat_dynamic_real,\n",
        "      past_feat_dynamic_real_dim=dataset.num_past_feat_dynamic_real)\n",
        "      '''\n",
        "\n",
        "      # Reset list values\n",
        "      rmse_list = []\n",
        "      mae_list = []\n",
        "      smape_list = []\n",
        "      r2_list = []\n",
        "      dates = get_values_by_day(df, offset=PREDICTION_LENGTH)\n",
        "      INITIAL_DATETIME = sorted(dates)[0]\n",
        "      INITIAL_DATE = INITIAL_DATETIME.split(\" \")[0]\n",
        "\n",
        "      # Loop over all dates in testdata\n",
        "      for date in sorted(dates):\n",
        "        # Step 3: Split data into training and testing sets\n",
        "        # Step 4: Create test sequences for time-series prediction\n",
        "        PREDICTION_START_DATETIME_STR = date\n",
        "        PREDICTION_START_DATE_STR = PREDICTION_START_DATETIME_STR.split(\" \")[0]\n",
        "        training_data, test_gen = split(dataset, date=pd.Period(PREDICTION_START_DATETIME_STR, freq=\"h\"))\n",
        "        test_data = test_gen.generate_instances(prediction_length=PREDICTION_LENGTH, windows=1)\n",
        "\n",
        "        # Step 6: Get probabilistic predictions\n",
        "        predictor = model.create_predictor(batch_size=BATCH_SIZE)\n",
        "        forecasts = list(predictor.predict(test_data.input))\n",
        "\n",
        "        # Step 7: Get point predictions\n",
        "        # Get predictions from forecasts\n",
        "        predictions = forecasts[0].mean_ts\n",
        "        # Feature added here!!!\n",
        "        # Truncate negative predictions to 0\n",
        "        predictions = np.maximum(predictions, 0)\n",
        "\n",
        "        # Get actuals for metric calculations later\n",
        "        actuals = df.loc[PREDICTION_START_DATETIME_STR:].iloc[:PREDICTION_LENGTH][\"dc_power\"]\n",
        "\n",
        "        # Change type\n",
        "        predictions = predictions.to_numpy()\n",
        "        actuals = actuals.to_numpy()\n",
        "\n",
        "        # Step 8: Do evaluations\n",
        "        rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
        "        rmse_list.append(rmse)\n",
        "        mae = mean_absolute_error(actuals, predictions)\n",
        "        mae_list.append(mae)\n",
        "        r2 = r2_score(actuals, predictions)\n",
        "        r2_list.append(r2)\n",
        "        smape = calc_smape(actuals, predictions)\n",
        "        smape_list.append(smape)\n",
        "        #print(f\"DATE={PREDICTION_START_DATETIME_STR}, RMSE={rmse:.4f}, MAE={mae:.4f}, SMAPE={smape:.2f}%, R^2={r2:.4f}\")\n",
        "\n",
        "      print(f\"PL={PREDICTION_LENGTH}, CL={CONTEXT_LENGTH}, Model={MOIRAI_MODEL_PATH_SAFE}\")\n",
        "      print(f\"Length of: RMSE={len(rmse_list)}, MAE={len(mae_list)}, SMAPE={len(smape_list)}, R2={len(r2_list)}\")\n",
        "      print(f\"Mean of: RMSE={np.mean(rmse_list):.4f}, MAE={np.mean(mae_list):.4f}, SMAPE={np.mean(smape_list):.2f}%, R^2={np.mean(r2_list):.4f}\")\n",
        "      RMSE_MED = np.median(rmse_list)\n",
        "      MAE_MED = np.median(mae_list)\n",
        "      SMAPE_MED = np.median(smape_list)\n",
        "      R2_MED = np.median(r2_list)\n",
        "      print(f\"Median of: RMSE={RMSE_MED:.4f}, MAE={MAE_MED:.4f}, SMAPE={SMAPE_MED:.2f}%, R^2={R2_MED:.4f}\")\n",
        "      print(\"\")\n",
        "\n",
        "\n",
        "      # Prepare Plot\n",
        "      # Get prediction intervals\n",
        "      lower_50 = forecasts[0].quantile(0.25)  # 25th percentile (lower bound)\n",
        "      upper_50 = forecasts[0].quantile(0.75)  # 75th percentile (upper bound)\n",
        "      lower_90 = forecasts[0].quantile(0.05)  # 5th percentile (lower bound)\n",
        "      upper_90 = forecasts[0].quantile(0.95)  # 95th percentile (upper bound)\n",
        "\n",
        "      # Plot filtered data\n",
        "      plt.plot(df[INITIAL_DATETIME:][\"dc_power\"].iloc[:PREDICTION_LENGTH], color=\"black\", linestyle=\"--\", label=\"True values\")\n",
        "      plt.plot(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH].index, predictions[len(predictions)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):], color=\"blue\", label=\"Predictions\")\n",
        "\n",
        "      # Add 50% Prediction Interval (Shaded)\n",
        "      plt.fill_between(\n",
        "          df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH].index,\n",
        "          lower_50[len(lower_50)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          upper_50[len(upper_50)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          color=\"blue\", alpha=0.2, label=\"50% Prediction Interval\"\n",
        "      )\n",
        "\n",
        "      # Add 90% Prediction Interval (Shaded)\n",
        "      plt.fill_between(\n",
        "          df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH].index,\n",
        "          lower_90[len(lower_90)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          upper_90[len(upper_90)-len(df[INITIAL_DATETIME:].iloc[:PREDICTION_LENGTH]):],\n",
        "          color=\"blue\", alpha=0.075, label=\"90% Prediction Interval\"\n",
        "      )\n",
        "\n",
        "      plt.legend(loc=\"upper right\", fontsize=\"small\")\n",
        "      plt.ylabel(\"DC Power (kW)\")\n",
        "      plt.xlabel(\"Date\")\n",
        "      plt.title(f\"{MOIRAI_MODEL_PATH_SAFE} Forecast (starts {INITIAL_DATETIME})\")\n",
        "      plt.figtext(0.1, 0.9000,f\"RMSE={RMSE_MED:.4f}, MAE={MAE_MED:.4f}, SMAPE={SMAPE_MED:.2f}%, R^2={R2_MED:.4f} (median values over {len(rmse_list)} samples)\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8700,f\"Pre_L={PREDICTION_LENGTH}, Con_L={CONTEXT_LENGTH}, PATCH={PATCH_SIZE}, BATCH={BATCH_SIZE}, LR={LR}, Epochs={NUM_EPOCHS}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8400,f\"Dataset={file_name_no_extension(FILEPATH)}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.8100,f\"Features={dataset.feat_dynamic_real}\", fontsize=6, color='black')\n",
        "      plt.figtext(0.1, 0.7800,f\"Past features={dataset.past_feat_dynamic_real}\", fontsize=6, color='black')\n",
        "\n",
        "      # Set y-axis limits\n",
        "      plt.ylim(0, 12.5)\n",
        "\n",
        "      # Set y-axis limits and x-axis formatting\n",
        "      ax = plt.gca()\n",
        "      ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n",
        "      ax.xaxis.set_major_locator(mdates.DayLocator())\n",
        "      plt.xticks(rotation=25)\n",
        "\n",
        "      # Adjust layout and save plot\n",
        "      plt.tight_layout()\n",
        "      plt.savefig(f\"PL={PREDICTION_LENGTH}-CL={CONTEXT_LENGTH}-RMSE={RMSE_MED:.4f}-{INITIAL_DATE}-{MOIRAI_MODEL_PATH_SAFE}.pdf\")\n",
        "      #plt.show()\n",
        "\n",
        "    else:\n",
        "      print(\"Invalid model type\")\n",
        "\n",
        "    # Reset plot before next round\n",
        "    plt.figure().clear()\n",
        "    plt.close()\n",
        "    plt.cla()\n",
        "    plt.clf()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5WvOywrXWbW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Step5.2: Prepare fine-tuned model by loading model weights from ckpt file\n",
        "checkpoint_path = \"/content/multirun/2025-02-11/15-40-14/0/checkpoints/epoch=13-step=1400.ckpt\"\n",
        "\n",
        "# Load the model\n",
        "model = MoiraiForecast.load_from_checkpoint(checkpoint_path,\n",
        "      prediction_length=PREDICTION_LENGTH,\n",
        "      context_length=CONTEXT_LENGTH,\n",
        "      patch_size=PATCH_SIZE,\n",
        "      num_samples=100,\n",
        "      target_dim=1,\n",
        "      feat_dynamic_real_dim=dataset.num_feat_dynamic_real,\n",
        "      past_feat_dynamic_real_dim=dataset.num_past_feat_dynamic_real)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN6y68zkhZS2"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning in regards to https://github.com/SalesforceAIResearch/uni2ts/blob/main/README.md#fine-tuning\n",
        "# Step 1 Set Data Path Directory\n",
        "!echo \"CUSTOM_DATA_PATH=/content/uni2ts/\" >> .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es3PtupPF-OB"
      },
      "outputs": [],
      "source": [
        "!echo \"PYTHONPATH=/content/uni2ts\" >> .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9NAOCxbYZwC",
        "outputId": "d60a0ab3-f3cd-41e4-873d-ec18a6e01367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUSTOM_DATA_PATH=/content/uni2ts/\n",
            "PYTHONPATH=/content/uni2ts\n"
          ]
        }
      ],
      "source": [
        "!cat .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogaNfoMChlvJ",
        "outputId": "a4ae6c56-0db1-4c82-e8b5-2e04a9bbb5de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inferred frequency: H. Using this value for the 'freq' parameter.\n",
            "Generating train split: 1 examples [00:00,  4.58 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 1/1 [00:00<00:00, 263.00 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 2.1 process dataset\n",
        "!python -m uni2ts.data.builder.simple customdataset /content/uni2ts/230401_250108_PT1H_Solcast_reduced_features.csv --dataset_type wide_multivariate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsF2EKXEktSI",
        "outputId": "048f19bc-a972-42e2-f7df-72612a5907b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     dc_power  air_temp  azimuth  cloud_opacity  dhi  dni  ghi  zenith\n",
            "date                                                                                  \n",
            "2023-04-01 01:00:00       0.0         8      -27           37.4    0    0    0     125\n",
            "2023-04-01 02:00:00       0.0         8      -43           46.6    0    0    0     120\n",
            "2023-04-01 03:00:00       0.0         8      -57           40.6    0    0    0     112\n",
            "2023-04-01 04:00:00       0.0         7      -70           47.6    0    0    0     103\n",
            "2023-04-01 05:00:00       0.0         7      -81           58.2    0    0    0      93\n",
            "...                       ...       ...      ...            ...  ...  ...  ...     ...\n",
            "2024-08-31 03:00:00       0.0        19      -55            0.0    0    0    0     107\n",
            "2024-08-31 04:00:00       0.0        19      -67           16.5    0    0    0      98\n",
            "2024-08-31 05:00:00       0.0        19      -79           39.8    7    0    7      88\n",
            "2024-08-31 06:00:00       0.0        19      -90           23.9   94   47  104      78\n",
            "2024-08-31 07:00:00       0.1        21     -101            0.0   89  575  303      68\n",
            "\n",
            "[12439 rows x 8 columns]\n",
            "Inferred frequency: H. Using this value for the 'freq' parameter.\n",
            "Generating train split: 8 examples [00:00, 49.89 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 8/8 [00:00<00:00, 2004.21 examples/s]\n",
            "                     dc_power  air_temp  azimuth  cloud_opacity  dhi  dni  ghi  zenith\n",
            "date                                                                                  \n",
            "2023-04-01 01:00:00       0.0         8      -27           37.4    0    0    0     125\n",
            "2023-04-01 02:00:00       0.0         8      -43           46.6    0    0    0     120\n",
            "2023-04-01 03:00:00       0.0         8      -57           40.6    0    0    0     112\n",
            "2023-04-01 04:00:00       0.0         7      -70           47.6    0    0    0     103\n",
            "2023-04-01 05:00:00       0.0         7      -81           58.2    0    0    0      93\n",
            "...                       ...       ...      ...            ...  ...  ...  ...     ...\n",
            "2025-01-07 20:00:00       0.0         1       79           32.1    0    0    0     131\n",
            "2025-01-07 21:00:00       0.0         1       65           73.2    0    0    0     140\n",
            "2025-01-07 22:00:00       0.0         2       45           33.5    0    0    0     149\n",
            "2025-01-07 23:00:00       0.0         1       18           40.6    0    0    0     154\n",
            "2025-01-08 00:00:00       0.0         2      -14           51.0    0    0    0     155\n",
            "\n",
            "[15552 rows x 8 columns]\n",
            "Inferred frequency: H. Using this value for the 'freq' parameter.\n",
            "Generating train split: 8 examples [00:00, 52.27 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 8/8 [00:00<00:00, 2196.11 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 2.2 Set offset\n",
        "!python -m uni2ts.data.builder.simple customdataset /content/uni2ts/230401_250108_PT1H_Solcast_reduced_features.csv --date_offset '2024-08-31 07:00:00'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "903wnpN-PDG2"
      },
      "outputs": [],
      "source": [
        "#!mv content/uni2ts/cli content/uni2ts/src/uni2ts/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqmjyQbeCqjU"
      },
      "outputs": [],
      "source": [
        "# Step 3 (move cli dir to src/uni2ts first!!!!)\n",
        "# Set Batch size here /content/uni2ts/src/uni2ts/cli/conf/finetune/default.yaml to lower value\n",
        "# For moirai large with A100 40GB RAM use 16 as batch size in val_dataloader and train_dataloader section\n",
        "\n",
        "#!python -m uni2ts.cli.train -cp conf/finetune run_name=example_run model=moirai_1.1_R_large data=etth1 val_data=etth1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z-quJtw5FZL",
        "outputId": "32c2b9eb-5eda-409b-b738-0c7dfc50fa08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-02-11 15:40:14,959][HYDRA] Launching 1 jobs locally\n",
            "[2025-02-11 15:40:14,959][HYDRA] \t#0 : run_name=example_run model=moirai_1.1_R_base data=customdataset val_data=customdataset model.module_kwargs.dropout_p=0.2 trainer.max_epochs=25 model.lr=1e-07 train_dataloader.batch_size=24 val_dataloader.batch_size=24\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "[2025-02-11 15:40:16,903][datasets][INFO] - PyTorch version 2.4.1 available.\n",
            "[2025-02-11 15:40:16,903][datasets][INFO] - TensorFlow version 2.18.0 available.\n",
            "[2025-02-11 15:40:16,904][datasets][INFO] - JAX version 0.4.33 available.\n",
            "Seed set to 0\n",
            "2025-02-11 15:40:17.397359: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739288417.422296   43645 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739288417.429823   43645 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name   | Type         | Params | Mode \n",
            "------------------------------------------------\n",
            "0 | module | MoiraiModule | 91.4 M | train\n",
            "------------------------------------------------\n",
            "91.4 M    Trainable params\n",
            "0         Non-trainable params\n",
            "91.4 M    Total params\n",
            "365.431   Total estimated model params size (MB)\n",
            "253       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Epoch 0: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.360, train/PackedNLLLoss=3.230]Metric val/PackedNLLLoss improved. New best score: 2.360\n",
            "Epoch 1: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.360, train/PackedNLLLoss=3.230]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.300, train/PackedNLLLoss=3.120]Metric val/PackedNLLLoss improved by 0.059 >= min_delta = 0.0. New best score: 2.302\n",
            "Epoch 2: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.300, train/PackedNLLLoss=3.120]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.270, train/PackedNLLLoss=3.050]Metric val/PackedNLLLoss improved by 0.033 >= min_delta = 0.0. New best score: 2.268\n",
            "Epoch 3: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.270, train/PackedNLLLoss=3.050]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.250, train/PackedNLLLoss=3.000]Metric val/PackedNLLLoss improved by 0.022 >= min_delta = 0.0. New best score: 2.247\n",
            "Epoch 4: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.250, train/PackedNLLLoss=3.000]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.220, train/PackedNLLLoss=2.980]Metric val/PackedNLLLoss improved by 0.022 >= min_delta = 0.0. New best score: 2.225\n",
            "Epoch 5: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.220, train/PackedNLLLoss=2.980]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.210, train/PackedNLLLoss=2.950]Metric val/PackedNLLLoss improved by 0.012 >= min_delta = 0.0. New best score: 2.213\n",
            "Epoch 6: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.210, train/PackedNLLLoss=2.950]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.200, train/PackedNLLLoss=2.930]Metric val/PackedNLLLoss improved by 0.014 >= min_delta = 0.0. New best score: 2.199\n",
            "Epoch 7: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.200, train/PackedNLLLoss=2.930]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.190, train/PackedNLLLoss=2.900]Metric val/PackedNLLLoss improved by 0.013 >= min_delta = 0.0. New best score: 2.186\n",
            "Epoch 8: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.190, train/PackedNLLLoss=2.900]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.170, train/PackedNLLLoss=2.890]Metric val/PackedNLLLoss improved by 0.012 >= min_delta = 0.0. New best score: 2.174\n",
            "Epoch 9: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.170, train/PackedNLLLoss=2.890]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.170, train/PackedNLLLoss=2.870]Metric val/PackedNLLLoss improved by 0.006 >= min_delta = 0.0. New best score: 2.167\n",
            "Epoch 10: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.170, train/PackedNLLLoss=2.870]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.160, train/PackedNLLLoss=2.860]Metric val/PackedNLLLoss improved by 0.004 >= min_delta = 0.0. New best score: 2.163\n",
            "Epoch 11: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.160, train/PackedNLLLoss=2.860]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.160, train/PackedNLLLoss=2.850]Metric val/PackedNLLLoss improved by 0.008 >= min_delta = 0.0. New best score: 2.156\n",
            "Epoch 12: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.160, train/PackedNLLLoss=2.850]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.850]Metric val/PackedNLLLoss improved by 0.004 >= min_delta = 0.0. New best score: 2.152\n",
            "Epoch 13: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.850]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.840]Metric val/PackedNLLLoss improved by 0.004 >= min_delta = 0.0. New best score: 2.148\n",
            "Epoch 14: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.840]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.830]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.830]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.820]Monitored metric val/PackedNLLLoss did not improve in the last 3 records. Best score: 2.148. Signaling Trainer to stop.\n",
            "Epoch 16: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.820]\n",
            "/usr/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 22 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n"
          ]
        }
      ],
      "source": [
        "# Use Hydra's Sweeping Feature for Hyperparameter Search\n",
        "!python -m uni2ts.cli.train --multirun -cp conf/finetune run_name=example_run model=moirai_1.1_R_base data=customdataset val_data=customdataset \\\n",
        "  model.module_kwargs.dropout_p=0.2 \\\n",
        "  trainer.max_epochs=25 \\\n",
        "  model.lr=1e-7 \\\n",
        "  train_dataloader.batch_size=24 \\\n",
        "  val_dataloader.batch_size=24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMAEx64Har1x"
      },
      "outputs": [],
      "source": [
        "# Show Tensorboard with results\n",
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir /content/multirun\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppn9_O3-WJJ0"
      },
      "outputs": [],
      "source": [
        "#Export outputs\n",
        "#!zip -r outputs.zip /content/outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_B_t-0YOvLq"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "#!python -m uni2ts.cli.eval run_name=example_eval_1 model=moirai_1.0_R_small model.patch_size=32 model.context_length=1000 data=etth1_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwrqN3dk8vB2"
      },
      "outputs": [],
      "source": [
        "#import gc\n",
        "\n",
        "# Invoke garbage collector\n",
        "#gc.collect()\n",
        "\n",
        "# Clear GPU cache\n",
        "#torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a4f780ab2f34058ad13df98379b3968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_17dcd2647d5f4d5bb3d345112f2c0573",
              "IPY_MODEL_01c89da94d494969a4fc42b5d325d3e9",
              "IPY_MODEL_ed5d7191b7424da6978121b053232ee5"
            ],
            "layout": "IPY_MODEL_bb926f24e5c044c88edf3f8aa35d9dd5"
          }
        },
        "17dcd2647d5f4d5bb3d345112f2c0573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8179b7b9b5d4af5932c9f3a8ca7c3f6",
            "placeholder": "​",
            "style": "IPY_MODEL_11b1247a206b47b98a23ba6417f61c14",
            "value": "config.json: 100%"
          }
        },
        "01c89da94d494969a4fc42b5d325d3e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_518dbb7452584a149145ed13f89a96d5",
            "max": 682,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f12af9cfc96849ca9541cf83d6f193b2",
            "value": 682
          }
        },
        "ed5d7191b7424da6978121b053232ee5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_898c5380e2ae46fdb4eaf0d0aa73f5fc",
            "placeholder": "​",
            "style": "IPY_MODEL_ca177609a13e4659b5636c57322987fe",
            "value": " 682/682 [00:00&lt;00:00, 58.4kB/s]"
          }
        },
        "bb926f24e5c044c88edf3f8aa35d9dd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8179b7b9b5d4af5932c9f3a8ca7c3f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11b1247a206b47b98a23ba6417f61c14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "518dbb7452584a149145ed13f89a96d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f12af9cfc96849ca9541cf83d6f193b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "898c5380e2ae46fdb4eaf0d0aa73f5fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca177609a13e4659b5636c57322987fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0b726e5623a41a8b86717f5f594bd7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b7010df5bb5340a8afb4a7c9cab4357b",
              "IPY_MODEL_731bc151e2484e92a05eeda2ef19cb24",
              "IPY_MODEL_3eb7ad77e1e6438fbc049831be45386a"
            ],
            "layout": "IPY_MODEL_f8a6731b9a2143a9a1ae658febeee7a4"
          }
        },
        "b7010df5bb5340a8afb4a7c9cab4357b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9920737715b45c5a9d94f968764cf4b",
            "placeholder": "​",
            "style": "IPY_MODEL_106422f6b78e4053bfb81bb84c1f2012",
            "value": "model.safetensors: 100%"
          }
        },
        "731bc151e2484e92a05eeda2ef19cb24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4226e758be2c428a8b120db8289583c3",
            "max": 55320200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e9e5d3ec35034839a5cb8c62e0990639",
            "value": 55320200
          }
        },
        "3eb7ad77e1e6438fbc049831be45386a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9794fa75a5145b49d54e546d85174f1",
            "placeholder": "​",
            "style": "IPY_MODEL_adcab91c97794e82934de361c7574494",
            "value": " 55.3M/55.3M [00:01&lt;00:00, 41.4MB/s]"
          }
        },
        "f8a6731b9a2143a9a1ae658febeee7a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9920737715b45c5a9d94f968764cf4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "106422f6b78e4053bfb81bb84c1f2012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4226e758be2c428a8b120db8289583c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9e5d3ec35034839a5cb8c62e0990639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9794fa75a5145b49d54e546d85174f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adcab91c97794e82934de361c7574494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}