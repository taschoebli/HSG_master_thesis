{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnSuayJCd541"
      },
      "outputs": [],
      "source": [
        "#MODEL_TYPE = \"LSTM\" # Choose between LSTM, DeepAR and MOIRAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cu7IvMymwyM",
        "outputId": "9fd4a740-d5aa-4ec8-d072-917187c2748c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "N023cFtseXHa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d4b36260-aee1-43aa-e291-ba597c5baf71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM model type stuff\n",
            "DeepAR model type stuff\n",
            "Requirement already satisfied: gluonts[torch] in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy<2.2,>=1.16 in /usr/local/lib/python3.11/dist-packages (from gluonts[torch]) (1.23.5)\n",
            "Requirement already satisfied: pandas<3,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gluonts[torch]) (2.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.11/dist-packages (from gluonts[torch]) (2.10.6)\n",
            "Requirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.11/dist-packages (from gluonts[torch]) (4.67.1)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.11/dist-packages (from gluonts[torch]) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gluonts[torch]) (4.12.2)\n",
            "Requirement already satisfied: torch<3,>=1.9 in /usr/local/lib/python3.11/dist-packages (from gluonts[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: lightning<2.5,>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from gluonts[torch]) (2.4.0)\n",
            "Requirement already satisfied: pytorch-lightning<2.5,>=2.2.2 in /usr/local/lib/python3.11/dist-packages (from gluonts[torch]) (2.4.0)\n",
            "Requirement already satisfied: scipy~=1.10 in /usr/local/lib/python3.11/dist-packages (from gluonts[torch]) (1.14.1)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning<2.5,>=2.2.2->gluonts[torch]) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (2025.3.0)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2.5,>=2.2.2->gluonts[torch]) (0.14.2)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2.5,>=2.2.2->gluonts[torch]) (24.2)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning<2.5,>=2.2.2->gluonts[torch]) (1.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.0->gluonts[torch]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.0->gluonts[torch]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.0->gluonts[torch]) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts[torch]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts[torch]) (2.27.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=1.9->gluonts[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=1.9->gluonts[torch]) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (3.11.14)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning<2.5,>=2.2.2->gluonts[torch]) (75.1.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.0->gluonts[torch]) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=1.9->gluonts[torch]) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<2.5,>=2.2.2->gluonts[torch]) (3.10)\n",
            "Requirement already satisfied: gluonts[mxnet] in /usr/local/lib/python3.11/dist-packages (0.16.0)\n",
            "Requirement already satisfied: numpy<2.2,>=1.16 in /usr/local/lib/python3.11/dist-packages (from gluonts[mxnet]) (1.23.5)\n",
            "Requirement already satisfied: pandas<3,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gluonts[mxnet]) (2.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.11/dist-packages (from gluonts[mxnet]) (2.10.6)\n",
            "Requirement already satisfied: tqdm~=4.23 in /usr/local/lib/python3.11/dist-packages (from gluonts[mxnet]) (4.67.1)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.11/dist-packages (from gluonts[mxnet]) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gluonts[mxnet]) (4.12.2)\n",
            "Requirement already satisfied: mxnet~=1.7 in /usr/local/lib/python3.11/dist-packages (from gluonts[mxnet]) (1.9.1)\n",
            "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.11/dist-packages (from mxnet~=1.7->gluonts[mxnet]) (2.32.3)\n",
            "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.11/dist-packages (from mxnet~=1.7->gluonts[mxnet]) (0.8.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.0->gluonts[mxnet]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.0->gluonts[mxnet]) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.0->gluonts[mxnet]) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts[mxnet]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts[mxnet]) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.0->gluonts[mxnet]) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.20.0->mxnet~=1.7->gluonts[mxnet]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.20.0->mxnet~=1.7->gluonts[mxnet]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.20.0->mxnet~=1.7->gluonts[mxnet]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.20.0->mxnet~=1.7->gluonts[mxnet]) (2025.1.31)\n",
            "MOIRAI model type stuff\n",
            "fatal: destination path 'uni2ts' already exists and is not an empty directory.\n",
            "/content/uni2ts\n",
            "Obtaining file:///content/uni2ts\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting datasets~=2.17.1 (from uni2ts==1.2.0)\n",
            "  Using cached datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting einops==0.7.* (from uni2ts==1.2.0)\n",
            "  Using cached einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting gluonts~=0.14.3 (from uni2ts==1.2.0)\n",
            "  Using cached gluonts-0.14.4-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (0.29.3)\n",
            "Collecting hydra-core==1.3 (from uni2ts==1.2.0)\n",
            "  Using cached hydra_core-1.3.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: jax[cpu] in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (0.5.2)\n",
            "Collecting jaxtyping~=0.2.24 (from uni2ts==1.2.0)\n",
            "  Using cached jaxtyping-0.2.38-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: lightning>=2.0 in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (2.4.0)\n",
            "Collecting multiprocess (from uni2ts==1.2.0)\n",
            "  Using cached multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting numpy~=1.26.0 (from uni2ts==1.2.0)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (3.10.15)\n",
            "Collecting python-dotenv==1.0.0 (from uni2ts==1.2.0)\n",
            "  Using cached python_dotenv-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (0.5.3)\n",
            "Collecting scipy~=1.11.3 (from uni2ts==1.2.0)\n",
            "  Using cached scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (2.18.0)\n",
            "Collecting torch<2.5,>=2.1 (from uni2ts==1.2.0)\n",
            "  Using cached torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Collecting omegaconf~=2.2 (from hydra-core==1.3->uni2ts==1.2.0)\n",
            "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core==1.3->uni2ts==1.2.0)\n",
            "  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from hydra-core==1.3->uni2ts==1.2.0) (24.2)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (7.7.1)\n",
            "Collecting jupyter (from uni2ts==1.2.0)\n",
            "  Using cached jupyter-1.1.1-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from uni2ts==1.2.0) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets~=2.17.1->uni2ts==1.2.0)\n",
            "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets~=2.17.1->uni2ts==1.2.0)\n",
            "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (4.67.1)\n",
            "Collecting xxhash (from datasets~=2.17.1->uni2ts==1.2.0)\n",
            "  Using cached xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets~=2.17.1->uni2ts==1.2.0)\n",
            "  Using cached fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (3.11.14)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets~=2.17.1->uni2ts==1.2.0) (6.0.2)\n",
            "Collecting pandas (from datasets~=2.17.1->uni2ts==1.2.0)\n",
            "  Using cached pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.11/dist-packages (from gluonts~=0.14.3->uni2ts==1.2.0) (2.10.6)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.11/dist-packages (from gluonts~=0.14.3->uni2ts==1.2.0) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gluonts~=0.14.3->uni2ts==1.2.0) (4.12.2)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping~=0.2.24->uni2ts==1.2.0)\n",
            "  Using cached wadler_lindig-0.1.4-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0->uni2ts==1.2.0) (0.14.2)\n",
            "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0->uni2ts==1.2.0) (1.7.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0->uni2ts==1.2.0) (2.4.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<2.5,>=2.1->uni2ts==1.2.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.5,>=2.1->uni2ts==1.2.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<2.5,>=2.1->uni2ts==1.2.0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<2.5,>=2.1->uni2ts==1.2.0) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch<2.5,>=2.1->uni2ts==1.2.0)\n",
            "  Using cached triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<2.5,>=2.1->uni2ts==1.2.0) (12.4.127)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->uni2ts==1.2.0) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->uni2ts==1.2.0) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->uni2ts==1.2.0) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->uni2ts==1.2.0) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->uni2ts==1.2.0) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->uni2ts==1.2.0) (3.0.13)\n",
            "Requirement already satisfied: jaxlib<=0.5.2,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from jax[cpu]->uni2ts==1.2.0) (0.5.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax[cpu]->uni2ts==1.2.0) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax[cpu]->uni2ts==1.2.0) (3.4.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter->uni2ts==1.2.0) (6.5.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter->uni2ts==1.2.0) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter->uni2ts==1.2.0) (7.16.6)\n",
            "Collecting jupyterlab (from jupyter->uni2ts==1.2.0)\n",
            "  Using cached jupyterlab-4.3.6-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->uni2ts==1.2.0) (2.8.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting multiprocess (from uni2ts==1.2.0)\n",
            "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (3.7)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (75.1.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->uni2ts==1.2.0) (3.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets~=2.17.1->uni2ts==1.2.0) (1.18.3)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (6.4.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0)\n",
            "  Using cached jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (4.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets~=2.17.1->uni2ts==1.2.0) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets~=2.17.1->uni2ts==1.2.0) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts~=0.14.3->uni2ts==1.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts~=0.14.3->uni2ts==1.2.0) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets~=2.17.1->uni2ts==1.2.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets~=2.17.1->uni2ts==1.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets~=2.17.1->uni2ts==1.2.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets~=2.17.1->uni2ts==1.2.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->uni2ts==1.2.0) (3.0.2)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (5.7.2)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (5.10.4)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (0.21.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook->jupyter->uni2ts==1.2.0) (1.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->uni2ts==1.2.0) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->uni2ts==1.2.0) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->uni2ts==1.2.0) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->uni2ts==1.2.0) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->uni2ts==1.2.0) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->uni2ts==1.2.0) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert->jupyter->uni2ts==1.2.0) (1.5.1)\n",
            "Collecting async-lru>=1.0.0 (from jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached async_lru-2.0.5-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: httpx>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->uni2ts==1.2.0) (0.28.1)\n",
            "Collecting jupyter-lsp>=2.0.0 (from jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached jupyter_lsp-2.2.5-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting jupyter-server<3,>=2.4.0 (from jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached jupyter_server-2.15.0-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting jupyterlab-server<3,>=2.27.1 (from jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached jupyterlab_server-2.27.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: notebook-shim>=0.2 in /usr/local/lib/python3.11/dist-packages (from jupyterlab->jupyter->uni2ts==1.2.0) (0.2.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<2.5,>=2.1->uni2ts==1.2.0) (1.3.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert->jupyter->uni2ts==1.2.0) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert->jupyter->uni2ts==1.2.0) (1.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->uni2ts==1.2.0) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.0->jupyterlab->jupyter->uni2ts==1.2.0) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab->jupyter->uni2ts==1.2.0) (0.14.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (0.8.4)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core>=4.6.1->notebook->jupyter->uni2ts==1.2.0) (4.3.7)\n",
            "Collecting jupyter-client>=6.1.12 (from ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0)\n",
            "  Using cached jupyter_client-7.4.9-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting jupyter-events>=0.11.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached jupyter_events-0.12.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jupyter-server-terminals>=0.4.4 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached jupyter_server_terminals-0.5.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting overrides>=5.0 (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: websocket-client>=1.7 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0) (1.8.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook->jupyter->uni2ts==1.2.0) (21.2.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->uni2ts==1.2.0) (0.4)\n",
            "Requirement already satisfied: babel>=2.10 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->uni2ts==1.2.0) (2.17.0)\n",
            "Collecting json5>=0.9.0 (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached json5-0.10.0-py3-none-any.whl.metadata (34 kB)\n",
            "Requirement already satisfied: jsonschema>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->uni2ts==1.2.0) (4.23.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook->jupyter->uni2ts==1.2.0) (2.21.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets->uni2ts==1.2.0) (0.2.13)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->nbconvert->jupyter->uni2ts==1.2.0) (2.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.0->jupyterlab->jupyter->uni2ts==1.2.0) (1.3.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->uni2ts==1.2.0) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->uni2ts==1.2.0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->jupyterlab->jupyter->uni2ts==1.2.0) (0.23.1)\n",
            "Collecting python-json-logger>=2.0.4 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached python_json_logger-3.3.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting rfc3339-validator (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting rfc3986-validator>=0.1.1 (from jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->uni2ts==1.2.0) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->uni2ts==1.2.0) (2.22)\n",
            "Collecting fqdn (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting isoduration (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0) (3.0.0)\n",
            "Collecting uri-template (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached uri_template-1.3.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0) (24.11.1)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server<3,>=2.4.0->jupyterlab->jupyter->uni2ts==1.2.0)\n",
            "  Using cached types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Using cached einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "Using cached hydra_core-1.3.0-py3-none-any.whl (153 kB)\n",
            "Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Using cached datasets-2.17.1-py3-none-any.whl (536 kB)\n",
            "Using cached gluonts-0.14.4-py3-none-any.whl (1.5 MB)\n",
            "Using cached jaxtyping-0.2.38-py3-none-any.whl (56 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "Downloading torch-2.4.1-cp311-cp311-manylinux1_x86_64.whl (797.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.1/797.1 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter-1.1.1-py2.py3-none-any.whl (2.7 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.1.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wadler_lindig-0.1.4-py3-none-any.whl (20 kB)\n",
            "Downloading jupyterlab-4.3.6-py3-none-any.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_lru-2.0.5-py3-none-any.whl (6.1 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_lsp-2.2.5-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_server-2.15.0-py3-none-any.whl (385 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.8/385.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyter_client-7.4.9-py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.5/133.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupyterlab_server-2.27.3-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json5-0.10.0-py3-none-any.whl (34 kB)\n",
            "Downloading jupyter_events-0.12.0-py3-none-any.whl (19 kB)\n",
            "Downloading jupyter_server_terminals-0.5.3-py3-none-any.whl (13 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_json_logger-3.3.0-py3-none-any.whl (15 kB)\n",
            "Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: uni2ts, antlr4-python3-runtime\n",
            "  Building editable for uni2ts (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uni2ts: filename=uni2ts-1.2.0-py3-none-any.whl size=14551 sha256=b85b0eb983365fc98a02c6147d824cc69a48de3904986b020574455762232511\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-sieh8p4m/wheels/e4/09/f3/135225f0ca94e25a20e8e24c54f78dd98d712c8760e0d0900a\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=d9db602005096c7baf385b7e8e4837d8cf53e5b1991601b73ed91ea8983027e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "Successfully built uni2ts antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, xxhash, wadler-lindig, uri-template, types-python-dateutil, triton, rfc3986-validator, rfc3339-validator, python-json-logger, python-dotenv, pyarrow-hotfix, overrides, omegaconf, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, json5, jedi, fsspec, fqdn, einops, dill, async-lru, scipy, pandas, nvidia-cusolver-cu12, multiprocess, jupyter-server-terminals, jupyter-client, jaxtyping, hydra-core, arrow, torch, isoduration, gluonts, datasets, jupyter-events, uni2ts, jupyter-server, jupyterlab-server, jupyter-lsp, jupyterlab, jupyter\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.3.1.170\n",
            "    Uninstalling nvidia-cusparse-cu12-12.3.1.170:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.3.1.170\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.5.147\n",
            "    Uninstalling nvidia-curand-cu12-10.3.5.147:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.5.147\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.1.3\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.1.3:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.1.3\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.4.127\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.4.5.8\n",
            "    Uninstalling nvidia-cublas-cu12-12.4.5.8:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.4.5.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.1\n",
            "    Uninstalling einops-0.8.1:\n",
            "      Successfully uninstalled einops-0.8.1\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.1.9\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.1.9:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.1.9\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 6.1.12\n",
            "    Uninstalling jupyter-client-6.1.12:\n",
            "      Successfully uninstalled jupyter-client-6.1.12\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: gluonts\n",
            "    Found existing installation: gluonts 0.16.0\n",
            "    Uninstalling gluonts-0.16.0:\n",
            "      Successfully uninstalled gluonts-0.16.0\n",
            "  Attempting uninstall: jupyter-server\n",
            "    Found existing installation: jupyter-server 1.16.0\n",
            "    Uninstalling jupyter-server-1.16.0:\n",
            "      Successfully uninstalled jupyter-server-1.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.4 which is incompatible.\n",
            "mizani 0.13.1 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2023.10.0 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.4 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.4.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed antlr4-python3-runtime-4.9.3 arrow-1.3.0 async-lru-2.0.5 datasets-2.17.1 dill-0.3.8 einops-0.7.0 fqdn-1.5.1 fsspec-2023.10.0 gluonts-0.14.4 hydra-core-1.3.0 isoduration-20.11.0 jaxtyping-0.2.38 jedi-0.19.2 json5-0.10.0 jupyter-1.1.1 jupyter-client-7.4.9 jupyter-events-0.12.0 jupyter-lsp-2.2.5 jupyter-server-2.15.0 jupyter-server-terminals-0.5.3 jupyterlab-4.3.6 jupyterlab-server-2.27.3 multiprocess-0.70.16 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 omegaconf-2.3.0 overrides-7.7.0 pandas-2.1.4 pyarrow-hotfix-0.6 python-dotenv-1.0.0 python-json-logger-3.3.0 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 scipy-1.11.4 torch-2.4.1 triton-3.0.0 types-python-dateutil-2.9.0.20241206 uni2ts-1.2.0 uri-template-1.3.0 wadler-lindig-0.1.4 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "608b960e6a5b4c27b33f0bcd6a50c3b6"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initialize Directories and Configurations\n",
        "print(\"LSTM model type stuff\")\n",
        "\n",
        "print(\"DeepAR model type stuff\")\n",
        "# install with support for torch models\n",
        "!pip install \"gluonts[torch]\"\n",
        "# install with support for mxnet models\n",
        "!pip install \"gluonts[mxnet]\"\n",
        "\n",
        "print(\"MOIRAI model type stuff\")\n",
        "!git clone https://github.com/taschoebli/uni2ts.git\n",
        "%cd uni2ts\n",
        "!pip install -e '.[notebook]'\n",
        "#!pip install uni2ts\n",
        "!touch .env\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch torchvision torchaudio -y\n",
        "!pip cache purge\n",
        "!pip install torch torchvision\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vF8ehEYXqPA9",
        "outputId": "ea531ea1-a990-4fb7-8a2e-b8497f8212b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.4.1\n",
            "Uninstalling torch-2.4.1:\n",
            "  Successfully uninstalled torch-2.4.1\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Files removed: 419\n",
            "Collecting torch\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting triton==3.2.0 (from torch)\n",
            "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.1.105\n",
            "    Uninstalling nvidia-nvtx-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.20.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.20.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.20.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.1.0.106\n",
            "    Uninstalling nvidia-cusparse-cu12-12.1.0.106:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.1.0.106\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.2.106\n",
            "    Uninstalling nvidia-curand-cu12-10.3.2.106:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.2.106\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.0.2.54\n",
            "    Uninstalling nvidia-cufft-cu12-11.0.2.54:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.0.2.54\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.1.105\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.1.105:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.1.105\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.1.3.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.1.3.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.1.3.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.4.5.107\n",
            "    Uninstalling nvidia-cusolver-cu12-11.4.5.107:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.4.5.107\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "uni2ts 1.2.0 requires torch<2.5,>=2.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.4.127 torch-2.6.0 torchvision-0.21.0 triton-3.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchgen",
                  "triton"
                ]
              },
              "id": "64eef4e568ea473b8e83610221151284"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6YkS5WEYYYx",
        "outputId": "5b8567a1-e3e9-44ad-9efc-0b4b31d8451f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "General imports loaded\n",
            "LSTM specific imports loaded\n",
            "DeepAR specific imports loaded\n",
            "MOIRAI specific imports loaded\n"
          ]
        }
      ],
      "source": [
        "#General imports\n",
        "print(\"General imports loaded\")\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import math\n",
        "from torch import nn\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "#if MODEL_TYPE == \"LSTM\":\n",
        "print(\"LSTM specific imports loaded\")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#elif MODEL_TYPE == \"DeepAR\":\n",
        "print(\"DeepAR specific imports loaded\")\n",
        "from gluonts.torch import DeepAREstimator\n",
        "from gluonts.dataset.pandas import PandasDataset\n",
        "from gluonts.dataset.split import split\n",
        "\n",
        "#elif MODEL_TYPE == \"MOIRAI\":\n",
        "print(\"MOIRAI specific imports loaded\")\n",
        "from huggingface_hub import hf_hub_download\n",
        "from uni2ts.model.moirai import MoiraiForecast, MoiraiModule\n",
        "from uni2ts.model.moirai_moe import MoiraiMoEForecast, MoiraiMoEModule\n",
        "from gluonts.dataset.pandas import PandasDataset\n",
        "from gluonts.dataset.split import split\n",
        "\n",
        "#else:\n",
        "  #print(\"Invalid model type\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "af2SkhDQZ0Bv"
      },
      "outputs": [],
      "source": [
        "# Functions, Classes and other Helpers\n",
        "\n",
        "# General ----------------------------------------------------------------------\n",
        "\n",
        "# Load data from csv\n",
        "def load_data(filepath):\n",
        "    data = pd.read_csv(filepath, index_col=0, parse_dates=True)\n",
        "    return data\n",
        "\n",
        "# Filename extractor\n",
        "def file_name_no_extension(filepath):\n",
        "  # Extract the filename without the folder path\n",
        "  filename_with_extension = os.path.basename(filepath)\n",
        "\n",
        "  # Remove the `.csv` extension\n",
        "  filename_without_extension = os.path.splitext(filename_with_extension)[0]\n",
        "\n",
        "  return filename_without_extension\n",
        "\n",
        "\n",
        "# Day extractor\n",
        "def get_values_by_day(df_internal, offset=0):\n",
        "  # Ensure index is datetime\n",
        "  if not isinstance(df_internal.index, pd.DatetimeIndex):\n",
        "      df_internal.index = pd.to_datetime(df_internal.index)\n",
        "\n",
        "  # Apply offset by removing the last 'offset' rows\n",
        "  if offset > 0:\n",
        "      df_internal = df_internal.iloc[:-offset]\n",
        "\n",
        "  # Filter timestamps to only include those at exactly 08:00\n",
        "  df_filtered = df_internal[df_internal.index.hour == 8]\n",
        "\n",
        "  # Group by date and store in a dictionary\n",
        "  #result = {}\n",
        "  #for date, group in df_filtered.groupby(df_filtered.index.date):\n",
        "      #result[str(date)] = group.index.tolist()\n",
        "  # Extract timestamps as strings instead of lists\n",
        "  result = {group.index[0].strftime('%Y-%m-%d %H:%M:%S') for date, group in df_filtered.groupby(df_filtered.index.date)}\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "# Calculate SMAPE metric\n",
        "def calc_smape(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Calculate the Symmetric Mean Absolute Percentage Error (SMAPE).\n",
        "  y_true: array-like, actual values\n",
        "  y_pred: array-like, predicted values\n",
        "  \"\"\"\n",
        "  epsilon = 1e-10  # Small constant to avoid division by zero\n",
        "  denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
        "  smape_values = np.abs(y_true - y_pred) / (denominator + epsilon)\n",
        "  smape_values = np.nan_to_num(smape_values, nan=0.0, posinf=0.0, neginf=0.0)  # Handle division by zero\n",
        "\n",
        "  return np.mean(smape_values) * 100  # Convert to percentage\n",
        "\n",
        "def optimize_memory():\n",
        "  # Memory optimization\n",
        "  # Set CUDA memory management configuration to avoid fragmentation\n",
        "  # https://pytorch.org/docs/stable/notes/cuda.html#using-custom-memory-allocators-for-cuda\n",
        "  os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "  # Check if CUDA is available\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(f\"Using device: {device}\")\n",
        "  torch.cuda.empty_cache()\n",
        "  os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"]\n",
        "\n",
        "\n",
        "# LSTM specific ----------------------------------------------------------------\n",
        "\n",
        "# Create sequences for time-series prediction\n",
        "def create_sequences(features, target, context_length, prediction_length):\n",
        "    X_seq, y_seq = [], []\n",
        "    for i in range(len(features) - context_length - prediction_length + 1):\n",
        "        X_seq.append(features[i:i+context_length])  # Past `context_length` values\n",
        "        y_seq.append(target[i+context_length:i+context_length+prediction_length])  # Next `prediction_length` values\n",
        "    return np.array(X_seq), np.array(y_seq)\n",
        "\n",
        "# Define the LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        out = self.fc(hidden[-1])  # Use the last hidden state\n",
        "        return out\n",
        "\n",
        "def train_LSTM_model(model, train_loader, criterion, optimizer, num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.4f}')\n",
        "    return epoch_loss/len(train_loader)\n",
        "\n",
        "def evaluate_LSTM_model(model, test_loader):\n",
        "    model.eval()\n",
        "    predictions, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            outputs = model(X_batch)\n",
        "            predictions.extend(outputs.tolist())\n",
        "            actuals.extend(y_batch.tolist())\n",
        "    # Feature added here!!!\n",
        "    # Truncate negative predictions to 0\n",
        "    predictions = np.maximum(predictions, 0)\n",
        "    return np.array(predictions), np.array(actuals)\n",
        "\n",
        "def get_index_and_timestamp_for_validation(hour, original_df, train_size):\n",
        "    \"\"\"\n",
        "    Finds the first available testdata tensor index that matches the given hour.\n",
        "\n",
        "    Parameters:\n",
        "    - hour (int): The desired hour (0-23).\n",
        "    - original_df (pd.DataFrame): The original dataset with a datetime index.\n",
        "    - train_size (int): The number of samples in the training dataset + Context Length.\n",
        "\n",
        "    Returns:\n",
        "    - test_index (int): The index relative to testdata tensor.\n",
        "    - timestamp (str): The corresponding datetime string.\n",
        "    \"\"\"\n",
        "    # Ensure datetime format\n",
        "    original_df.index = pd.to_datetime(original_df.index)\n",
        "\n",
        "    # Extract test dataset portion\n",
        "    test_df = original_df.iloc[train_size:]  # This contains only the test set\n",
        "\n",
        "    # Find the first row where the hour matches\n",
        "    for i, timestamp in enumerate(test_df.index):\n",
        "        if timestamp.hour == hour:\n",
        "            test_index = i  # Relative index in testdata tensor\n",
        "            return test_index, timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    raise ValueError(f\"No test data found at hour {hour}:00.\")\n",
        "\n",
        "\n",
        "def truncate_tensor_with_interval(tensor, start_index, offset=1):\n",
        "    \"\"\"\n",
        "    Truncates the given tensor along the first dimension, starting at start_index,\n",
        "    and selects every `offset`-th row.\n",
        "\n",
        "    Parameters:\n",
        "    - tensor (torch.Tensor): The input tensor (shape: [X, Y]).\n",
        "    - start_index (int): The base index from which truncation starts.\n",
        "    - offset (int): Step interval to select elements (e.g., every 4th, 12th, etc.).\n",
        "\n",
        "    Returns:\n",
        "    - truncated_tensor (torch.Tensor): The truncated tensor with selected intervals.\n",
        "    \"\"\"\n",
        "    if start_index >= tensor.shape[0]:\n",
        "        raise ValueError(\"Start index exceeds tensor dimensions.\")\n",
        "\n",
        "    # Make sure that below 24 hours, offset is set to one day!\n",
        "    if offset < 24:\n",
        "      offset = 24\n",
        "\n",
        "    return tensor[start_index::offset]  # Truncate and select every `offset` step\n",
        "\n",
        "def data_validation_LSTM(df, actuals, X_train, CONTEXT_LENGTH, PREDICTION_LENGTH, hour=8):\n",
        "    \"\"\"\n",
        "    Validates the actual LSTM predictions against the true values in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The original dataset with a datetime index.\n",
        "    - actuals (torch.Tensor): The model's predicted values (shape: [X, Y]).\n",
        "    - X_train (numpy array or tensor): The training dataset.\n",
        "    - CONTEXT_LENGTH (int): The number of past timesteps used for forecasting.\n",
        "    - PREDICTION_LENGTH (int): The number of future timesteps predicted.\n",
        "    - hour (int): The desired hour (default: 8 AM).\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Get the test index and timestamp\n",
        "    INDEX, timestamp = get_index_and_timestamp_for_validation(hour, df, train_size=len(X_train) + CONTEXT_LENGTH)\n",
        "\n",
        "    # Step 2: Extract predictions using the computed index\n",
        "    actuals_daily = truncate_tensor_with_interval(actuals, INDEX, offset=PREDICTION_LENGTH)\n",
        "\n",
        "    # Step 3: Retrieve true values from `df['OT']`\n",
        "    start_index = len(X_train) + CONTEXT_LENGTH + INDEX\n",
        "    true_values = df['OT'].iloc[start_index:start_index + PREDICTION_LENGTH].values  # Convert to NumPy array\n",
        "\n",
        "    # Step 4: Format values for comparison\n",
        "    formatted_actuals = np.round(actuals_daily[0], 1)  # Convert tensor to NumPy & round\n",
        "    formatted_true_values = np.round(true_values, 1)  # Round true values for comparison\n",
        "\n",
        "    # Step 5: Print Debugging Information\n",
        "    print(\"🔍 Validation at timestamp:\", timestamp)\n",
        "    print(\"Predicted Values (Rounded):\")\n",
        "    print(f\"{formatted_actuals}\")\n",
        "    print(\"True Values (Rounded):\")\n",
        "    print(f\"{formatted_true_values}\")\n",
        "\n",
        "    # Step 6: Perform Validation Check\n",
        "    validation_passed = np.array_equal(formatted_actuals, formatted_true_values)\n",
        "\n",
        "    print(\"\\n✅ Validation Passed\" if validation_passed else \"\\n❌ Validation Failed\")\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "def days_between_dates(date_str1: str, date_str2: str, date_format: str = \"%Y-%m-%d %H:%M:%S\") -> int:\n",
        "    \"\"\"\n",
        "    Returns the number of days between two datetime strings.\n",
        "\n",
        "    :param date_str1: First date string\n",
        "    :param date_str2: Second date string\n",
        "    :param date_format: Format of the date strings (default: \"%Y-%m-%d %H:%M:%S\")\n",
        "    :return: Number of days between the two dates (absolute value)\n",
        "    \"\"\"\n",
        "    date1 = datetime.strptime(date_str1, date_format)\n",
        "    date2 = datetime.strptime(date_str2, date_format)\n",
        "    return abs((date2 - date1).days)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b_AgDYt-8_z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7oYYV-DYpgB",
        "outputId": "d4a73184-0d23-4b95-ca78-6e37b6ba96f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM model definitions and constants loaded\n",
            "DeepAR model type stuff\n",
            "MOIRAI model type stuff\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# Definitions and Constants\n",
        "FILEPATH = '/content/sample_data/160701_180626_ETTh1.csv'\n",
        "PREDICTION_LENGTH_LIST = [96] # Use the past 4, 12, 24, 48, 96, 336 time steps for prediction (4 hours to 14 days)\n",
        "CONTEXT_LENGTH_FOLD_LIST = [10, 21, 32, 42, 52] # Use 1, 2, 4 and 8 fold PREDICTION_LENGTH\n",
        "BEGINNING_TIMESTAMP = \"2017-07-04 08:00:00\"\n",
        "START_HOUR = 8 # Use to start plots at this hour\n",
        "TRAIN_DATA_LAST_INDEX = 8786 # Corresponds to '2024-04-01 00:00:00', used for LSTM and DeepAR in 230401_250108_PT1H_Solcast_reduced_features.csv\n",
        "\n",
        "print(\"LSTM model definitions and constants loaded\")\n",
        "LSTM_HIDDEN_SIZE = 64  # Number of hidden units -> # of neurons in the LSTM's hidden layer\n",
        "LSTM_BATCH_SIZE = 64  # Batch size -> # of samples processed in parallel\n",
        "LSTM_NUM_LAYERS = 2  # Number of LSTM layers -> # of stacked LSTM layers\n",
        "LSTM_LR = 1e-3\n",
        "LSTM_NUM_EPOCHS = 25\n",
        "\n",
        "print(\"DeepAR model type stuff\")\n",
        "DeepAR_HIDDEN_SIZE = 64 # Number of RNN cells for each layer (default: 40)\n",
        "DeepAR_NUM_LAYERS = 2 # Number of RNN layers (default: 2)\n",
        "DeepAR_LR = 1e-3\n",
        "DeepAR_NUM_EPOCHS = 25\n",
        "\n",
        "print(\"MOIRAI model type stuff\")\n",
        "MOIRAI_PATCH_SIZE = 32 # patch size, Number of samples for each layer or sequence, 32 or 64 recommended for hourly\n",
        "MOIRAI_BATCH_SIZE = 16 # batch size, samples processed in parallel\n",
        "MOIRAI_MODEL = \"moirai-1.1-R\"  # model name: choose from {'moirai-1.1-R', 'moirai-moe-1.0-R'}\n",
        "MOIRAI_SIZE = \"base\"  # model size: choose from {'small', 'base', 'large'}\n",
        "MOIRAI_MODEL_STR = f\"Salesforce/{MOIRAI_MODEL}-{MOIRAI_SIZE}\"\n",
        "MOIRAI_MODEL_PATH_SAFE = MOIRAI_MODEL_STR.replace(\"Salesforce/\", \"\").replace(\"/\", \"-\").replace(\".\", \"-\")\n",
        "\n",
        "optimize_memory()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eOuzkZPrXDYf"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load data from csv\n",
        "df = load_data(FILEPATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "id": "AwyPxPrUcrFi",
        "outputId": "fc2dc48f-65d6-41c8-adbd-a11d52843bf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM feature scaling\n",
            "DeepAR OR MOIRAI feature scaling\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# For Unisolar\\ndataset = PandasDataset(df, target=\"dc_power\",\\n                      past_feat_dynamic_real=[\\'dc_power\\', \\'CloudOpacity\\', \\'Ghi\\', \\'ApparentTemperature\\', \\'AirTemperature\\', \\'DewPointTemperature\\', \\'RelativeHumidity\\', \\'WindSpeed\\', \\'WindDirection\\'],\\n                      feat_dynamic_real=[\\'CloudOpacity\\', \\'Ghi\\', \\'ApparentTemperature\\', \\'AirTemperature\\', \\'DewPointTemperature\\', \\'RelativeHumidity\\', \\'WindSpeed\\', \\'WindDirection\\'])\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "## Step 2: Feature Scaling\n",
        "print(\"LSTM feature scaling\")\n",
        "# Separate features and target\n",
        "LSTM_target_column = 'OT'\n",
        "\n",
        "# For Solcast\n",
        "LSTM_features = [\n",
        "    'HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL'\n",
        "]\n",
        "\"\"\"\n",
        "# For Unisolar\n",
        "LSTM_features = [\n",
        "    'CloudOpacity', 'Ghi', 'ApparentTemperature', 'AirTemperature', 'DewPointTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection'\n",
        "]\n",
        "\"\"\"\n",
        "X = df[LSTM_features]\n",
        "y = df[LSTM_target_column]\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "#scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Convert target to a numpy array\n",
        "y = y.to_numpy()\n",
        "\n",
        "print(\"DeepAR OR MOIRAI feature scaling\")\n",
        "# https://ts.gluon.ai/dev/api/gluonts/gluonts.dataset.pandas.html\n",
        "# Convert into GluonTS dataset with features\n",
        "\n",
        "# For Solcast\n",
        "# For ETTH1\n",
        "dataset = PandasDataset(df, target=\"OT\",\n",
        "                        past_feat_dynamic_real=['OT', 'HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL'],\n",
        "                        feat_dynamic_real=['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL'])\n",
        "\"\"\"\n",
        "# For Unisolar\n",
        "dataset = PandasDataset(df, target=\"dc_power\",\n",
        "                      past_feat_dynamic_real=['dc_power', 'CloudOpacity', 'Ghi', 'ApparentTemperature', 'AirTemperature', 'DewPointTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection'],\n",
        "                      feat_dynamic_real=['CloudOpacity', 'Ghi', 'ApparentTemperature', 'AirTemperature', 'DewPointTemperature', 'RelativeHumidity', 'WindSpeed', 'WindDirection'])\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD2g7m_5hOaq",
        "outputId": "c42b3b7e-a365-4a6d-bbb6-e497587f632a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM model - Main block\n"
          ]
        }
      ],
      "source": [
        "# Main block - run for several iterations\n",
        "\n",
        "for prediction_length in sorted(PREDICTION_LENGTH_LIST):\n",
        "  for context_length_fold in sorted(CONTEXT_LENGTH_FOLD_LIST):\n",
        "    PREDICTION_LENGTH = prediction_length\n",
        "    CONTEXT_LENGTH = context_length_fold * PREDICTION_LENGTH\n",
        "\n",
        "    print(\"LSTM model - Main block\")\n",
        "    # Step 3: Create sequences\n",
        "    X_seq, y_seq = create_sequences(X_scaled, y, CONTEXT_LENGTH, PREDICTION_LENGTH)\n",
        "\n",
        "    # Step 4: Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, train_size=TRAIN_DATA_LAST_INDEX, random_state=None, shuffle=False)\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "    # Step 5: Create DataLoader for batch processing\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=LSTM_BATCH_SIZE, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=LSTM_BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    # Step 6: Create the model\n",
        "    input_size = X_train.shape[2]  # Number of features -> 7 eg. 'air_temp', 'albedo', etc.\n",
        "    model = LSTMModel(input_size, LSTM_HIDDEN_SIZE, LSTM_NUM_LAYERS, output_size=PREDICTION_LENGTH)\n",
        "\n",
        "    # Step 7: Define the loss function and optimizer\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), LSTM_LR)\n",
        "\n",
        "    # Step 8: Train the model\n",
        "    LOSS = train_LSTM_model(model, train_loader, criterion, optimizer, LSTM_NUM_EPOCHS)\n",
        "\n",
        "    # Step 9: Evaluate the model\n",
        "    LSTM_predictions, LSTM_actuals = evaluate_LSTM_model(model, test_loader)\n",
        "    data_validation_LSTM(df, LSTM_actuals, X_train, CONTEXT_LENGTH, PREDICTION_LENGTH, hour=START_HOUR)\n",
        "    INDEX, TIMESTAMP = get_index_and_timestamp_for_validation(START_HOUR, df, train_size=len(X_train) + CONTEXT_LENGTH)\n",
        "    #TIMESTAMP_DATE = TIMESTAMP.split(\" \")[0]\n",
        "    #LSTM_actuals_daily = truncate_tensor_with_interval(LSTM_actuals, INDEX, offset=PREDICTION_LENGTH)\n",
        "    #LSTM_predictions_daily = truncate_tensor_with_interval(LSTM_predictions, INDEX, offset=PREDICTION_LENGTH)\n",
        "\n",
        "    # To allign with MOIRAI and DeepAR\n",
        "    LSTM_PREDICTIONS_INDEX_SHIFTED = days_between_dates(TIMESTAMP, BEGINNING_TIMESTAMP) * 24\n",
        "\n",
        "    # Step 10: Calc metrics\n",
        "\n",
        "    LSTM_rmse = np.sqrt(np.mean((LSTM_predictions - LSTM_actuals)**2))\n",
        "    LSTM_mae = mean_absolute_error(LSTM_actuals, LSTM_predictions)\n",
        "    LSTM_r2 = r2_score(LSTM_actuals, LSTM_predictions)\n",
        "    LSTM_smape = calc_smape(LSTM_actuals, LSTM_predictions)\n",
        "    print(f\"PL={PREDICTION_LENGTH}, CL={CONTEXT_LENGTH}, Model=LSTM\")\n",
        "    #print(f\"Length of: RMSE={len(LSTM_rmse_list)}, MAE={len(LSTM_mae_list)}, SMAPE={len(LSTM_smape_list)}, R2={len(LSTM_r2_list)}\")\n",
        "    #print(f\"Mean of: RMSE={np.mean(LSTM_rmse_list):.4f}, MAE={np.mean(LSTM_mae_list):.4f}, SMAPE={np.mean(LSTM_smape_list):.2f}%, R^2={np.mean(LSTM_r2_list):.4f}\")\n",
        "    #LSTM_RMSE_MED = np.median(LSTM_rmse_list)\n",
        "    #LSTM_MAE_MED = np.median(LSTM_mae_list)\n",
        "    #LSTM_SMAPE_MED = np.median(LSTM_smape_list)\n",
        "    #LSTM_R2_MED = np.median(LSTM_r2_list)\n",
        "    #print(f\"Median of: RMSE={LSTM_RMSE_MED:.4f}, MAE={LSTM_MAE_MED:.4f}, SMAPE={LSTM_SMAPE_MED:.2f}%, R^2={LSTM_R2_MED:.4f}\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "    print(\"DeepAR model type stuff\")\n",
        "\n",
        "    # Step 3: Split data into training and testing sets\n",
        "    TESTDATA_LENGTH = len(df) - TRAIN_DATA_LAST_INDEX\n",
        "\n",
        "    # Step 4: Create test sequences for time-series prediction\n",
        "    training_data, test_gen = split(dataset, date=pd.Period(BEGINNING_TIMESTAMP, freq=\"h\"))\n",
        "    test_data = test_gen.generate_instances(prediction_length=PREDICTION_LENGTH, windows=1)\n",
        "\n",
        "    # Step5: Define and Train the model\n",
        "    model = DeepAREstimator(\n",
        "        prediction_length=PREDICTION_LENGTH, context_length=CONTEXT_LENGTH, freq=dataset.freq,\n",
        "        trainer_kwargs={\"max_epochs\": DeepAR_NUM_EPOCHS},\n",
        "        num_feat_dynamic_real=dataset.num_feat_dynamic_real,\n",
        "        num_layers=DeepAR_NUM_LAYERS,\n",
        "        hidden_size=DeepAR_HIDDEN_SIZE,\n",
        "        lr=DeepAR_LR,\n",
        "    ).train(training_data)\n",
        "\n",
        "    # Step 6: Get probabilistic predictions\n",
        "    forecasts = list(model.predict(test_data.input))\n",
        "\n",
        "    # Step 7: Get point predictions\n",
        "    # Get predictions from forecasts\n",
        "    predictions = forecasts[0].mean_ts\n",
        "\n",
        "    # Feature added here!!!\n",
        "    # Truncate negative predictions to 0\n",
        "    predictions = np.maximum(predictions, 0)\n",
        "\n",
        "    # Get actuals for metric calculations later\n",
        "    actuals = df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH][\"OT\"]\n",
        "\n",
        "    # Change type\n",
        "    DeepAR_predictions = predictions.to_numpy()\n",
        "    DeepAR_actuals = actuals.to_numpy()\n",
        "\n",
        "    # Step 8: Do evaluations\n",
        "    DeepAR_rmse = np.sqrt(np.mean((DeepAR_predictions - DeepAR_actuals)**2))\n",
        "    DeepAR_mae = mean_absolute_error(DeepAR_actuals, DeepAR_predictions)\n",
        "    DeepAR_r2 = r2_score(DeepAR_actuals, DeepAR_predictions)\n",
        "    DeepAR_smape = calc_smape(DeepAR_actuals, DeepAR_predictions)\n",
        "\n",
        "    print(f\"PL={PREDICTION_LENGTH}, CL={CONTEXT_LENGTH}, Model=DeepAR\")\n",
        "    #print(f\"Length of: RMSE={len(DeepAR_rmse_list)}, MAE={len(DeepAR_mae_list)}, SMAPE={len(DeepAR_smape_list)}, R2={len(DeepAR_r2_list)}\")\n",
        "    #print(f\"Mean of: RMSE={np.mean(DeepAR_rmse_list):.4f}, MAE={np.mean(DeepAR_mae_list):.4f}, SMAPE={np.mean(DeepAR_smape_list):.2f}%, R^2={np.mean(DeepAR_r2_list):.4f}\")\n",
        "    #DeepAR_RMSE_MED = np.median(DeepAR_rmse_list)\n",
        "    #DeepAR_MAE_MED = np.median(DeepAR_mae_list)\n",
        "    #DeepAR_SMAPE_MED = np.median(DeepAR_smape_list)\n",
        "    #DeepAR_R2_MED = np.median(DeepAR_r2_list)\n",
        "    #print(f\"Median of: RMSE={DeepAR_RMSE_MED:.4f}, MAE={DeepAR_MAE_MED:.4f}, SMAPE={DeepAR_SMAPE_MED:.2f}%, R^2={DeepAR_R2_MED:.4f}\")\n",
        "    #print(\"\")\n",
        "\n",
        "    # Prepare for Plot\n",
        "    # Get prediction intervals\n",
        "    DeepAR_lower_50 = forecasts[0].quantile(0.25)  # 25th percentile (lower bound)\n",
        "    DeepAR_upper_50 = forecasts[0].quantile(0.75)  # 75th percentile (upper bound)\n",
        "    DeepAR_lower_90 = forecasts[0].quantile(0.05)  # 5th percentile (lower bound)\n",
        "    DeepAR_upper_90 = forecasts[0].quantile(0.95)  # 95th percentile (upper bound)\n",
        "\n",
        "\n",
        "    print(\"MOIRAI model type stuff\")\n",
        "\n",
        "    # Step 2.1: Prepare pre-trained model by downloading model weights from huggingface hub\n",
        "    if \"moirai-moe\" in MOIRAI_MODEL_STR:\n",
        "      model = MoiraiMoEForecast(\n",
        "        module=MoiraiMoEModule.from_pretrained(MOIRAI_MODEL_STR),\n",
        "        prediction_length=PREDICTION_LENGTH,\n",
        "        context_length=CONTEXT_LENGTH,\n",
        "        patch_size=MOIRAI_PATCH_SIZE,\n",
        "        num_samples=100,\n",
        "        target_dim=1,\n",
        "        feat_dynamic_real_dim=dataset.num_feat_dynamic_real,\n",
        "        past_feat_dynamic_real_dim=dataset.num_past_feat_dynamic_real,\n",
        "      )\n",
        "      #print(\"MOE used\")\n",
        "    else:\n",
        "      model = MoiraiForecast(\n",
        "        module=MoiraiModule.from_pretrained(MOIRAI_MODEL_STR),\n",
        "        prediction_length=PREDICTION_LENGTH,\n",
        "        context_length=CONTEXT_LENGTH,\n",
        "        patch_size=MOIRAI_PATCH_SIZE,\n",
        "        num_samples=100,\n",
        "        target_dim=1,\n",
        "        feat_dynamic_real_dim=dataset.num_feat_dynamic_real,\n",
        "        past_feat_dynamic_real_dim=dataset.num_past_feat_dynamic_real,\n",
        "      )\n",
        "      #print(\"No MOE used\")\n",
        "\n",
        "    '''\n",
        "\n",
        "    # Step 2.2: Prepare fine-tuned model by loading model weights from ckpt file\n",
        "    checkpoint_path = \"/content/last.ckpt\"\n",
        "\n",
        "    # Load the model\n",
        "    model = MoiraiForecast.load_from_checkpoint(checkpoint_path,\n",
        "    prediction_length=PREDICTION_LENGTH,\n",
        "    context_length=CONTEXT_LENGTH,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    num_samples=100,\n",
        "    target_dim=1,\n",
        "    feat_dynamic_real_dim=dataset.num_feat_dynamic_real,\n",
        "    past_feat_dynamic_real_dim=dataset.num_past_feat_dynamic_real)\n",
        "    '''\n",
        "\n",
        "    # Step 6: Get probabilistic predictions\n",
        "    predictor = model.create_predictor(batch_size=MOIRAI_BATCH_SIZE)\n",
        "    forecasts = list(predictor.predict(test_data.input))\n",
        "\n",
        "    # Step 7: Get point predictions\n",
        "    # Get predictions from forecasts\n",
        "    predictions = forecasts[0].mean_ts\n",
        "    # Feature added here!!!\n",
        "    # Truncate negative predictions to 0\n",
        "    predictions = np.maximum(predictions, 0)\n",
        "\n",
        "    # Get actuals for metric calculations later\n",
        "    actuals = df.loc[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH][\"OT\"]\n",
        "\n",
        "    # Change type\n",
        "    MOIRAI_predictions = predictions.to_numpy()\n",
        "    MOIRAI_actuals = actuals.to_numpy()\n",
        "\n",
        "    # Step 8: Do evaluations\n",
        "    MOIRAI_rmse = np.sqrt(np.mean((MOIRAI_predictions - MOIRAI_actuals)**2))\n",
        "    MOIRAI_mae = mean_absolute_error(MOIRAI_actuals, MOIRAI_predictions)\n",
        "    MOIRAI_r2 = r2_score(MOIRAI_actuals, MOIRAI_predictions)\n",
        "    MOIRAI_smape = calc_smape(MOIRAI_actuals, MOIRAI_predictions)\n",
        "\n",
        "    print(f\"PL={PREDICTION_LENGTH}, CL={CONTEXT_LENGTH}, Model={MOIRAI_MODEL_PATH_SAFE}\")\n",
        "    #print(f\"Length of: RMSE={len(MOIRAI_rmse_list)}, MAE={len(MOIRAI_mae_list)}, SMAPE={len(MOIRAI_smape_list)}, R2={len(MOIRAI_r2_list)}\")\n",
        "    #print(f\"Mean of: RMSE={np.mean(MOIRAI_rmse_list):.4f}, MAE={np.mean(MOIRAI_mae_list):.4f}, SMAPE={np.mean(MOIRAI_smape_list):.2f}%, R^2={np.mean(MOIRAI_r2_list):.4f}\")\n",
        "    #MOIRAI_RMSE_MED = np.median(MOIRAI_rmse_list)\n",
        "    #MOIRAI_MAE_MED = np.median(MOIRAI_mae_list)\n",
        "    #MOIRAI_SMAPE_MED = np.median(MOIRAI_smape_list)\n",
        "    #MOIRAI_R2_MED = np.median(MOIRAI_r2_list)\n",
        "    #print(f\"Median of: RMSE={MOIRAI_RMSE_MED:.4f}, MAE={MOIRAI_MAE_MED:.4f}, SMAPE={MOIRAI_SMAPE_MED:.2f}%, R^2={MOIRAI_R2_MED:.4f}\")\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "    # Prepare for Plot\n",
        "    # Get prediction intervals\n",
        "    MOIRAI_lower_50 = forecasts[0].quantile(0.25)  # 25th percentile (lower bound)\n",
        "    MOIRAI_upper_50 = forecasts[0].quantile(0.75)  # 75th percentile (upper bound)\n",
        "    MOIRAI_lower_90 = forecasts[0].quantile(0.05)  # 5th percentile (lower bound)\n",
        "    MOIRAI_upper_90 = forecasts[0].quantile(0.95)  # 95th percentile (upper bound)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Plot filtered data all togehter\n",
        "    plt.plot(df[BEGINNING_TIMESTAMP:][\"OT\"].iloc[:PREDICTION_LENGTH], color=\"black\", linestyle=\"--\", label=\"True values\")\n",
        "    plt.plot(df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH].index, MOIRAI_predictions, color=\"orange\", label=\"MOIRAI Predictions\")\n",
        "    plt.plot(df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH].index, DeepAR_predictions, color=\"blue\", label=\"DeepAR Predictions\")\n",
        "    plt.plot(df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH].index, LSTM_predictions[(LSTM_PREDICTIONS_INDEX_SHIFTED-1)+START_HOUR], color=\"green\", label=\"LSTM Predictions\")\n",
        "\n",
        "    # Add 50% Prediction Interval (Shaded)\n",
        "    plt.fill_between(\n",
        "        df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH].index,\n",
        "        MOIRAI_lower_50[len(MOIRAI_lower_50)-len(df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH]):],\n",
        "        MOIRAI_upper_50[len(MOIRAI_upper_50)-len(df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH]):],\n",
        "        color=\"orange\", alpha=0.2, label=\"MOIRAI 50% PI\"\n",
        "    )\n",
        "\n",
        "    # Add 90% Prediction Interval (Shaded)\n",
        "    plt.fill_between(\n",
        "        df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH].index,\n",
        "        MOIRAI_lower_90[len(MOIRAI_lower_90)-len(df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH]):],\n",
        "        MOIRAI_upper_90[len(MOIRAI_upper_90)-len(df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH]):],\n",
        "        color=\"orange\", alpha=0.075, label=\"MOIRAI 90% PI\"\n",
        "    )\n",
        "    # Add 50% Prediction Interval (Shaded)\n",
        "    plt.fill_between(\n",
        "        df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH].index,\n",
        "        DeepAR_lower_50[len(DeepAR_lower_50)-len(df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH]):],\n",
        "        DeepAR_upper_50[len(DeepAR_upper_50)-len(df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH]):],\n",
        "        color=\"blue\", alpha=0.2, label=\"DeepAR 50% PI\"\n",
        "    )\n",
        "\n",
        "    # Add 90% Prediction Interval (Shaded)\n",
        "    plt.fill_between(\n",
        "        df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH].index,\n",
        "        DeepAR_lower_90[len(DeepAR_lower_90)-len(df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH]):],\n",
        "        DeepAR_upper_90[len(DeepAR_upper_90)-len(df[BEGINNING_TIMESTAMP:].iloc[:PREDICTION_LENGTH]):],\n",
        "        color=\"blue\", alpha=0.075, label=\"DeepAR 90% PI\"\n",
        "    )\n",
        "\n",
        "    plt.legend(loc=\"upper right\", fontsize=\"small\")\n",
        "    plt.ylabel(\"Oil Temperature\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.title(f\"LSTM, DeepAR & MOIRAI Base Forecast (starts {BEGINNING_TIMESTAMP})\")\n",
        "    plt.figtext(0.1, 0.9000, f\"MOIRAI: RMSE={MOIRAI_rmse:.4f}, MAE={MOIRAI_mae:.4f}, SMAPE={MOIRAI_smape:.2f}%, R^2={MOIRAI_r2:.4f}\", fontsize=6, color='black')\n",
        "    plt.figtext(0.1, 0.8700, f\"DeepAR: RMSE={DeepAR_rmse:.4f}, MAE={DeepAR_mae:.4f}, SMAPE={DeepAR_smape:.2f}%, R^2={DeepAR_r2:.4f}\", fontsize=6, color='black')\n",
        "    plt.figtext(0.1, 0.8400, f\"LSTM: RMSE={LSTM_rmse:.4f}, MAE={LSTM_mae:.4f}, SMAPE={LSTM_smape:.2f}%, R^2={LSTM_r2:.4f}\", fontsize=6, color='black')\n",
        "    plt.figtext(0.1, 0.8100,f\"LSTM&DeepAR&MOIRAI_Features={dataset.feat_dynamic_real}\", fontsize=4, color='black')\n",
        "    plt.figtext(0.1, 0.7800,f\"DeepAR&MOIRAI_Past_Features={dataset.past_feat_dynamic_real}\", fontsize=4, color='black')\n",
        "    plt.figtext(0.1, 0.7500,f\"LSTM: HIDDEN_SIZE={LSTM_HIDDEN_SIZE}, BATCH={LSTM_BATCH_SIZE}, LR={LSTM_LR}, Epochs={LSTM_NUM_EPOCHS}\", fontsize=4, color='black')\n",
        "    plt.figtext(0.1, 0.7200,f\"MOIRAI: PATCH={MOIRAI_PATCH_SIZE}, BATCH={MOIRAI_BATCH_SIZE}, LR=n.A., Epochs=n.A.\", fontsize=4, color='black')\n",
        "    plt.figtext(0.1, 0.6900,f\"DeepAR: HIDDEN_SIZE={DeepAR_HIDDEN_SIZE}, RNN_LAYERS={DeepAR_NUM_LAYERS}, LR={DeepAR_LR}, Epochs={DeepAR_NUM_EPOCHS}\", fontsize=4, color='black')\n",
        "    plt.figtext(0.1, 0.6600,f\"Dataset={file_name_no_extension(FILEPATH)}\", fontsize=4, color='black')\n",
        "    plt.figtext(0.1, 0.6300,f\"Pre_L={PREDICTION_LENGTH}, Con_L={CONTEXT_LENGTH}\", fontsize=6, color='black')\n",
        "\n",
        "    # Set y-axis limits\n",
        "    plt.ylim(12.0, 30.0)\n",
        "\n",
        "    # Set y-axis limits and x-axis formatting\n",
        "    ax = plt.gca()\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%m-%d\"))\n",
        "    ax.xaxis.set_major_locator(mdates.DayLocator())\n",
        "    plt.xticks(rotation=25)\n",
        "\n",
        "    # Adjust layout and save plot\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"PL={PREDICTION_LENGTH}-CL={CONTEXT_LENGTH}-{BEGINNING_TIMESTAMP}-LSTM_DeepAR_MOIRAIBase_Forecasts.pdf\")\n",
        "    #plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    # Reset plot before next round\n",
        "    plt.figure().clear()\n",
        "    plt.close()\n",
        "    plt.cla()\n",
        "    plt.clf()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5WvOywrXWbW"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Step5.2: Prepare fine-tuned model by loading model weights from ckpt file\n",
        "checkpoint_path = \"/content/multirun/2025-02-11/15-40-14/0/checkpoints/epoch=13-step=1400.ckpt\"\n",
        "\n",
        "# Load the model\n",
        "model = MoiraiForecast.load_from_checkpoint(checkpoint_path,\n",
        "      prediction_length=PREDICTION_LENGTH,\n",
        "      context_length=CONTEXT_LENGTH,\n",
        "      patch_size=PATCH_SIZE,\n",
        "      num_samples=100,\n",
        "      target_dim=1,\n",
        "      feat_dynamic_real_dim=dataset.num_feat_dynamic_real,\n",
        "      past_feat_dynamic_real_dim=dataset.num_past_feat_dynamic_real)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CN6y68zkhZS2"
      },
      "outputs": [],
      "source": [
        "# Fine-tuning in regards to https://github.com/SalesforceAIResearch/uni2ts/blob/main/README.md#fine-tuning\n",
        "# Step 1 Set Data Path Directory\n",
        "!echo \"CUSTOM_DATA_PATH=/content/uni2ts/\" >> .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "es3PtupPF-OB"
      },
      "outputs": [],
      "source": [
        "!echo \"PYTHONPATH=/content/uni2ts\" >> .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9NAOCxbYZwC",
        "outputId": "d60a0ab3-f3cd-41e4-873d-ec18a6e01367"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUSTOM_DATA_PATH=/content/uni2ts/\n",
            "PYTHONPATH=/content/uni2ts\n"
          ]
        }
      ],
      "source": [
        "!cat .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogaNfoMChlvJ",
        "outputId": "a4ae6c56-0db1-4c82-e8b5-2e04a9bbb5de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inferred frequency: H. Using this value for the 'freq' parameter.\n",
            "Generating train split: 1 examples [00:00,  4.58 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 1/1 [00:00<00:00, 263.00 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 2.1 process dataset\n",
        "!python -m uni2ts.data.builder.simple customdataset /content/uni2ts/230401_250108_PT1H_Solcast_reduced_features.csv --dataset_type wide_multivariate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsF2EKXEktSI",
        "outputId": "048f19bc-a972-42e2-f7df-72612a5907b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     dc_power  air_temp  azimuth  cloud_opacity  dhi  dni  ghi  zenith\n",
            "date                                                                                  \n",
            "2023-04-01 01:00:00       0.0         8      -27           37.4    0    0    0     125\n",
            "2023-04-01 02:00:00       0.0         8      -43           46.6    0    0    0     120\n",
            "2023-04-01 03:00:00       0.0         8      -57           40.6    0    0    0     112\n",
            "2023-04-01 04:00:00       0.0         7      -70           47.6    0    0    0     103\n",
            "2023-04-01 05:00:00       0.0         7      -81           58.2    0    0    0      93\n",
            "...                       ...       ...      ...            ...  ...  ...  ...     ...\n",
            "2024-08-31 03:00:00       0.0        19      -55            0.0    0    0    0     107\n",
            "2024-08-31 04:00:00       0.0        19      -67           16.5    0    0    0      98\n",
            "2024-08-31 05:00:00       0.0        19      -79           39.8    7    0    7      88\n",
            "2024-08-31 06:00:00       0.0        19      -90           23.9   94   47  104      78\n",
            "2024-08-31 07:00:00       0.1        21     -101            0.0   89  575  303      68\n",
            "\n",
            "[12439 rows x 8 columns]\n",
            "Inferred frequency: H. Using this value for the 'freq' parameter.\n",
            "Generating train split: 8 examples [00:00, 49.89 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 8/8 [00:00<00:00, 2004.21 examples/s]\n",
            "                     dc_power  air_temp  azimuth  cloud_opacity  dhi  dni  ghi  zenith\n",
            "date                                                                                  \n",
            "2023-04-01 01:00:00       0.0         8      -27           37.4    0    0    0     125\n",
            "2023-04-01 02:00:00       0.0         8      -43           46.6    0    0    0     120\n",
            "2023-04-01 03:00:00       0.0         8      -57           40.6    0    0    0     112\n",
            "2023-04-01 04:00:00       0.0         7      -70           47.6    0    0    0     103\n",
            "2023-04-01 05:00:00       0.0         7      -81           58.2    0    0    0      93\n",
            "...                       ...       ...      ...            ...  ...  ...  ...     ...\n",
            "2025-01-07 20:00:00       0.0         1       79           32.1    0    0    0     131\n",
            "2025-01-07 21:00:00       0.0         1       65           73.2    0    0    0     140\n",
            "2025-01-07 22:00:00       0.0         2       45           33.5    0    0    0     149\n",
            "2025-01-07 23:00:00       0.0         1       18           40.6    0    0    0     154\n",
            "2025-01-08 00:00:00       0.0         2      -14           51.0    0    0    0     155\n",
            "\n",
            "[15552 rows x 8 columns]\n",
            "Inferred frequency: H. Using this value for the 'freq' parameter.\n",
            "Generating train split: 8 examples [00:00, 52.27 examples/s]\n",
            "Saving the dataset (1/1 shards): 100% 8/8 [00:00<00:00, 2196.11 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 2.2 Set offset\n",
        "!python -m uni2ts.data.builder.simple customdataset /content/uni2ts/230401_250108_PT1H_Solcast_reduced_features.csv --date_offset '2024-08-31 07:00:00'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "903wnpN-PDG2"
      },
      "outputs": [],
      "source": [
        "#!mv content/uni2ts/cli content/uni2ts/src/uni2ts/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KqmjyQbeCqjU"
      },
      "outputs": [],
      "source": [
        "# Step 3 (move cli dir to src/uni2ts first!!!!)\n",
        "# Set Batch size here /content/uni2ts/src/uni2ts/cli/conf/finetune/default.yaml to lower value\n",
        "# For moirai large with A100 40GB RAM use 16 as batch size in val_dataloader and train_dataloader section\n",
        "\n",
        "#!python -m uni2ts.cli.train -cp conf/finetune run_name=example_run model=moirai_1.1_R_large data=etth1 val_data=etth1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z-quJtw5FZL",
        "outputId": "32c2b9eb-5eda-409b-b738-0c7dfc50fa08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2025-02-11 15:40:14,959][HYDRA] Launching 1 jobs locally\n",
            "[2025-02-11 15:40:14,959][HYDRA] \t#0 : run_name=example_run model=moirai_1.1_R_base data=customdataset val_data=customdataset model.module_kwargs.dropout_p=0.2 trainer.max_epochs=25 model.lr=1e-07 train_dataloader.batch_size=24 val_dataloader.batch_size=24\n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "[2025-02-11 15:40:16,903][datasets][INFO] - PyTorch version 2.4.1 available.\n",
            "[2025-02-11 15:40:16,903][datasets][INFO] - TensorFlow version 2.18.0 available.\n",
            "[2025-02-11 15:40:16,904][datasets][INFO] - JAX version 0.4.33 available.\n",
            "Seed set to 0\n",
            "2025-02-11 15:40:17.397359: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1739288417.422296   43645 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1739288417.429823   43645 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name   | Type         | Params | Mode \n",
            "------------------------------------------------\n",
            "0 | module | MoiraiModule | 91.4 M | train\n",
            "------------------------------------------------\n",
            "91.4 M    Trainable params\n",
            "0         Non-trainable params\n",
            "91.4 M    Total params\n",
            "365.431   Total estimated model params size (MB)\n",
            "253       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Epoch 0: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.360, train/PackedNLLLoss=3.230]Metric val/PackedNLLLoss improved. New best score: 2.360\n",
            "Epoch 1: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.360, train/PackedNLLLoss=3.230]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.300, train/PackedNLLLoss=3.120]Metric val/PackedNLLLoss improved by 0.059 >= min_delta = 0.0. New best score: 2.302\n",
            "Epoch 2: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.300, train/PackedNLLLoss=3.120]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.270, train/PackedNLLLoss=3.050]Metric val/PackedNLLLoss improved by 0.033 >= min_delta = 0.0. New best score: 2.268\n",
            "Epoch 3: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.270, train/PackedNLLLoss=3.050]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.250, train/PackedNLLLoss=3.000]Metric val/PackedNLLLoss improved by 0.022 >= min_delta = 0.0. New best score: 2.247\n",
            "Epoch 4: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.250, train/PackedNLLLoss=3.000]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.220, train/PackedNLLLoss=2.980]Metric val/PackedNLLLoss improved by 0.022 >= min_delta = 0.0. New best score: 2.225\n",
            "Epoch 5: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.220, train/PackedNLLLoss=2.980]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.210, train/PackedNLLLoss=2.950]Metric val/PackedNLLLoss improved by 0.012 >= min_delta = 0.0. New best score: 2.213\n",
            "Epoch 6: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.210, train/PackedNLLLoss=2.950]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.200, train/PackedNLLLoss=2.930]Metric val/PackedNLLLoss improved by 0.014 >= min_delta = 0.0. New best score: 2.199\n",
            "Epoch 7: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.200, train/PackedNLLLoss=2.930]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.190, train/PackedNLLLoss=2.900]Metric val/PackedNLLLoss improved by 0.013 >= min_delta = 0.0. New best score: 2.186\n",
            "Epoch 8: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.190, train/PackedNLLLoss=2.900]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.170, train/PackedNLLLoss=2.890]Metric val/PackedNLLLoss improved by 0.012 >= min_delta = 0.0. New best score: 2.174\n",
            "Epoch 9: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.170, train/PackedNLLLoss=2.890]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.170, train/PackedNLLLoss=2.870]Metric val/PackedNLLLoss improved by 0.006 >= min_delta = 0.0. New best score: 2.167\n",
            "Epoch 10: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.170, train/PackedNLLLoss=2.870]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.160, train/PackedNLLLoss=2.860]Metric val/PackedNLLLoss improved by 0.004 >= min_delta = 0.0. New best score: 2.163\n",
            "Epoch 11: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.160, train/PackedNLLLoss=2.860]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.160, train/PackedNLLLoss=2.850]Metric val/PackedNLLLoss improved by 0.008 >= min_delta = 0.0. New best score: 2.156\n",
            "Epoch 12: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.160, train/PackedNLLLoss=2.850]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.850]Metric val/PackedNLLLoss improved by 0.004 >= min_delta = 0.0. New best score: 2.152\n",
            "Epoch 13: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.850]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.840]Metric val/PackedNLLLoss improved by 0.004 >= min_delta = 0.0. New best score: 2.148\n",
            "Epoch 14: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.840]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15: |          | 100/? [02:12<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.830]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16: |          | 100/? [02:12<00:00,  0.76it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.830]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.820]Monitored metric val/PackedNLLLoss did not improve in the last 3 records. Best score: 2.148. Signaling Trainer to stop.\n",
            "Epoch 16: |          | 100/? [02:13<00:00,  0.75it/s, v_num=0, val/PackedNLLLoss=2.150, train/PackedNLLLoss=2.820]\n",
            "/usr/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 22 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n"
          ]
        }
      ],
      "source": [
        "# Use Hydra's Sweeping Feature for Hyperparameter Search\n",
        "!python -m uni2ts.cli.train --multirun -cp conf/finetune run_name=example_run model=moirai_1.1_R_base data=customdataset val_data=customdataset \\\n",
        "  model.module_kwargs.dropout_p=0.2 \\\n",
        "  trainer.max_epochs=25 \\\n",
        "  model.lr=1e-7 \\\n",
        "  train_dataloader.batch_size=24 \\\n",
        "  val_dataloader.batch_size=24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMAEx64Har1x"
      },
      "outputs": [],
      "source": [
        "# Show Tensorboard with results\n",
        "#%load_ext tensorboard\n",
        "#%tensorboard --logdir /content/multirun\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppn9_O3-WJJ0"
      },
      "outputs": [],
      "source": [
        "#Export outputs\n",
        "#!zip -r outputs.zip /content/outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_B_t-0YOvLq"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "#!python -m uni2ts.cli.eval run_name=example_eval_1 model=moirai_1.0_R_small model.patch_size=32 model.context_length=1000 data=etth1_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwrqN3dk8vB2"
      },
      "outputs": [],
      "source": [
        "#import gc\n",
        "\n",
        "# Invoke garbage collector\n",
        "#gc.collect()\n",
        "\n",
        "# Clear GPU cache\n",
        "#torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}